{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/stonet2000/rl-with-lux-2-rl-problem-solving?scriptVersionId=117891467\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"## Setup Code\n\nBefore we start lets install some dependencies. This will also run some extra code that your local notebook may not need to due to how Kaggle Notebooks are setup.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade luxai_s2\n!pip install pettingzoo==1.12.0 gym==0.21.0 stable-baselines3\n!pip install --upgrade \"importlib_metadata<5.0\"","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-02-01T04:21:29.615147Z","iopub.execute_input":"2023-02-01T04:21:29.616142Z","iopub.status.idle":"2023-02-01T04:22:54.922196Z","shell.execute_reply.started":"2023-02-01T04:21:29.615792Z","shell.execute_reply":"2023-02-01T04:22:54.92029Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting luxai_s2\n  Downloading luxai_s2-2.1.1-py3-none-any.whl (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m234.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from luxai_s2) (1.21.6)\nCollecting importlib-metadata<5.0\n  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from luxai_s2) (1.7.3)\nCollecting vec-noise\n  Downloading vec_noise-1.1.4.zip (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.1/134.1 kB\u001b[0m \u001b[31m582.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting pettingzoo\n  Downloading PettingZoo-1.22.3-py3-none-any.whl (816 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.1/816.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from luxai_s2) (1.1.0)\nCollecting pygame\n  Downloading pygame-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from luxai_s2) (3.5.2)\nCollecting gym==0.21.0\n  Downloading gym-0.21.0.tar.gz (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting omegaconf\n  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym==0.21.0->luxai_s2) (2.1.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0->luxai_s2) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0->luxai_s2) (3.8.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->luxai_s2) (4.33.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->luxai_s2) (1.4.3)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->luxai_s2) (2.8.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->luxai_s2) (23.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->luxai_s2) (0.11.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->luxai_s2) (3.0.9)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->luxai_s2) (9.1.1)\nCollecting antlr4-python3-runtime==4.9.*\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.7/site-packages (from omegaconf->luxai_s2) (6.0)\nCollecting gymnasium>=0.26.0\n  Downloading gymnasium-0.27.1-py3-none-any.whl (883 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting gymnasium-notices>=0.0.1\n  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\nCollecting jax-jumpy>=0.2.0\n  Downloading jax_jumpy-0.2.0-py3-none-any.whl (11 kB)\nCollecting typing-extensions>=3.6.4\n  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->luxai_s2) (1.15.0)\nBuilding wheels for collected packages: gym, antlr4-python3-runtime, vec-noise\n  Building wheel for gym (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616824 sha256=d8c5b618843dacabb8ecd25b1e58be526c37d7cd768d9355e68f109e405a1870\n  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=9ab290b640a1487182b494b1caf6b85528c72b83181874fa7172c2ac393d1575\n  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n  Building wheel for vec-noise (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for vec-noise: filename=vec_noise-1.1.4-cp37-cp37m-linux_x86_64.whl size=85824 sha256=ebb80a7ced4c42233134fe136c6d689475f9580c5b19fac3901fe93732b1ee58\n  Stored in directory: /root/.cache/pip/wheels/fc/0c/19/5932b4834cf3204ed2ae845e788f07c79b3279c302d55d6fa8\nSuccessfully built gym antlr4-python3-runtime vec-noise\nInstalling collected packages: gymnasium-notices, antlr4-python3-runtime, vec-noise, typing-extensions, pygame, omegaconf, jax-jumpy, importlib-metadata, gymnasium, gym, pettingzoo, luxai_s2\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.1.1\n    Uninstalling typing_extensions-4.1.1:\n      Successfully uninstalled typing_extensions-4.1.1\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib-metadata 6.0.0\n    Uninstalling importlib-metadata-6.0.0:\n      Successfully uninstalled importlib-metadata-6.0.0\n  Attempting uninstall: gym\n    Found existing installation: gym 0.26.2\n    Uninstalling gym-0.26.2:\n      Successfully uninstalled gym-0.26.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\nthinc 8.0.17 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\ntensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\ntensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\ntensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.4.0 which is incompatible.\ntensorflow-transform 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\nspacy 3.3.2 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.2 which is incompatible.\nflake8 5.0.4 requires importlib-metadata<4.3,>=1.1.0; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\ncmudict 1.0.13 requires importlib-metadata<6.0.0,>=5.1.0, but you have importlib-metadata 4.13.0 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.40.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 8.0.0 which is incompatible.\naiobotocore 2.4.2 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.54 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 gym-0.21.0 gymnasium-0.27.1 gymnasium-notices-0.0.1 importlib-metadata-4.13.0 jax-jumpy-0.2.0 luxai_s2-2.1.1 omegaconf-2.3.0 pettingzoo-1.22.3 pygame-2.1.2 typing-extensions-4.4.0 vec-noise-1.1.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pettingzoo==1.12.0\n  Downloading PettingZoo-1.12.0.tar.gz (756 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.1/756.1 kB\u001b[0m \u001b[31m990.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: gym==0.21.0 in /opt/conda/lib/python3.7/site-packages (0.21.0)\nCollecting stable-baselines3\n  Downloading stable_baselines3-1.7.0-py3-none-any.whl (171 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.8/171.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from pettingzoo==1.12.0) (1.21.6)\nRequirement already satisfied: importlib-metadata>=4.8.1 in /opt/conda/lib/python3.7/site-packages (from gym==0.21.0) (4.13.0)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym==0.21.0) (2.1.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from stable-baselines3) (3.5.2)\nRequirement already satisfied: typing-extensions<5,>=4.0 in /opt/conda/lib/python3.7/site-packages (from stable-baselines3) (4.4.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from stable-baselines3) (1.3.5)\nRequirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.7/site-packages (from stable-baselines3) (1.11.0+cpu)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym==0.21.0) (3.8.0)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (9.1.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (1.4.3)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (3.0.9)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (23.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (4.33.3)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (0.11.0)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->stable-baselines3) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.15.0)\nBuilding wheels for collected packages: pettingzoo\n  Building wheel for pettingzoo (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pettingzoo: filename=PettingZoo-1.12.0-py3-none-any.whl size=873583 sha256=d39e40e98294745d02210909cf7e4eb7db138316da0569a2809ea4086ef69583\n  Stored in directory: /root/.cache/pip/wheels/50/1d/da/6a09fdb8c06333aa981d1239e40cc8aa5cbac80b30498b1e85\nSuccessfully built pettingzoo\nInstalling collected packages: stable-baselines3, pettingzoo\n  Attempting uninstall: pettingzoo\n    Found existing installation: PettingZoo 1.22.3\n    Uninstalling PettingZoo-1.22.3:\n      Successfully uninstalled PettingZoo-1.22.3\nSuccessfully installed pettingzoo-1.12.0 stable-baselines3-1.7.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: importlib_metadata<5.0 in /opt/conda/lib/python3.7/site-packages (4.13.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib_metadata<5.0) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib_metadata<5.0) (4.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile /opt/conda/lib/python3.7/site-packages/luxai_s2/version.py\n__version__ = \"\"\n# this code above is used for Kaggle Notebooks\n# You might not need to run this but if you get an attribute error about the gym package, run it","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-02-01T04:22:54.926526Z","iopub.execute_input":"2023-02-01T04:22:54.927102Z","iopub.status.idle":"2023-02-01T04:22:54.937617Z","shell.execute_reply.started":"2023-02-01T04:22:54.927055Z","shell.execute_reply":"2023-02-01T04:22:54.935941Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Overwriting /opt/conda/lib/python3.7/site-packages/luxai_s2/version.py\n","output_type":"stream"}]},{"cell_type":"code","source":"import importlib\nimport importlib_metadata\n# kaggle has 6.0.0 installed but we need version <5.0\nimportlib.reload(importlib_metadata)\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-02-01T04:22:54.940164Z","iopub.execute_input":"2023-02-01T04:22:54.941173Z","iopub.status.idle":"2023-02-01T04:22:55.267001Z","shell.execute_reply.started":"2023-02-01T04:22:54.941101Z","shell.execute_reply":"2023-02-01T04:22:55.265127Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<module 'importlib_metadata' from '/opt/conda/lib/python3.7/site-packages/importlib_metadata/__init__.py'>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Reinforcement Learning for Lux AI Season 2 🤖\n\nPart 2 of the RL series will now dig into building a working RL agent for the Lux AI Challenge, Season 2!\n\nLux AI is designed to be intuitive to understand, but heavily layered in complexity and interactions of game mechanics in an multi-agent cooperative and competitive environment. \n\nLux AI Season 2's rules can be found here: https://www.lux-ai.org/specs-s2. Make sure to read them to learn how to the game works, and the rest of this tutorial will be much easier to understand.\n\nPart 1 of the series covered the single-agent RL setup, but Lux AI Season 2 is multi-agent! Moreover, the environment has different phases and a complex action space which makes it difficult to learn or use of the box. \n\nThis tutorial will cover simple tools and tricks on how to reduce a complex problem into a easier one! We will primarily focus on three things: \n\n1. Simplifying the action space with controllers/action wrappers\n2. Simplifying observations\n3. Transforming the three phase Lux AI game into a single phase game\n\nThis starter kit is also implemented in https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/rl/sb3\n\nWe highly **recommend running this code on a GPU** as RL training can be fairly slow and needs good tuning.\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Simplifying the Action Space\n\nThe action space is quite complicated in Lux S2 as each robot can move, dig, transfer/pickup, all in addition to being able to combine any sequence of these primitives into an action queue of up to length 20. For machine learning, such a massive action space leads to the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality), making any ML algorithm have a much harder time to learn something useful, especially in RL.\n\nTo handle this, we can program a custom Controller that translates actions from one action space to the original action space and adds a few tricks and heuristics to be integrated with RL training. Since the original lux action space is large, this controller can be a little complicated. For those who want to dive straight into training you can use the controller as is. \n\nFor a high-level overview this controller will\n- Define a massively simplified action space\n- Translate actions from the discrete action space into the Lux S2 action space `action_to_lux_action`\n- Add a heuristic factory action to build one Heavy robot\n- Generate action masks where False = an action is invalid\n\nOverall, the action space of the controller is a discrete action space with just 12 dimensions to control just one heavy robot. It allows for a robot's 4 directional movement, transferring ice in 4 directions in addition to center, picking up power, digging, and a no-op action. This doesn't include factory actions, self destruct, recharging, transferring other types of resources, or longer planned action queues in the action space, which are all open problems for you to potentially tackle!\n\nThe controller also includes a trick to allow agents to reduce power costs incurred by action queue updates. The controller skips updating action queues if the existing action queue is the same as the new one the agent wants to use for the robot.\n\nWhile this simplification doesn't include adding in more complex things like more heavy robots or planting lichen, it will train out a succesful policy that with simple modifications, will beat the majority of bots using the rule-based starter kits.\n\nMore advanced usages can consider how to model the actions of different types of units on a game board (e.g. heavy, light, or factory) by using a MultiDiscrete action space. A more practical and likely winning solution can be to use a image-like controller by generating actions for each tile on the board and only using the actions with friendly units on that tile. See [Season 1's solution by ToadBrigade](https://www.kaggle.com/competitions/lux-ai-2021/discussion/294993) and our previous [research paper: Emergent Collective Intelligence from Massive-Agent Cooperation and Competition](https://arxiv.org/abs/2301.01609) for how a image-like controller can work.\n","metadata":{"tags":[]}},{"cell_type":"code","source":"import sys\nfrom typing import Any, Dict\n\nimport numpy as np\nimport numpy.typing as npt\nfrom gym import spaces\n\n\n# Controller class copied here since you won't have access to the luxai_s2 package directly on the competition server\nclass Controller:\n    def __init__(self, action_space: spaces.Space) -> None:\n        self.action_space = action_space\n\n    def action_to_lux_action(\n        self, agent: str, obs: Dict[str, Any], action: npt.NDArray\n    ):\n        \"\"\"\n        Takes as input the current \"raw observation\" and the parameterized action and returns\n        an action formatted for the Lux env\n        \"\"\"\n        raise NotImplementedError()\n\n    def action_masks(self, agent: str, obs: Dict[str, Any]):\n        \"\"\"\n        Generates a boolean action mask indicating in each discrete dimension whether it would be valid or not\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass SimpleUnitDiscreteController(Controller):\n    def __init__(self, env_cfg) -> None:\n        \"\"\"\n        A simple controller that controls only the robot that will get spawned.\n        Moreover, it will always try to spawn one heavy robot if there are none regardless of action given\n\n        For the robot unit\n        - 4 cardinal direction movement (4 dims)\n        - a move center no-op action (1 dim)\n        - transfer action just for transferring ice in 4 cardinal directions or center (5)\n        - pickup action for power (1 dims)\n        - dig action (1 dim)\n        - no op action (1 dim) - equivalent to not submitting an action queue which costs power\n\n        It does not include\n        - self destruct action\n        - recharge action\n        - planning (via actions executing multiple times or repeating actions)\n        - factory actions\n        - transferring power or resources other than ice\n\n        To help understand how to this controller works to map one action space to the original lux action space,\n        see how the lux action space is defined in luxai_s2/spaces/action.py\n\n        \"\"\"\n        self.env_cfg = env_cfg\n        self.move_act_dims = 4\n        self.transfer_act_dims = 5\n        self.pickup_act_dims = 1\n        self.dig_act_dims = 1\n        self.no_op_dims = 1\n\n        self.move_dim_high = self.move_act_dims\n        self.transfer_dim_high = self.move_dim_high + self.transfer_act_dims\n        self.pickup_dim_high = self.transfer_dim_high + self.pickup_act_dims\n        self.dig_dim_high = self.pickup_dim_high + self.dig_act_dims\n        self.no_op_dim_high = self.dig_dim_high + self.no_op_dims\n\n        self.total_act_dims = self.no_op_dim_high\n        action_space = spaces.Discrete(self.total_act_dims)\n        super().__init__(action_space)\n\n    def _is_move_action(self, id):\n        return id < self.move_dim_high\n\n    def _get_move_action(self, id):\n        # move direction is id + 1 since we don't allow move center here\n        return np.array([0, id + 1, 0, 0, 0, 1])\n\n    def _is_transfer_action(self, id):\n        return id < self.transfer_dim_high\n\n    def _get_transfer_action(self, id):\n        id = id - self.move_dim_high\n        transfer_dir = id % 5\n        return np.array([1, transfer_dir, 0, self.env_cfg.max_transfer_amount, 0, 1])\n\n    def _is_pickup_action(self, id):\n        return id < self.pickup_dim_high\n\n    def _get_pickup_action(self, id):\n        return np.array([2, 0, 4, self.env_cfg.max_transfer_amount, 0, 1])\n\n    def _is_dig_action(self, id):\n        return id < self.dig_dim_high\n\n    def _get_dig_action(self, id):\n        return np.array([3, 0, 0, 0, 0, 1])\n\n    def action_to_lux_action(\n        self, agent: str, obs: Dict[str, Any], action: npt.NDArray\n    ):\n        shared_obs = obs[\"player_0\"]\n        lux_action = dict()\n        units = shared_obs[\"units\"][agent]\n        for unit_id in units.keys():\n            unit = units[unit_id]\n            choice = action\n            action_queue = []\n            no_op = False\n            if self._is_move_action(choice):\n                action_queue = [self._get_move_action(choice)]\n            elif self._is_transfer_action(choice):\n                action_queue = [self._get_transfer_action(choice)]\n            elif self._is_pickup_action(choice):\n                action_queue = [self._get_pickup_action(choice)]\n            elif self._is_dig_action(choice):\n                action_queue = [self._get_dig_action(choice)]\n            else:\n                # action is a no_op, so we don't update the action queue\n                no_op = True\n\n            # simple trick to help agents conserve power is to avoid updating the action queue\n            # if the agent was previously trying to do that particular action already\n            if len(unit[\"action_queue\"]) > 0 and len(action_queue) > 0:\n                same_actions = (unit[\"action_queue\"][0] == action_queue[0]).all()\n                if same_actions:\n                    no_op = True\n            if not no_op:\n                lux_action[unit_id] = action_queue\n\n            break\n\n        factories = shared_obs[\"factories\"][agent]\n        if len(units) == 0:\n            for unit_id in factories.keys():\n                lux_action[unit_id] = 1  # build a single heavy\n\n        return lux_action\n\n    def action_masks(self, agent: str, obs: Dict[str, Any]):\n        \"\"\"\n        Defines a simplified action mask for this controller's action space\n\n        Doesn't account for whether robot has enough power\n        \"\"\"\n\n        # compute a factory occupancy map that will be useful for checking if a board tile\n        # has a factory and which team's factory it is.\n        shared_obs = obs[agent]\n        factory_occupancy_map = (\n            np.ones_like(shared_obs[\"board\"][\"rubble\"], dtype=int) * -1\n        )\n        factories = dict()\n        for player in shared_obs[\"factories\"]:\n            factories[player] = dict()\n            for unit_id in shared_obs[\"factories\"][player]:\n                f_data = shared_obs[\"factories\"][player][unit_id]\n                f_pos = f_data[\"pos\"]\n                # store in a 3x3 space around the factory position it's strain id.\n                factory_occupancy_map[\n                    f_pos[0] - 1 : f_pos[0] + 2, f_pos[1] - 1 : f_pos[1] + 2\n                ] = f_data[\"strain_id\"]\n\n        units = shared_obs[\"units\"][agent]\n        action_mask = np.zeros((self.total_act_dims), dtype=bool)\n        for unit_id in units.keys():\n            action_mask = np.zeros(self.total_act_dims)\n            # movement is always valid\n            action_mask[:4] = True\n\n            # transferring is valid only if the target exists\n            unit = units[unit_id]\n            pos = np.array(unit[\"pos\"])\n            # a[1] = direction (0 = center, 1 = up, 2 = right, 3 = down, 4 = left)\n            move_deltas = np.array([[0, 0], [0, -1], [1, 0], [0, 1], [-1, 0]])\n            for i, move_delta in enumerate(move_deltas):\n                transfer_pos = np.array(\n                    [pos[0] + move_delta[0], pos[1] + move_delta[1]]\n                )\n                # check if theres a factory tile there\n                if (\n                    transfer_pos[0] < 0\n                    or transfer_pos[1] < 0\n                    or transfer_pos[0] >= len(factory_occupancy_map)\n                    or transfer_pos[1] >= len(factory_occupancy_map[0])\n                ):\n                    continue\n                factory_there = factory_occupancy_map[transfer_pos[0], transfer_pos[1]]\n                if factory_there in shared_obs[\"teams\"][agent][\"factory_strains\"]:\n                    action_mask[\n                        self.transfer_dim_high - self.transfer_act_dims + i\n                    ] = True\n\n            factory_there = factory_occupancy_map[pos[0], pos[1]]\n            on_top_of_factory = (\n                factory_there in shared_obs[\"teams\"][agent][\"factory_strains\"]\n            )\n\n            # dig is valid only if on top of tile with rubble or resources or lichen\n            board_sum = (\n                shared_obs[\"board\"][\"ice\"][pos[0], pos[1]]\n                + shared_obs[\"board\"][\"ore\"][pos[0], pos[1]]\n                + shared_obs[\"board\"][\"rubble\"][pos[0], pos[1]]\n                + shared_obs[\"board\"][\"lichen\"][pos[0], pos[1]]\n            )\n            if board_sum > 0 and not on_top_of_factory:\n                action_mask[\n                    self.dig_dim_high - self.dig_act_dims : self.dig_dim_high\n                ] = True\n\n            # pickup is valid only if on top of factory tile\n            if on_top_of_factory:\n                action_mask[\n                    self.pickup_dim_high - self.pickup_act_dims : self.pickup_dim_high\n                ] = True\n                action_mask[\n                    self.dig_dim_high - self.dig_act_dims : self.dig_dim_high\n                ] = False\n\n            # no-op is always valid\n            action_mask[-1] = True\n            break\n        return action_mask\n","metadata":{"execution":{"iopub.status.busy":"2023-02-01T04:22:55.271032Z","iopub.execute_input":"2023-02-01T04:22:55.271927Z","iopub.status.idle":"2023-02-01T04:22:55.632256Z","shell.execute_reply.started":"2023-02-01T04:22:55.271856Z","shell.execute_reply":"2023-02-01T04:22:55.630975Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## 2. Simplifying the Observation Space\n\nLux S2 is fully observable which means you can see everything on the map, the opponents units etc. However, this is very high dimensional and not necessarily easy to learn from due to the curse of dimensionality (again!). We want to simplify this observation space in a way that contains sufficient information to learn a good policy but is also easy to learn from.\n\nFor this tutorial, we will create a state-based observation space (no image like features e.g. the rubble, ice, ore maps) with some feature engineering that includes useful information such as the distance to the closest factory and ice tile. The wrapper we provide below will use the `gym.ObservationWrapper` interface. Note that since we are focusing on just controlling one heavy robot, the observation wrapper is written to only support one heavy robot (and returns 0 if there are none).\n\n\nMore advanced solutions can look into using the full set of observations and designing the appropriate neural net architecture to process them. One idea would be to use convolutional neural networks to process board features like images. See [Season 1's solution by ToadBrigade](https://www.kaggle.com/competitions/lux-ai-2021/discussion/294993) and our previous [research paper: Emergent Collective Intelligence from Massive-Agent Cooperation and Competition](https://arxiv.org/abs/2301.01609) for example architectures and feature engineering choices.\n","metadata":{"tags":[]}},{"cell_type":"code","source":"from typing import Any, Dict\n\nimport gym\nimport numpy as np\nimport numpy.typing as npt\nfrom gym import spaces\n\n\nclass SimpleUnitObservationWrapper(gym.ObservationWrapper):\n    \"\"\"\n    A simple state based observation to work with in pair with the SimpleUnitDiscreteController\n\n    It contains info only on the first robot, the first factory you own, and some useful features. If there are no owned robots the observation is just zero.\n    No information about the opponent is included. This will generate observations for all teams.\n\n    Included features:\n    - First robot's stats\n    - distance vector to closest ice tile\n    - distance vector to first factory\n\n    \"\"\"\n\n    def __init__(self, env: gym.Env) -> None:\n        super().__init__(env)\n        self.observation_space = spaces.Box(-999, 999, shape=(13,))\n\n    def observation(self, obs):\n        return SimpleUnitObservationWrapper.convert_obs(obs, self.env.state.env_cfg)\n\n    # we make this method static so the submission/evaluation code can use this as well\n    @staticmethod\n    def convert_obs(obs: Dict[str, Any], env_cfg: Any) -> Dict[str, npt.NDArray]:\n        observation = dict()\n        shared_obs = obs[\"player_0\"]\n        ice_map = shared_obs[\"board\"][\"ice\"]\n        ice_tile_locations = np.argwhere(ice_map == 1)\n\n        for agent in obs.keys():\n            obs_vec = np.zeros(\n                13,\n            )\n\n            factories = shared_obs[\"factories\"][agent]\n            factory_vec = np.zeros(2)\n            for k in factories.keys():\n                # here we track a normalized position of the first friendly factory\n                factory = factories[k]\n                factory_vec = np.array(factory[\"pos\"]) / env_cfg.map_size\n                break\n            units = shared_obs[\"units\"][agent]\n            for k in units.keys():\n                unit = units[k]\n\n                # store cargo+power values scaled to [0, 1]\n                cargo_space = env_cfg.ROBOTS[unit[\"unit_type\"]].CARGO_SPACE\n                battery_cap = env_cfg.ROBOTS[unit[\"unit_type\"]].BATTERY_CAPACITY\n                cargo_vec = np.array(\n                    [\n                        unit[\"power\"] / battery_cap,\n                        unit[\"cargo\"][\"ice\"] / cargo_space,\n                        unit[\"cargo\"][\"ore\"] / cargo_space,\n                        unit[\"cargo\"][\"water\"] / cargo_space,\n                        unit[\"cargo\"][\"metal\"] / cargo_space,\n                    ]\n                )\n                unit_type = (\n                    0 if unit[\"unit_type\"] == \"LIGHT\" else 1\n                )  # note that build actions use 0 to encode Light\n                # normalize the unit position\n                pos = np.array(unit[\"pos\"]) / env_cfg.map_size\n                unit_vec = np.concatenate(\n                    [pos, [unit_type], cargo_vec, [unit[\"team_id\"]]], axis=-1\n                )\n\n                # we add some engineered features down here\n                # compute closest ice tile\n                ice_tile_distances = np.mean(\n                    (ice_tile_locations - np.array(unit[\"pos\"])) ** 2, 1\n                )\n                # normalize the ice tile location\n                closest_ice_tile = (\n                    ice_tile_locations[np.argmin(ice_tile_distances)] / env_cfg.map_size\n                )\n                obs_vec = np.concatenate(\n                    [unit_vec, factory_vec - pos, closest_ice_tile - pos], axis=-1\n                )\n                break\n            observation[agent] = obs_vec\n\n        return observation","metadata":{"execution":{"iopub.status.busy":"2023-02-01T04:22:55.634276Z","iopub.execute_input":"2023-02-01T04:22:55.634671Z","iopub.status.idle":"2023-02-01T04:22:55.653172Z","shell.execute_reply.started":"2023-02-01T04:22:55.634634Z","shell.execute_reply":"2023-02-01T04:22:55.651209Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## 3. Transforming Lux S2 into a Single Phase\n\nNormally RL frameworks like Stable Baselines 3, RLlib, Tianshou etc. expect the action space and observation space to be consistent throughout an episode. Lux S2 does not conform to this as we add some additional complexity like bidding and factory placement phases. A simple way to get around this is to **upgrade the reset function.**\n\nPreviously we saw that `env.reset()` resets an environment to a clean slate. We will upgrade this function by building a environment wrapper that not only resets to the clean slate, but also handles the bidding and factory placement phases so effectively agents that are learning start from game states with factories already placed.\n\nBelow will build a wrapper that works with the SB3 package. To do this, we want to provide the wrapper a bidding policy and factory placement policy which will be used by all teams to handle the first two phases in the reset function. The code below does just that by overriding the environment's reset function in the wrapper. \n\nFurthermore, we want to use the Controller we defined earlier, so that is also an argument to the SB3Wrapper and we use it to transform actions inside the `env.step` function","metadata":{"tags":[]}},{"cell_type":"code","source":"from typing import Callable, Dict\n\nimport gym\nimport numpy as np\nimport numpy.typing as npt\nfrom gym import spaces\n\nimport luxai_s2.env\nfrom luxai_s2.env import LuxAI_S2\nfrom luxai_s2.state import ObservationStateDict\nfrom luxai_s2.unit import ActionType, BidActionType, FactoryPlacementActionType\nfrom luxai_s2.utils import my_turn_to_place_factory\nfrom luxai_s2.wrappers.controllers import (\n    Controller,\n)\n\n\nclass SB3Wrapper(gym.Wrapper):\n    def __init__(\n        self,\n        env: LuxAI_S2,\n        bid_policy: Callable[\n            [str, ObservationStateDict], Dict[str, BidActionType]\n        ] = None,\n        factory_placement_policy: Callable[\n            [str, ObservationStateDict], Dict[str, FactoryPlacementActionType]\n        ] = None,\n        controller: Controller = None,\n    ) -> None:\n        \"\"\"\n        A environment wrapper for Stable Baselines 3. It reduces the LuxAI_S2 env\n        into a single phase game and places the first two phases (bidding and factory placement) into the env.reset function so that\n        interacting agents directly start generating actions to play the third phase of the game.\n\n        It also accepts a Controller that translates action's in one action space to a Lux S2 compatible action\n\n        Parameters\n        ----------\n        bid_policy: Function\n            A function accepting player: str and obs: ObservationStateDict as input that returns a bid action\n            such as dict(bid=10, faction=\"AlphaStrike\"). By default will bid 0\n        factory_placement_policy: Function\n            A function accepting player: str and obs: ObservationStateDict as input that returns a factory placement action\n            such as dict(spawn=np.array([2, 4]), metal=150, water=150). By default will spawn in a random valid location with metal=150, water=150\n        controller : Controller\n            A controller that parameterizes the action space into something more usable and converts parameterized actions to lux actions.\n            See luxai_s2/wrappers/controllers.py for available controllers and how to make your own\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.env = env\n        \n        assert controller is not None\n        \n        # set our controller and replace the action space\n        self.controller = controller\n        self.action_space = controller.action_space\n\n        # The simplified wrapper removes the first two phases of the game by using predefined policies (trained or heuristic)\n        # to handle those two phases during each reset\n        if factory_placement_policy is None:\n            def factory_placement_policy(player, obs: ObservationStateDict):\n                potential_spawns = np.array(\n                    list(zip(*np.where(obs[\"board\"][\"valid_spawns_mask\"] == 1)))\n                )\n                spawn_loc = potential_spawns[\n                    np.random.randint(0, len(potential_spawns))\n                ]\n                return dict(spawn=spawn_loc, metal=150, water=150)\n\n        self.factory_placement_policy = factory_placement_policy\n        if bid_policy is None:\n            def bid_policy(player, obs: ObservationStateDict):\n                faction = \"AlphaStrike\"\n                if player == \"player_1\":\n                    faction = \"MotherMars\"\n                return dict(bid=0, faction=faction)\n\n        self.bid_policy = bid_policy\n\n        self.prev_obs = None\n\n    def step(self, action: Dict[str, npt.NDArray]):\n        \n        # here, for each agent in the game we translate their action into a Lux S2 action\n        lux_action = dict()\n        for agent in self.env.agents:\n            if agent in action:\n                lux_action[agent] = self.controller.action_to_lux_action(\n                    agent=agent, obs=self.prev_obs, action=action[agent]\n                )\n            else:\n                lux_action[agent] = dict()\n        \n        # lux_action is now a dict mapping agent name to an action\n        obs, reward, done, info = self.env.step(lux_action)\n        self.prev_obs = obs\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        # we upgrade the reset function here\n        \n        # we call the original reset function first\n        obs = self.env.reset(**kwargs)\n        \n        # then use the bid policy to go through the bidding phase\n        action = dict()\n        for agent in self.env.agents:\n            action[agent] = self.bid_policy(agent, obs[agent])\n        obs, _, _, _ = self.env.step(action)\n        \n        # while real_env_steps < 0, we are in the factory placement phase\n        # so we use the factory placement policy to step through this\n        while self.env.state.real_env_steps < 0:\n            action = dict()\n            for agent in self.env.agents:\n                if my_turn_to_place_factory(\n                    obs[\"player_0\"][\"teams\"][agent][\"place_first\"],\n                    self.env.state.env_steps,\n                ):\n                    action[agent] = self.factory_placement_policy(agent, obs[agent])\n                else:\n                    action[agent] = dict()\n            obs, _, _, _ = self.env.step(action)\n        self.prev_obs = obs\n        \n        return obs\n","metadata":{"execution":{"iopub.status.busy":"2023-02-01T04:22:55.655298Z","iopub.execute_input":"2023-02-01T04:22:55.655836Z","iopub.status.idle":"2023-02-01T04:22:56.066925Z","shell.execute_reply.started":"2023-02-01T04:22:55.655762Z","shell.execute_reply":"2023-02-01T04:22:56.065634Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"pygame 2.1.2 (SDL 2.0.16, Python 3.7.12)\nHello from the pygame community. https://www.pygame.org/contribute.html\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Defining a Bid and Factory Placement policy\n\nTo test the code above, we can program some heuristic bid and factory placement policies","metadata":{}},{"cell_type":"code","source":"def zero_bid(player, obs):\n    # a policy that always bids 0\n    faction = \"AlphaStrike\"\n    if player == \"player_1\":\n        faction = \"MotherMars\"\n    return dict(bid=0, faction=faction)\n\ndef place_near_random_ice(player, obs):\n    \"\"\"\n    This policy will place a single factory with all the starting resources\n    near a random ice tile\n    \"\"\"\n    if obs[\"teams\"][player][\"metal\"] == 0:\n        return dict()\n    potential_spawns = list(zip(*np.where(obs[\"board\"][\"valid_spawns_mask\"] == 1)))\n    potential_spawns_set = set(potential_spawns)\n    done_search = False\n    \n    # simple numpy trick to find locations adjacent to ice tiles.\n    ice_diff = np.diff(obs[\"board\"][\"ice\"])\n    pot_ice_spots = np.argwhere(ice_diff == 1)\n    if len(pot_ice_spots) == 0:\n        pot_ice_spots = potential_spawns\n    \n    # pick a random ice spot and search around it for spawnable locations.\n    trials = 5\n    while trials > 0:\n        pos_idx = np.random.randint(0, len(pot_ice_spots))\n        pos = pot_ice_spots[pos_idx]\n        area = 3\n        for x in range(area):\n            for y in range(area):\n                check_pos = [pos[0] + x - area // 2, pos[1] + y - area // 2]\n                if tuple(check_pos) in potential_spawns_set:\n                    done_search = True\n                    pos = check_pos\n                    break\n            if done_search:\n                break\n        if done_search:\n            break\n        trials -= 1\n    \n    if not done_search:\n        spawn_loc = potential_spawns[np.random.randint(0, len(potential_spawns))]\n        pos = spawn_loc\n    \n    # this will spawn a factory at pos and with all the starting metal and water\n    metal = obs[\"teams\"][player][\"metal\"]\n    return dict(spawn=pos, metal=metal, water=metal)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T04:22:56.068839Z","iopub.execute_input":"2023-02-01T04:22:56.069505Z","iopub.status.idle":"2023-02-01T04:22:56.083026Z","shell.execute_reply.started":"2023-02-01T04:22:56.069467Z","shell.execute_reply":"2023-02-01T04:22:56.081767Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"So **without the wrapper**, when we reset the environment it looks like this:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nenv = gym.make(\"LuxAI_S2-v0\")\nenv.reset(seed=0)\nimg = env.render(\"rgb_array\")\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T04:22:56.08455Z","iopub.execute_input":"2023-02-01T04:22:56.085367Z","iopub.status.idle":"2023-02-01T04:22:56.439449Z","shell.execute_reply.started":"2023-02-01T04:22:56.085321Z","shell.execute_reply":"2023-02-01T04:22:56.438006Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f24177200d0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"**With the wrapper**, when we reset the environment it looks like this:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nenv = gym.make(\"LuxAI_S2-v0\")\nenv = SB3Wrapper(env, zero_bid, place_near_random_ice, controller=SimpleUnitDiscreteController(env.env_cfg))\nenv.reset(seed=0)\nimg = env.render(\"rgb_array\")\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T04:22:56.440958Z","iopub.execute_input":"2023-02-01T04:22:56.441342Z","iopub.status.idle":"2023-02-01T04:22:57.807643Z","shell.execute_reply.started":"2023-02-01T04:22:56.441305Z","shell.execute_reply":"2023-02-01T04:22:57.805392Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f2417a46350>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA8v0lEQVR4nO19abAk11XmdzJrfVv361UttaSWZEm2ZOQ2ksFgsA0GW9jjkYkJCBEBVgyOgZjBAUTwAwMRAxMOxwCD4cfE4MAePGhmvODAeCwmBFhW2DjskbXYkuzWvrXVm3p9r99Sa+Y98yOzKvMuVZmVlfVqu5/itSpv3S23U/d89yzEzLCwsJhfOOOegIWFxXhhhYCFxZzDCgELizmHFQIWFnMOKwQsLOYcVghYWMw5RiYEiOhOInqOiF4koo+MahwLC4vhQKOwEyAiF8DzAH4WwEkAjwL4JWZ+OvfBLCwshsKoVgI/AuBFZn6ZmVsAPg/grhGNZWFhMQQKI+r3KgAnYscnAfxor8qrlSJfuVwZ0VSGBzPgCSGX5dg/5dhX33EIcB1HGs8XDG01mHpC1OcoHRwi0JAXgMN/1HuSqtuduvgTMPzTF7YuMPN+tXxUQsB0btI9IqJfA/BrAHBoqYzPfuD2EU1leHi+wPmtmlSWpxpFGd4CArSXhwyXPV6n4DjYtVCBEyvcbDTR9HxtPmpPpimqtfT56OemtqmWCnCd4RakzAyfGUK5JwT9QdTmk/LSm65tHshy77Pi6H//lx+YykelDpwEcHXs+DCA0/EKzPxJZr6Dme9YrRRHNA0LC4skjEoIPArgRiK6johKAO4GcN+IxrKwsBgCI1EHmNkjog8D+GcALoBPM/NToxhrpzBKb0tT33ktE+NdC2bUW23pe5XrMKHoOtqSPYu6IJjhC/lcPSG0ZbwK13EkFSYtxqzuTw1GxQmAme8HcP+o+rcYHL5g1BQhACQLnILjoFIc/lHxhIAvZP7B84WBN5DhEKVX3hVk41tGM9akuu2PTAhYTCaYs71PO0lgWewsrNmwhcWcw64EUkLVSZk5V1uBSYKjbvXltAogw0I7bde6LcOoNu3mD1YIpIDrEPYuVaWyrUYL9bY3sjHVh159ERky6QcADhkIxgFfFYcIS+WSNF4WUs4E1yFUSvIj1/J8iRgkBOcWH7/tC+ksHIdQcJzMPMGokHTP0rQxgUAjZTmtEEgBIkLRdaWyvF6MSQMR4JIDR10O5NI3wY1dN9MLwNCfd1aIjAnl16YWlhOwsJhzWCFgYTHnmBl1gJnRVPRLE8oFd2hbdaCjk+/MupSIEscKdGl9v51Jt6ePL7hVQ6BgT364+eaB+KyHmU4WPT0v5PV8sMk7KgXSnuvsCAEAW80W2n5/C7g9i1W4U7b+YeZkQ5Qe5UmPQaVYQKU4m74bJn7BQseUvQ4WFhZ5wwoBC4s5x8yoAx3E9TCjX3xO45h81U1797mMRT30S4p/1HmDoB3J1SnfgCg7BgNJMCpOxtTrNKoVaa/PTAkB9ZyLjoOValkqK+RECCxWSqiWIl3aFwJr2w2Mwo7Q+P53rGo6daDzBtxDKTbYFE084tdVCEbLcJ3zI/1mRQykw0wJARUOBez3KBjhguNIypTn76wZaxpHIFUGMEIBECucBsMb03mo7+lO225RgtHTNMFyAhYWcw4rBCws5hwzow4QOoZA0TKttMMGAQz05wRyXDUGpF+/At3IKDAekut5vkAz5ggV+EmMRoUywXVI4iiYgwhEUOadZr2vnisQnm+sTCaOO8MkBUydXT4AmCEhAADLldLYxubuf/3q5AdVtwczHMN+haS7Qteva6225A1ZcAi7Fyo7JgRUxyxfMJre4N6ZKkeSxniqIw9SnWq/OomxtXcAQ9yuoYQAER0HsAnAB+Ax8x1EtAfA3wI4AuA4gF9k5rVhxkk5l1EPMVHIyxrOSLrtEMz3LL8Z5Gkx2G81YPpup6NNDLNayWO9/FPMfJSZ7wiPPwLgQWa+EcCD4bGFhcWEYhRK810A7g0/3wvgAyMYw8LCIicMywkwgK8QEQP4K2b+JICDzHwGAJj5DBEdGHaS0wLVYjDt/nGWpWNAcinGQcqK0GECxVk3CqS+xifGjn3BuFxvSv1US8Vcog1nhfHqGK9ttuxCSV2PU9NM+2wMow4Me2ffxsynwxf9ASJ6Nm1DNQ3ZvCKr7phO31Utg3TLQhMnoHpillLkJrDIhkkwOhpKHWDm0+H/zwH4EoJsxGeJ6BAAhP8/16OtTUNmYTEByCwEiGiRiJY7nwG8G8AxBOnG7gmr3QPgy8NO0sLCYnQYRh04COBL4XKmAOCzzPxPRPQogC8Q0YcAvArgF4af5pSA9SWdtNzvsdrLxglQYjvVMKgTpls2qjH0Q/KBLxiteOZiCpyzRrUta9LJVe9QLQR8z74MxlLxvqHfM1Nk5yxOFsn3VTfwMtYZ8XZjZiHAzC8DeJOh/CKAdw0zqWmFetP1Y0ObHhF3k0CGWmqAYMHKA82s5weEXuZIEcYYjXYbLU+2KlxdrO6YHZ1qCEQASgXZyMjzBXxVACvtBMw8StJ5mHJM5CP/DP2CpM5pB+IjzZTF4CwgrczP+mhkTUMmzWvMXnOd6XcEXBCSHFMaKEGHvlYZLawDkYXFnMOuBDJC0/3Dw6QfI7NKkFRH0Vm7pbFloyFiEGltU/6qpFlmsFnnHQVP0KvH/KL5Jo81y7BCICO2m23U21Gab8GBoY3+YEbHzPqLGhgYyaXarrzhYdc93QBXqSagL/VUg6KgQBFoJomiYKvZkuZQKrgoKc5AWeA4hHJR7scUxq2l2DIw69deC69GPQSHejGN19sQtSmhm2mBFQIZ0Wz72G5GQsBEHuk7BdCO1XZp+gHCGIeKlY9qQUjESi6CTt9ymUoMmsKSqQZFTc/T8hXmIgSI4CT04wuBtu9r5ZqwUD0LDXzI6Gm3yYflBCws5hxWCFhYzDmsOpAZJv0/QQUwLPWNbQyko9rS0fbE9JDjrNSRttRiZYo9EYJzg1wn3g9IS+U2jngO/QyByFSHTCqTQd1KYVCkwtR3FvTiLUZ5fa0QyAiGQcdPwQEIlbxTCC1mhq9UEsxajkXXka3miACQrkvHow119tM1ByJFbwapHIDKGzB2Vyvj9a4zXGuVt0jiCIz99ihPNijKhxg05dJUrSPzhhUCOSHLdtVU27bQeH79B4El/dLBcgIWFnMOKwQsLOYcVh1IAU8IrG83pOV7o91O5ACeP/AmPHfwaPe40q7hR1/8Ryw0N6I2LKsSjGAfPN6XJ4Kw4FF0XEK1VJDIOYcA19HnI5VQ8E+c5GODFyFpRk1k6Gfn0PJ8SVfm0OpKokUNhBp1vohBV9sMJKDBQlP1PlSRhjzMgp1QuawQSAEhGBsNOeRWkgAAgFO7r8ejR36me7zcWMNtx7+GakcIsNlYSChlni/QaEfGMQRTejWZ5QcCY0B5dwJgkkk+hmpQ1INUSzrZEcLzfc1DkGL/AkM4RqkGRdDPvZf3oYqRvK55MY59YIXAKEHU9wZONTFoMTOwnICFxZzDrgQGQJIhkFpWaW1hdfts93ixcRmu0G3eNSh2QETyXnHgN5B9iagaB2nGQjF9uzOMamC009CiIYVGTVGZ7iyktpO+QORlqV+PFP3Eu+zMccK3THvBCoGUSGMIpOLoD/4Frz/1SPeYWER8QB845EjkVMl14VbkRVvACcTbJHZrhMnJJm5AFJy2GqGYzcrziCAgX9+O8VK/4U1OUFCK0hgU9Xr306Q8mxZYITBClL0GSl6je2zyEOwF1arPURW3jMY6O8AzjQDJVy2rhWBemGbDpEROgIg+TUTniOhYrGwPET1ARC+E/1+Nffd7RPQiET1HRO8Z1cQtLCzyQRpi8G8A3KmUGfMNEtEtAO4GcGvY5i+JDAbtFhYWE4NEdYCZv0FER5TiuwC8M/x8L4CvA/jdsPzzzNwE8AoRvYggIclDOc13bEhyFqoXF/H4NW9Hs1Dtll174Rlce+Hp/h1rJCDBIVaqkBYRyCWZExgm55Z2btq6WvFG3OmFL5t0c5NBD/SypMhCpuuW2WNwNOyAL4Q0T8cQcn0YZOUEeuUbvArAt2P1ToZlGmYtDVm9uIiHbngvNqp7umXvePaLkhDQHpHw3VJTUWk3mLr/REVEGieQF2uvP8q6gdFOotdo/W3/DJ6FBo4g0TDK0HcvrmEUOQwZCAyl4rsjjpMr2ZG3nUBq8WjTkFlYTAayCoFe+QZPArg6Vu8wgNPZp2dhYTFqZFUHOvkG/xhyvsH7AHyWiP4cwJUAbgTwiLGHaURsSdYoLqDlRmpMrbyMheaGtOwpteuJXSqUQK45NPLU3CWDmsx96MFRdDVHN4ZyiCAUniQpVVnQM4Fj7UhtRwRSOBGiNIyHKf+TeYtyGpAoBIjocwhIwH1EdBLAHyJ4+bV8g8z8FBF9AcDTADwAv8HMKUzkJh/qA/bQDT+HJw+/rXu83FjD+578NBZam92ySmurb5+9Xvg0ur3KG2g0AumFo+MM0qHl+9hutpRSeU4l18FiuSSVVUtF6YXyhUCt1ZbqmI2DVC5D/bprF9ktKjiEYiFNtGM55Hka4W3yapwEpNkd+KUeXxnzDTLzxwB8bJhJTQO2S8tYWzgg3dSV+iWsNNa6x2kJNIl0SvVLFLZLEASmOuN8CE2h01RrxJ7htWLTFgZmPi9jHVOyU2kcVn0u048/qQZF1oHIwmLOYc2GB0CPbY6dnsZcII3DkrqYV7mVXmWjQtKW5aTCCoGUYMjv+7UXnpVu+kJzA0W/OfQeek+eQD1O8YSlfQjVcOKBZ52qRuRjLKSnCpNDpfvMaLTl7EYl14UjeUhxeD8UtlJ7CymFkCbp3NLcP21sc6+aVyGlaJcGef/sWCGQEuq9u/XUt3HrqcguKuvLr+e4M+ucpl9D3aYog0MRVM861npSz0xkPFdTLkawzAl4vsC2aEmz2lUtw4GrtZPmp3r/oXMtZWJQv4x6GrY8kNYIKXPnOcJyAhYWcw4rBCws5hxzrw6k0dE6S31pyd+j2bhoQgbApKQG00KIQnMYIoNfQhAzhKU6pmg7meYpOUt1ymQSsBPIZBCo+nZnia+fh9wGCieRdljzc5PgrJQBNtrwDqDR9vDaxjYEi771VJ1/0vYETq/egP9z+3/oHhMLvO/7/xM3XHhq6L41hxmTbp+mH8jtOv3Kzjk9UqXHUHAcrFQja00GUG+20YqlK2fF/qBbUeENSLEySnNe6nkABv0/5QVKescJQLngShXzFgtzLwQEA23fNxqpjANZJX/bLeHi0qGoH/bRKlS0ehNnsGKYkJ77UAYRwVU8L81efWpYtGRPw6wY5XU1eYzmCcsJWFjMOawQsLCYc8y0OtD2fdRb7b56XjNMcSVrA+lUg7w0iDyWekuNy7jtxDejPpmxUruokGXdAaW2ms2NFtlIrsNdy5zB5l1wHFSL0SPHAFqeJ6liFP4b52Bangc/xtm4RCi6rnTdCq5sR8AcOCzJpKPB+1AZKxWZZ/CGVPtRQQBcx0l0/NLaWWJwODTaHs5clj35tFRdMBxPBj0gIelZ2Lt1Bu9//FNSWUFx4DTqxAYjm1HpzUXXwa44ocfAxW1fcioKDHrkEbcVj8FSwcWuqivNqVxwAwIthM+MZk05f+5xbgOenGo9GpSZeYl4m0rBQUELGz1+zLQQAHq85DMIAqMgPKUMo2OrMkCzjuy3zxoXTIbdiX79Ah3Wf7JAGC3BlxWTJ5YsLCx2FBOxEmAOI6qG6ER36Sc1zVFq9Dqd/jtwCpB/HQUg/J1bIajjswjmME1gBByKiC3jO7cqj1+6XmqLPD5Ll830vHRSqkGppxZkmbHJToCV477jThAmQgi0fR+n1qP0XCuViqQ7mlBrtXFxuyYXKkYsvmDpQQWAQ7c5qO6ObkltjXH6ewIsr6SVfpPdg0j+x4hCGbj6dgfFalRn/STj/HO6FNAdiwxjZnyy1IdVJ8MMHcc97Rg4v7Ut7eUvlotYUiICpZ5PgmehOruWJ7Bea3TnSRSMXy5Ej7NDhJVKOVm6K6eaJpR3EPzXIF1iRa7joFIsSN27WXPFjRgTIQQEM2qt6C2sFpMjknlCSG0AA1ljeHOqq4Tlg4qFWMK9SWtFlnSLyQEW9xHKy9SdXz05NWGuUOeZlfRrtD1JCJQTQnL1nI/h4ibNSTCjpYT3qhTlR5nCHYSdgspbEBEKjjORHICKrGnI/oiIThHRE+Hfe2Pf2TRkFhZThKxpyADgL5j5aPh3PwCbhszCYgqRNQ1ZL9yFDGnI1H3Xtq9Hk1XR9HxtuW/a41fr1NZYGqy+BrBQjYUMc0ydmqp3PSEI2xcYrVpUp7mh8w0mPT3NstLIXCgRglTvtsA4SHZO0ceWvQijdr319vRgJUKQHgZch3ydd3rB7YRLfWlGyiTcKVADOhiGE/gwEX0QwGMAfoeZ15AxDdm+qpyB6HK9icv1Zt/BjQIgoQ4AnHjMVyvptgQZPMB6ub9KUXMajJe/5Se+0Hk6tZgK+xkHpTEoMtfLJgYEK5sjoRdh8hUaduRsICKUCi5KGTmQSURWO4FPALgBwFEAZwB8PCw33TvzsxhLQ7ZcGt8FnTyTEguLnUUmIcDMZ5nZZ2YB4FMIlvyATUNmYTF1yKQOENGhTlZiAD8PoLNzMEQashQbuuqSlHsdqPWSf+9VB6KkJr3TU6t14gY10dagVE9rp+vfWt/9p9eFyYBFGt6R4w+R0ipqoxsGGbf3snAZWkQgNsVEksbXIhQxp7rPagbopDrzgKxpyN5JREcRPC3HAfw6gOxpyLjvOwy3BBz6IUJpMbo5m2cFzj03OC/AKTmA5JDSOgmoPTuKEYIp+EWQLy9+DFyxawkF15HKBmcJGBe362i0vViJ3s8j170bx/fd0j3eVb+An37271HxarE2Ss8GR5ztZhstL9LuHSKsLlZRdBO0e4MHJ5tM/WJQ/ItADGw22qi3+ll8AYvlEsrFyPtQMGOz0ZIsT6vFAqql+cqSnTUN2V/3qZ97GjJygOUrSLL089vyg5KWGDQJAL0szcrBEL4Kg5NsahQdhm79lgXMnEiuAsCpXdfhmUN3dI8PbJzA291/CER4v/4hn1vL86X8fC4Rdi/okY2SEPCr5mvba2wG0PI9tP3+AqdcLKAcC13OHBg9xYVA0Z0/d5r5O2MLCwsJVghYWMw5JsJ3AOhPC7IA6msMP7ZEbW6Z9H29p6Q6HUMlRS1NmJHu5AIKdFOZiCPJoCYi2WJEnGa8Q2i0PEP2XkhtSq4LN0WACi1qEGSFfk/tHK669KJ07Aivr0rUPY8+RkZMhKYSNUifG8NXCb3Qg7SvSmakDPrzCEDgbxKPSCwEB8v/tgcK+YwieXB78C8Mgii58IG+9wcAHCcIhjoNJOPECIF+8FrA8YeVKDGK453Js8tkd6Y+lMyGMpPRj/JgEBS9lQEfsq5KxHAUfT8NR3BybcPgFhsdO0Q4vLqMxQG99kzj//jz9+GtL9wfjQOBot8yuOn270e9Yp4QOL+xnYLP1Ld8TK9XcshzPUKxeg0v15vYiPEkBdfBleRiYb0Bpysc6r1NlQjwqmVc2l3FumLRqo5VKRYSPWEnBVMhBABAJBBV2ZHOXMhE4AFZOPvkNoL1DbKkl67XWEnjF0QblMQCpujHBAHJA7lnT6P6tTSRt9J0PIHKZh0FT93A6jFpBgrbDRQcBlfkHYReRPE0wHICFnMLEgLkDx7RxWkn73pPE6wQsLCYc0yMOjBo3jazDYCu76v1hMICCrBG8gjBWl8BCReVORTo5rLxHck56VXnGARSVw3prXr6HdqtGgvpnoBp7Aj2LFSxEuMNWr6Pi9t1Y6RctX/5e31ZT6C+96xRXMC3bnw/Nqp7umXXXHoedxx/UBmrfz/BWP2NyaJ6urViv76FgX84K1w8JcoACLe7dewifaXAkMPhAeG9j43V9Dysx7xFiQiL5eKOBjpJi4kRAsPCfLOTrQNVQdEhCnVWW0g6X5eLTkhzpc8ouc5KpYxycbhb03nogEh3rbfauLRdz+Q0pV1HAxEXR9sp4rkr3oxzK1dL5aoQSKNLG4nCNHNMMjoy7AKd4CKeEWXc7LT6tDM9V/JYni+kHxdCYI2IyZMBVh0YBlleJovJBgNYYxeX2BmY9J1WzMxKwMIiD1TA+Em3hre5tUn80R4JJlMIpOQHegWpjH9O9CkIjVXi9XwhtNRYruNqxjCSUQuF/2jubf3naKoRpEaTK+Xxq9Ty/fCaKLqzOr/Ejfpk0xyHfezZek0qW65f1Ps2Kvz5/AarhliblVXUS0vd46rXxvW1U4hbod3ktOADKCRMQbNJIdJvrPIseELAUbcjE8YhEFxntEZHEykEUu2Ba+QWoBpxmb0DhfzCM8MTsTIOdOe4ZZlDhGVHNfxwQmUq9PFjgB3V8scwAYJGBKr398Sly7K+bRICKQSDKWy5PyAB2x3LcL373alycwv/6vFPQTjR72nBb0GobTTCMUcoff+/Iz+LJ655e/f44PZ53Pq1P0Wlvt4tKycbNoTPmmpgpsxdeRTAjPVa0+A2rhfEi4qug9XF6khVk4kUAqNE0p4Cg7UEpYwehiexG5321TKF6lIREEr9GfugLJmGzOUXJA2bqYDAqLa2MuX5G9UD3yxUsVXe1b2YS+0GmHKixVJMXLAwWJ6q/cj3LCnBTh6wxKDF/IIoWSIbwCNcmo8Dk7MSmCCqPe0tNj4LSQp/nki6Zr0U90HnlfE80jbTnZxGhXCFFQ7ILqG9XAXX+g8aN+IWpQLayxWwF/kORI5hcbuATpck14n1a6JDesVTGSRq0qA2NxMjBJITfeUD1YCEKPD24qgAxUIB5MhRclSPvSBfom5VkxBxrMf+dlpjGdVOob9CTUzmOrGiXdWylr1HG39ktv3Aeq2OtlDdlUbzLNx45rtYrF/qHi96dVykBloLJTixCEzxs20WF/Dda38qIBSJ0F4oYd/Gy7jmxMPReUBfUgfaQXQuQR353BiGHxIlbL3nC2w25OAw6nPnOISFUlEL8tLUfCLMmBghsJNQjX6c2AvOzCgVXBRYeekN7YK+ojqj0mdN/ab5gTd6Q7LMKC5VSlhdqA45w2zwhcBWoylFJBrlj8H1Z5/E9WeflMq2AGwHvtndsvg93ags4L6b34O1pYPhBBlv+cGDuPrVh6V+hMq/hIZI8RdWq2OEfLd9wdist/SwdLGCguMEhkixsqbnY6vZ2+ApjjRpyK4moq8R0TNE9BQR/VZYvoeIHiCiF8L/r8ba2FRkFhZTgjTEoIcgucgbALwVwG+E6cY+AuBBZr4RwIPhsU1FZmExZUgTaPQMggQjYOZNInoGQVahuwC8M6x2L4CvA/hdZExFNi4k7ttCVwX61U0DNSKRiUgwOfH005WNNjfdb+JDsdQNC7MzzE74xhMIjkNSOnCGnk5+pHPQOCKFqGRG2auh0trulhW9ljG0uhaWPc7JGCImqWMBwTUxOXTJdke6Y5RggEScb0gXgh0YkBMIcxK+GcDDAA52cg8w8xkiOhBWS5WKLJ6GbG9l56gJ1duLiOA6iF1lArn6RVZfCjWPfSAoBp8PGxT+LK9fr9tNah2l4qn1DZxej2pVigUc2bcLhR3wdiMCDq+uSGWbjRZOXLo8cF9pnvc0EaM68+qgXLuEX/7GR6VtwSJ7mtGVZkfCgadpfILBjiTFi7TxhWZlBEOIdZlraHk+zm1sKfPRTqsnUr99RLQE4IsAfpuZN/r8UiRtnAUFzJ8E8EkAuH5XdWSi30SoqQkoCOhe+ODGmC169HMmA2HTfz6jNIbJgsCzOrr8gjWbvpGhszOjlu0kku4HMaPcrsmkcEqJn+Ve9yKBE+nEIW5aKmMhIioiEACfYea/D4vPEtGh8PtDAM6F5TYVmYXFFCHN7gAhSDbyDDP/eeyr+wDcE36+B8CXY+V3E1GZiK7DQKnILCwsdhpp1IG3AfgVAN8noifCst8H8McAvkBEHwLwKoBfAJA9FVkOUM1MzMSLopsQSc4xHUsvdXVlzA+Y08pV5x+ApDyHqfsecHG/g5xcTxjDwic2MvWjO/kkNSQKbSniNZxALYjqELRo78rD5pgpvp6ziPdtJh3VWSf7lqRFmt2Bb6L34/6uHm1yT0WWBkbrKwPLLhUZPHpMwkMdYJSaqzBTEjuC8csAPbRbGsGUNeekCqM7gZDLCLqgEKrjT8dmUCEHE1ngHhGSNYI3R8ycxWCWi6Vtx7FO+KVpZzGrmDQ6N19YL0ILiznHzK0E4kjWwCwmFf2W7r2+Mqeci+AWASf2xLMA2k05doTq5ANAixpkMszqtSrs9/z1WltofFSwb9239TDP+cwIAZPRB5Mhk49sMGe+EYbCXjd9VItEKZTZiMZIGnccYA6cZpJmkYYDUC0PD7+hiAPXR498fUvg+1+ro1WP6rmOYrtAhEA0xO6HyYbEcSQuo0sc9xEwDMNSvAebbSK4JQxx2yZGCKgGPLn1m1BmvKHKMYfWXnrF2dMTx71ySrMTkOb5MAmFYpWwsBoziXYZTHJ4eWLFGjSMQamGltfnrYdg1znnZG4hKYLVKGA5AQuLOYcVAhYWc46JUQd2CqbwTUmLy7wJxr59GZa6OzY2gkg2l7brcDsBOAlYKBWxUCr2b5gjTEt5Ux2tjdJIKN6Ra+c8oBCVNWuMet1Dqx01LLkOSHkrXEde6BP0AKBakGLSCURWnddgNkpLMh4L2iWRBOkxkULA5ERiujByhXQvi0OyAREb2mk6J+m046h056z9ZvWiU9H0PPzg4mWp7PDqys4JATalgFOr6JaAqhBgQz8nnmniB09HZb4Q2Gy0pHbVUkGLIFVwHWnJHKRcl/t2DFMWkAlrNbKQkSEwcAJaFUPZMJSBVQcsLOYcVghYWMw5JlMdgK4SJC5ke9j7J+qWxPD17NM7hl7bXXJxOiUhnUqQvj8guA+CWYs+lAZJEYpMS/bOceI2oKLGMevnn8pXwODmkwbmbcJ4vzCu9+UIQQZOwFQnYXq96qTdVpxIIbBroYI9C5WBFZ0s5gX1lofTlzdlw5IUF36U0D3G0jXSjGXSNEw8UcbZy1u4tFXvW0t92Quug+v27e4bzlww4/iFddRaUbhvT8gpvXvNyfTCS1fAwPUQ5EfKdRwslotSX0XXkewEjPyUcUYyOcgwvOTKfr+Rj4LcUWAjoFOIml+SgkE8USdSCBQcB9VScUeizHQiC8neXip5qF/lNDPLIkeyCh+d3MzWzoRmyhj28ftVdJ1UsQJrrTa2mu3EenGYhKQqAo2/7irBy6zlk3AMyT9TPYWmX/20bRPamAyIEvsZwMDIcgIWFnMOKwQsLOYcE6EOuA5hpVLqHielxUqLtu+j3mprelkcnhBYrpSlJX+t1UYrxhamIWfSgAGzMZBhgmqRaXmbVCcVUZoT+bGw28XiqhNlZmIHW60m6u3eS33BjLYvBvYV6X0dB+snSCXX3/5kkFW4fP3lEOMUdqZGujbGHjKkrteMjBImFh87CRMhBIoFF1fv2ZV7v/WWh1cvXtZY5DiWykVcs3c33DA+NDNwcu0yLm035IoGkicPpHlRdX1X95AzklUGq7pRYd+1Dm54S7krBFp1xuP3r2NrbWe2XpLciIGYZ18MarRjh0gKFZ5WFzc6EMWG6wguLdK10r/Qp6gTiqY6epPUGCYN2R8R0SkieiL8e2+szUBpyDpbgurfsGCk2CJEJ6RUOGbHRZiivw6vHP9vVBjjpsTQIALIIZBDutCcBJD5Oev3Nw9IsxLopCH7LhEtA/gOET0QfvcXzPxn8cpKGrIrAXyViG7aqWCjFhYWgyFxJcDMZ5j5u+HnTQCdNGS9cBfCNGTM/AqAThoyCwuLCcQwacjeBuDDRPRBAI8hWC2sIWUasjiano9XLqx3j3dXy9i9UOkux3wh8NrlLYmsM1qDKPBSWLk12h5eOb8eixREWK6WsWcxStfd8oXmVJMJBqs2TePngDCTy0yGQLoOrEcjMvAG2h578nU0QtlzP/V8C5dei4x+hA9sXRYpbAV2bsmtGguVFx3c/OMVFCtR6dkX2jjzvBdrlM5ugKFY+jHDdZRLa3SM0+fIBpfEeL1v3vSv8dKBH+oer9bO4z3HPoNqextZMEwask8A+CiC8/wogI8D+FX0vkZqf91chPsqRdRaEYuseqwxA/W2JxmspI0uo48rH/uCpbEJwOpiBUuVcres0fagQo/+kk0DNpGARiuyPsdRy15HYZmJhMyquCvWKPUtRn1L1vhMRJyho5FyLDKo6+ILAG4B2H3QRWUpWhBvvCZApGuu0v3u8TLrUasHjwiUhvQ7v3Qlju97Q7dmbeMEPCc7x585DRkzn2Vmn5kFgE8hWvKnSkPGzJ9k5juY+Y7lks1cbmExLmROQ9bJQxji5wEcCz/bNGQWFlOEYdKQ/RIRHUWwgjkO4NcBIEsaMrcILB+MFkGOENIS3RfJgSZyg2Et5hBhuVySFthNz0M7yf0wMUIO9zYEip1vqUpY2OV058bM2Ljoo93sbwdg6ldT0Q1zTLWCNUTJ0eugL99A4XUtxOz3W77AdrOVZgbm8bSi3mcjPODyWR+1y9F9rG/ma9fQvUSkG/Bol6dbh+Q6Ckewf/MUjpx/pnu82NzA6d3Xo+Q3E2ZzzFhK4w4xDQBvvG6Bv/CHr+sen3uW8dpTMmE1aLSZtPXUVg6Aq1ZXsBojBk0ur6fWNnFxuy7VMY1leOdiY7P2EjIYKp954PoCbvrxMpxQa/JajCcfrGHtjMyR6J516QhFFamyLxleLrO1Xe/OXIdw2+GDWInxLxe2anj69HlD38kWcmmEgBRYioBCUWkkCFAs9hySxyciOSIxIlsX9Thei0g/D7WfTv9q5/ESzynBj3EAF5cO4Yt3fBi18rLWVxyP//sf+w4z36GWT4TFIAhwS7HTdDiVB1pOQyfXIdIsy/KyIzFsFujjO0ChBDgudaskjW8kGHMU+CYLuSxwiVBwo5WA62TrMw2hZmrkteXzIAIyTmHHUBAtFEW0Ui74LbQKZTSLC5n6sw5EFhZzjslYCTAgfGmdPLXgHp/TtulVQfhRTfbZvOefwqnGXCf6nNZZikCSrqrpst1+YnVITvSaFHlImmMf5oKiSqaJDg1tWT98l7mCwHCFB9cfLC5DBxMhBFrbjFcfjpEz69MpBZL9FHTDIEB/6dR+1s/6ePobje4LJARj+5LsfcfQ9X3V6Ig5mVsBGx5yo66dcNzpJ/bFarWMw6u7ojpk9hgd1ICplyqgeumRYaZJKk3BcXB4zzKKbrSNvdlo4dJ2/0hLaZGJlIUsFJcba3jPsc+g7Zb6tAC+36N8IoSA1wQuvSJfjqSLoV28FGHKTfXGSYyqVmYwHANAbUNg63KMBITOmegvPPcoS56XdiXZcN0g53lUX8TOcfwlKxcLOLR7qe/YpliB3Q7jc1QNc8y9peItVEIvDocIqwvVrrDqXNe8hEAeqLS3ccvp7LvwlhOwsJhzWCFgYTHnmAh1QIVLJAWAZDA8X0wzX9gXkkpCQGWJEI9/2WoCjW1htAPoe6yUeW4J2wv7pKg0ldolFFq1+PAaeqfGik/bECVHbTeInt9PTTOwl4FRjb6sV0nPON3Qqc5KJVmtYTQ9T5o8M6NckE3dEw3HYm2jocyqivo8EENRvkzqUPa3YyKFwO7FCg4sL3aPPSHw6sXLqSLeThcYpjfj6J1VLKxEd/n0C20c+3q9+0AzB5xAvKVghs/yg+r5suC4uOdafOP9/xleMTKE+uGv/hdc89xXpfHViLsE7mHUIp+LWkeNkpPG6pPBySHHmeGoFjSkcxIBlyE1kyoxAw76R/Jt+wLPnbko9bN/eRG3XrlfGuvZMxcSn0/VociYhhyKIOZ0diHDYCKFQEcWdy6Q0TpNOTbuDqUkCycNnQg9UUFuPYPJATuxX7Ecn668DIhGhe4KYOB2+v4DadJlemE5AQuLOYcVAhYWc46JVAdUOETYt7TQX1d0GO7hJqgiGx1deFFus3dpASVXjV8g10mThnt1oYJKMarn+QJnN7f6z5GTHZjAwEvfaaIQ86XYWvODPX+FY4uzAgzZyYlBePmW92Ftz5FuWWNhFb5iUPLq638WawdvjpXIyjUBOHT82zj46mNSOydg4qR6WghvUnRgMF4hB/9QKGG7hxpSWyngwrXBHFeZcZfXQuP8JcmrtHP+qhWfaY8fGhEXXbVShXDktjKKsWt96aSPiyci3b7gOrhy9zKKMf+GquH5YLBOMDJLvkgmYyWNcA3bav1L5yFbZ/ZCWk1vMoSA8rCod5MIWKmW0Q9UAEpHPNBKdLHWTwIXXpKtz1YqpVQveRIWyyUsxqbU9Hyc39qGPwBLa7SJYcZrL8gPvBCKAGADoagY2QginL7mLTh15MfkesqTcf7wm3H+8Jv7zrK8dQH7jz8qlyr3jKC/iOozeBIOPl5awDGnTxCZagmoRqTwuvDwvvUNQHEvppA1j4+lLmu5UzFWEucsCiXClTcVUV6MyrwmS0LAQfADpO4GaFBvJjOYRsOQZIlY1A+TIQQA9KNr0tmXs6Gu+YXMI5T01ISjTppnojtifqTX9x0XTznuQE/wt50C3kgOrs1pDiYTY+r3C2SsM1uwnIDFjsEjSsyco0IQQUzwjsMsYIJWAkOCAW46EPXol8tpE0quajAyHw8UgVGpXcLi5Si8o++W0FjcEwQoGACt8gq2VqJocsQCi9vnAY6nagv0b44KAvOhfguJlBFNNH0bkKPyUGisZDDE0cpi2qEQjPqWkIK4aNGawMb9f3W9mecuYVanIq2flHOaHSEggPbzC9LapuQDR/bKV0JNRT2zYMYtD/0P3Oj8727R+v7X4dE7/wB+zFgoGYTjt74XJ296Z7ek2NzGT/7db6LYikJcC2Y4ceHCDHaC9vE6KvafaaNaF2hWCOevKEEYVG9fBIZP3RkRwPH7GK7x1b17UqKDsJDlX2Ob8b2v1KWyIFZHjCPwBV46d0mJLKS/mIkGTinRyyNylEgUAkRUAfANAOWw/t8x8x8S0R4AfwvgCIIYg78Y5h0AEf0egA8B8AH8JjP/80hmL88UaJNaguK8BjJmRqmxgULsxasv7Rv8J4sIXnkRXjki6/xCxbisV42FNAJLI8+AUkvAKxCaZQeip3xWzaN1azvTy2O0yIvNiQXQrAlD6DB55LbvG0OHxWEqmxak+VlsAvhpZn4TgKMA7iSitwL4CIAHmflGAA+Gx2oasjsB/CURzeuraJEABlBbdNBYcEZrG2vRE2nSkDEzb4WHxfCPEaQbuzcsvxfAB8LPd8GmIbNIAwK2VgrY3F1AY8H+TowLqTiB8Jf8OwBeB+C/MfPDRHSQmc8AADOfIaIDYfWB05DNAgqOg4Mri5JuuNloYrORMXz2kAiWp7LfxcL2Bdz02Ocg3Oi2n7nhJ7Cx74aB+vYLJbxw+91wvejc9p7+PvaffEIyFnJk+s64YbuxmvzyH7/53djmiMdY2DiLa178OtxYsE0oXE/H6KZfJqZehjkm9PM0TAvu/iN1lQm6V2f2VVQqIRDmDThKRLsBfImI3tinumk22p2Q0pBVhzfeGTcKroODK3LUnNPr2FEhoKrfqn5brV3ETd/5rNRme9eVAwsBUSjhxdvvlspufPRz2HtCFgKqhZzmvJXywX3l9e/GuaUj3eP9J5/EoZe/CcSEEEGAJPdzE0ch25KkZUdM3MIsYSCqnJnXAXwdga5/tpOFKPz/ubCaTUM28yDD306OaZEn0qQh2x+uAEBEVQA/A+BZBOnG7gmr3QPgy+Fnm4bMwogrWGCFB8jww4x9LLB7yl11Jx1p1IFDAO4NeQEHwBeY+f8S0UMAvkBEHwLwKoBfAJApDdm8QtaYc+gsfkgEItlYJ9NvKDOc5jbcWPQhEKG9tFfadPdKC2jEtiCJBarNTTgiyuh8q9fGL9e28PlCGb2CY/tuGa3KMkBAhYEPtht4Dg5ekeoUUV/YC6+bbINRbdfgiLh60PkmxSnGPu/0OiONhmSMlZGqVjokCgFm/h4AzcOEmS8CeFePNh8D8LHMs5phqCG3UgmC0DilS2gRhSGn4v2Q9gA5ilcfE0EIRxox+aFn7H/8y9j/2N91S/zyEp774CcgKhEHcvLmd+HstW/p9ldqbOD2B/4Ey+snu3UurW/h8LHncQ85aHZ7l8/+/NW348l3/FswEaoAvssCa6XdUp31/a/Dt97/MVB4QRy/jTc9+je4+vhDsVmHvEjc8cpg5KOSdYHTUf8rki/ysQ8cZrE0OxaD04DwjTeZtKZprMbEk0gvJUwWM+vsN3NifkATCvXLqKyd6h57lWWQsqxvV5bRrkS58LxaFcKRHy9PCPjNNhYBLMJ8/g0PKJZWAceFj4hoikMUyqjFzJgdvxVbFcSQwvwub4+8acSc2NBaWFj0gl0J5IQ0KcAmFikceRLPJO9z1cIEj3CsOYcVAjnBZ8blWkNyklGj4ejQ6StyCFdcV5AjC637uHjKi3EC0MiEoEh2cuE4kRDWIiXX/YGTj6PQirLptCrLeO26H4NfrHTbbF31Rpy//d9064hiBaIgRyjadf4FrJ59tjtesVVDqbnZ/+xJf58XN17Ddcf+QQqx/dr1bwtIxxDV7Yu44sSjID8gHR3hS9xDMOt0y3zVGSgvzaCjiWjEbIoBdHsK+d7mjYkQAuoNm0YVzfMFTq9vSvHn064ESDn5G+4oY2FXpKmder6FS2e8IIY3Qj02rBuHozrvqDG3wVqK9Wuf/QqueeYr3ePLe47gwlVvioQAES7f/HZcvvntfc/hwKuP4Q0PfVqzGNQcbSTnHP3R3nXhZdz2L/9VKtvae0QSAkuXT+K2b/0VirEdC5dI8xpUxzM9V5qwyPHhy0L5jcMwyXICFhZzDisELCzmHBOhDkwjRkn6MQMcD1KxgzwYITD0ITGgfdcoyTplPpR5LF09SdtulmGFQEac3djGei0i1ARDin6TGQw8+U9ytJt2k7t8ABAzDDKEuI4OCeyQRFQSkSZPCnAkgbZr8zTe8aXfgaC4M44pvJf8YpSbmygoRIUx8IZ6vmoBEdS8e2998M/gF6LQzq7fRMVvShyA6zhSq8B5SuUjevMT/TDrdgRWCGSEJ3wp9hzzMCkhIzAz6gqpLkTANXf3EgJzQT2clmIsBEAr018E2aLG9T0pLmFHAJhILkkQhAyb/CJ2vpCrJUGts7B9Xq/jmEhHEzGoSZlML7UaWWiWYDkBC4s5hxUCFhZzDqsOZMRCqQh/IVok+4KxUW/2Tb+tOgt1VsuqcZzWhckwSK2npdwKl+uqsRAMur20vpU18k6LpD1vMkTbUfteLBWxZyGKEMRgXNisoTUgl0IUphiLde4QaTYBJg5AVQW044FmMj7E74dLhMVyUeJAGm0vMVV6B1YIZMTqQhW7Yw900/Ox3WxB+PLbKpnqsPyiRLp9vFAfixDk/uOooW4s1HUO6h5qL6FunxhE5RKKECLWBUzSbohZH4d0cnsXqzh6zRXdY08wHnnlFNa2G9octX6UsShWuyMUZCFA3bpS56rMUzkCMp/LoOiKqB2QKgXXwRUrS3Bj+RLPb2yj6dX7tIq1H9XEZh3qL5/5XptK9ZcpQQZo9Qb0PYz1zQbBBE14qC9GWis2oyBQvo/nfeDQE9HUtWmloRSYVx6ZMCKTwdx76zNObDdkUKtDywlYWMw57EogJ3SWpOretPSrHaro0co6WHurkWxVUoARqg4xtFv97XMotBuYRn87/Rc+uY6xnx1U8NXMVhFvESszzNqk6uh1+p+IOs6gsEIgJxQdF9fu3a3pzoOGtwJ0a0R3Txvu1Y3uk+97jOcfaWDjgpyaS5EToUFR/Jj0wUiJ7kMUCCppQiqlmTbkVfL3jhIROain7/+rbbUXzNFaGdUFlfTMy1nn6j0rKLmxgLka9xAWJiHDdBwiuAbnqbQYJg3ZHwH4dwA6lhy/z8z3h23GkIZsvHAcwlKllFwxA9xdDgpXel0rwnaLUSyrerv8ixEIkmTdntTwO+GhzFPIdXoF7El8DHvo/mYugfrWMZGeqY2F+tTJ+iotlkuoFKfzNzXNrDtpyLaIqAjgm0T0j+F3f8HMfxavrKQhuxLAV4noJhts1MJiMjFMGrJeuAs2DZmFxdRgmDRkPwfgw0T0QQCPAfidMCvxXKYhGyVadYGtM35X6fc9KflOCFlvpzDSMCucgL7fr+v7/Xs2t0izjG55Pi5uRYFAfGZ4QjcUMnEC2lhp1Gt9V3Hg5b5DwFK5BMfpr54MS86NE8OkIfsEgI8ieB4+CuDjAH4VKTfHZy0N2Shx8aSPFx+vSy+07wvlZZE3/Dv7/yon4CRo84F1oEICZnh5op6jlpe2G3jkldPK94BL+oJUt+RTXjpTFCHT+BpvoFsFkFI/3nXRdXHDgT0oK/r+9L7yOjKnIWPms8zsc2Dx8SlES/6B05Ct2DRkyYiHF5jGfb8pByl/s4TMacg6eQhD/DyAY+Fnm4bMwmKKMEwasv9FREcR/EYdB/DrACY+DdkkhgGf5Yy3WZDf5ci0Uz93GCYN2a/0aTOxacjOb9Y07ypNLBgNfrhfFQAwehAmGQLtqpaxf3mx53x7QTUE6lgjqt+bvBbjcKCkRaOOH2FUxpzTq2PgFrRde1OdNH4LUIVHf3sAaPVnCyvVcmq7hYmxbhhlXPU4aq22lA/AtC7QrP6MiUX0NirPbbIeVMskK7OU6Lyiqveh7gikGP0YLO/MziYxc6BelkEZYXYWMhn5qMeGHQKlfpLRUaciKcezBiJCpVhILQSsA5GFxZzDCgELiznHxKgDw4KZ0fL8vpF9ALPePk60fR9bDdnyZ6FU1I1TMGgcgQCJRj5k5jfijUh1KMoRSXv7veqk6SmTf4OBo5h1zJAQAM5ubqPe9pQv5EPfYKGWF1JZSSlYqzWwXmtGfRBwy5X7UXH0W9N9sSnQdWVCz/A2a16DajDvsFfS63CS9MiANOp3t04CBxAvMe3dd8tiX3QNpUipo/QVH3sOZMAMCQEEv/LxF2NHf/QNrDZ3/+mNwG03bgk0wJBqOHHJ8o/DOvJYeieGebPJQzH9vPphIEEwYB0jwZimjmJCOA8vfhyWE7CwmHNYIWBhMeeYWnWg1mpjvRZFqWUw2ilDLMdRWQEOvN6BE27XMwMXXhTYOi/r25NoaahCdYQRqrGQiTZAsgaSSB6mnV/GdbbJJiAJpSrh5reWpcavveBj7fTEGK9ODKZWCLR9HxuNZnLFBBSrhD3XEtxS8LQIn7F5lrB9IaqTpwCQ9XggT95d0/8V/VbV9YMymSxk9KiTGyeQjesf1NqvUCIcurEo5SvcPN/AuubKZmHVAQuLOYcVAhYWc46JVAd8wWh6ft8lXy5pwCcUjbYnGTW1/Gx6rMHHKFn/T1Fn1MjCAWjbnIKxtSaktu0GNKh9F11H8ucouu4suhdImEghsNFoYqupxc+SMGmWf3nipXOXpAfPF6y9zKYnkzULH9WBqIexULwOWAtdPuq3QOvdlF1IM+QzGBPFiprbjMfvl9Nw+W1oUNvtW1rAlbuWpQoFZ7YXzBMpBJiD+HN964z992o0YGZ4yrnPssDLE4rhI5o1YSAUTfkIIriOg2JhviJdzbaIs7CwSMRMC4FBVrHd9GCGABpZHVgsxoxU5sf2vk2kOpAFBGBXtYKCq8g1w0o6rkqUieD/gMBu8DAwA8ueQGE5OZJQvMgXAhe3aiNZuk8CWTeNMPEG8ZKC4+CKXUtSHsHlEWWRmmTMjhCgIFRXtTR4+HJxElJUoCUAS8u9apvR9Dys1+oQ/iheV7MBz07B6BiVU1+jRr9fetd1cHDXIsqFmXkNMmGm1QELC4tkWCFgYTHnsELAwmLOQZPgHUdE5wFsA7iQVHcKsQ/2vKYNs3pu1zLzfrVwIoQAABDRY8x8x7jnkTfseU0fZvncTLDqgIXFnMMKAQuLOcckCYFPjnsCI4I9r+nDLJ+bhonhBCwsLMaDSVoJWFhYjAFjFwJEdCcRPUdELxLRR8Y9n0FBRJ8monNEdCxWtoeIHiCiF8L/r8a++73wXJ8joveMZ9bJIKKriehrRPQMET1FRL8Vlk/1uRFRhYgeIaInw/P6T2H5VJ/XUOAwYcc4/gC4AF4CcD2AEoAnAdwyzjllOIe3A/hhAMdiZX8K4CPh548A+JPw8y3hOZYBXBeeuzvuc+hxXocA/HD4eRnA8+H8p/rcELgvLIWfiwAeBvDWaT+vYf7GvRL4EQAvMvPLzNwC8HkAd415TgOBmb8B4JJSfBeAe8PP9wL4QKz888zcZOZXALyI4BpMHJj5DDN/N/y8CeAZAFdhys+NA2yFh8XwjzHl5zUMxi0ErgJwInZ8Miybdhxk5jNA8DIBOBCWT+X5EtERAG9G8Ks59edGRC4RPQHgHIAHmHkmzisrxi0ETH6es7xdMXXnS0RLAL4I4LeZeaNfVUPZRJ4bM/vMfBTAYQA/QkRv7FN9as4rK8YtBE4CuDp2fBjALKSHOEtEhwAg/P+5sHyqzpeIiggEwGeY+e/D4pk4NwBg5nUAXwdwJ2bovAbFuIXAowBuJKLriKgE4G4A9415TnngPgD3hJ/vAfDlWPndRFQmousA3AjgkTHMLxEUROP4awDPMPOfx76a6nMjov1EtDv8XAXwMwCexZSf11AYNzMJ4L0ImOeXAPzBuOeTYf6fA3AGQBvBr8aHAOwF8CCAF8L/74nV/4PwXJ8D8HPjnn+f8/oJBMve7wF4Ivx777SfG4DbADwentcxAP8xLJ/q8xrmz1oMWljMOcatDlhYWIwZVghYWMw5rBCwsJhzWCFgYTHnsELAwmLOYYWAhcWcwwoBC4s5hxUCFhZzjv8P4e9+XlkMg0EAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"Success! Our upgraded reset function makes the environment now start from the start of the normal game phase, meaning the action space can be consistently the same throughout the game.","metadata":{}},{"cell_type":"markdown","source":"## 4. Training with RL\n\nIn the previous tutorial, we saw how to train an agent with SB3 in single-agent environments. Handling true multi-agent via training separate or shared policies to control all agents requires a few extra things so instead, for the purpose of a tutorial we will treat Lux S2 like a single agent environment by training a policy for one team and letting the other team simply do nothing.\n\nMoreover, we want to define our own reward function to encourage our robots to seek ice, dig it, and return to a factory so it can generate water and survive longer. To do this all, we will just create a custom environment wrapper.\n\n\n","metadata":{}},{"cell_type":"code","source":"import copy\nclass CustomEnvWrapper(gym.Wrapper):\n    def __init__(self, env: gym.Env) -> None:\n        \"\"\"\n        Adds a custom reward and turns the LuxAI_S2 environment into a single-agent environment for easy training\n        \"\"\"\n        super().__init__(env)\n        self.prev_step_metrics = None\n\n    def step(self, action):\n        agent = \"player_0\"\n        opp_agent = \"player_1\"\n\n        opp_factories = self.env.state.factories[opp_agent]\n        for k in opp_factories.keys():\n            factory = opp_factories[k]\n             # set enemy factories to have 1000 water to keep them alive the whole around and treat the game as single-agent\n            factory.cargo.water = 1000\n\n        # submit actions for just one agent to make it single-agent\n        # and save single-agent versions of the data below\n        action = {agent: action}\n        obs, _, done, info = self.env.step(action)\n        obs = obs[agent]\n        done = done[agent]\n        \n        # we collect stats on teams here. These are useful stats that can be used to help generate reward functions\n        stats: StatsStateDict = self.env.state.stats[agent]\n\n        info = dict()\n        metrics = dict()\n        metrics[\"ice_dug\"] = (\n            stats[\"generation\"][\"ice\"][\"HEAVY\"] + stats[\"generation\"][\"ice\"][\"LIGHT\"]\n        )\n        metrics[\"water_produced\"] = stats[\"generation\"][\"water\"]\n\n        # we save these two to see often the agent updates robot action queues and how often enough\n        # power to do so and succeed (less frequent updates = more power is saved)\n        metrics[\"action_queue_updates_success\"] = stats[\"action_queue_updates_success\"]\n        metrics[\"action_queue_updates_total\"] = stats[\"action_queue_updates_total\"]\n\n        # we can save the metrics to info so we can use tensorboard to log them to get a glimpse into how our agent is behaving\n        info[\"metrics\"] = metrics\n\n        reward = 0\n        if self.prev_step_metrics is not None:\n            # we check how much ice and water is produced and reward the agent for generating both\n            ice_dug_this_step = metrics[\"ice_dug\"] - self.prev_step_metrics[\"ice_dug\"]\n            water_produced_this_step = (\n                metrics[\"water_produced\"] - self.prev_step_metrics[\"water_produced\"]\n            )\n            # we reward water production more as it is the most important resource for survival\n            reward = ice_dug_this_step / 100 + water_produced_this_step\n\n        self.prev_step_metrics = copy.deepcopy(metrics)\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        obs = self.env.reset(**kwargs)[\"player_0\"]\n        self.prev_step_metrics = None\n        return obs","metadata":{"execution":{"iopub.status.busy":"2023-02-01T04:22:57.813091Z","iopub.execute_input":"2023-02-01T04:22:57.813559Z","iopub.status.idle":"2023-02-01T04:22:57.831275Z","shell.execute_reply.started":"2023-02-01T04:22:57.81352Z","shell.execute_reply":"2023-02-01T04:22:57.829636Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### 3.1 Defining the Environment and using Wrappers","metadata":{}},{"cell_type":"markdown","source":"Next, we will define a `make_env` function and use it with SB3 to create multiple environments in parallel that scale with the number of CPU cores you have. A future tutorial will show a variant that creates a single jax-powered environment to achieve the same functionality but scaling with GPU.\n\nWe will use the SB3Wrapper, the controller and observation wrapper we defined, and the custom env wrapper as well. These put together will give us an environment that resets to the start of the normal game phase, has a consistent and simplified observation and action space, and contains our reward function.","metadata":{}},{"cell_type":"code","source":"from stable_baselines3.common.vec_env import SubprocVecEnv\nfrom stable_baselines3.common.monitor import Monitor\nfrom gym.wrappers import TimeLimit\ndef make_env(env_id: str, rank: int, seed: int = 0, max_episode_steps=200):\n    def _init() -> gym.Env:\n        # verbose = 0\n        # collect_stats=True lets us track stats like total ice dug during an episode to help create reward functions\n        # max factories set to 2 for simplification and keeping returns consistent as we survive longer \n        # if there are more initial resources\n        env = gym.make(env_id, verbose=0, collect_stats=True, MAX_FACTORIES=2)\n\n        # Add a SB3 wrapper to make it work with SB3 and simplify the action space with the controller\n        # this will remove the bidding phase and factory placement phase. For factory placement we use\n        # the provided place_near_random_ice function which will randomly select an ice tile and place a factory near it.\n        env = SB3Wrapper(\n            env,\n            factory_placement_policy=place_near_random_ice,\n            controller=SimpleUnitDiscreteController(env.env_cfg),\n        )\n        \n        # changes observation to include a few simple features\n        env = SimpleUnitObservationWrapper(\n            env\n        )\n        \n        # convert to single agent, adds our reward\n        env = CustomEnvWrapper(env)  \n        \n        # Add a timelimit to the environment, which can truncate episodes, speed up training\n        env = TimeLimit(\n            env, max_episode_steps=max_episode_steps\n        )\n        env = Monitor(env) # for SB3 to allow it to record metrics\n        env.reset(seed=seed + rank)\n        set_random_seed(seed)\n        return env\n\n    return _init","metadata":{"execution":{"iopub.status.busy":"2023-02-01T04:22:57.833143Z","iopub.execute_input":"2023-02-01T04:22:57.833655Z","iopub.status.idle":"2023-02-01T04:23:00.257303Z","shell.execute_reply.started":"2023-02-01T04:22:57.833612Z","shell.execute_reply":"2023-02-01T04:23:00.255777Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Next we will define a useful callback function to log some of the custom metrics we defined earlier in the CustomEnvWrapper","metadata":{}},{"cell_type":"code","source":"from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\nclass TensorboardCallback(BaseCallback):\n    def __init__(self, tag: str, verbose=0):\n        super().__init__(verbose)\n        self.tag = tag\n\n    def _on_step(self) -> bool:\n        c = 0\n\n        for i, done in enumerate(self.locals[\"dones\"]):\n            if done:\n                info = self.locals[\"infos\"][i]\n                c += 1\n                for k in info[\"metrics\"]:\n                    stat = info[\"metrics\"][k]\n                    self.logger.record_mean(f\"{self.tag}/{k}\", stat)\n        return True","metadata":{"execution":{"iopub.status.busy":"2023-02-01T04:23:00.259116Z","iopub.execute_input":"2023-02-01T04:23:00.260056Z","iopub.status.idle":"2023-02-01T04:23:00.273055Z","shell.execute_reply.started":"2023-02-01T04:23:00.260003Z","shell.execute_reply":"2023-02-01T04:23:00.270232Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Training Setup\n\nNow we can prepare for training by creating training and evaluation environments, as well as defining our algorithm and model.","metadata":{}},{"cell_type":"code","source":"import os.path as osp\nfrom stable_baselines3.common.utils import set_random_seed\nfrom stable_baselines3.ppo import PPO\n\nset_random_seed(42)\nlog_path = \"logs/exp_1\"\nnum_envs = 4\n\n# set max episode steps to 200 for training environments to train faster\nenv = SubprocVecEnv([make_env(\"LuxAI_S2-v0\", i, max_episode_steps=200) for i in range(num_envs)])\nenv.reset()\n# set max episode steps to 1000 to match original environment\neval_env = SubprocVecEnv([make_env(\"LuxAI_S2-v0\", i, max_episode_steps=1000) for i in range(4)])\neval_env.reset()\nrollout_steps = 4000\npolicy_kwargs = dict(net_arch=(128, 128))\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    n_steps=rollout_steps // num_envs,\n    batch_size=800,\n    learning_rate=3e-4,\n    policy_kwargs=policy_kwargs,\n    verbose=1,\n    n_epochs=2,\n    target_kl=0.05,\n    gamma=0.99,\n    tensorboard_log=osp.join(log_path),\n)\n\neval_callback = EvalCallback(\n    eval_env,\n    best_model_save_path=osp.join(log_path, \"models\"),\n    log_path=osp.join(log_path, \"eval_logs\"),\n    eval_freq=24_000,\n    deterministic=False,\n    render=False,\n    n_eval_episodes=5,\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T04:23:00.275059Z","iopub.execute_input":"2023-02-01T04:23:00.275512Z","iopub.status.idle":"2023-02-01T04:23:10.001359Z","shell.execute_reply.started":"2023-02-01T04:23:00.275472Z","shell.execute_reply":"2023-02-01T04:23:09.999882Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Using cpu device\n","output_type":"stream"}]},{"cell_type":"markdown","source":"With our callback functions and model defined, we can now begin training using `model.learn`. If you want to skip this training you can also just use the pretrained model that's in the downloaded files called `best_model.zip`.\n\nTo track the progress we recommend using tensorboard which you can run with\n```\ntensorboard --logdir logs\n```","metadata":{}},{"cell_type":"code","source":"total_timesteps = 10_000_000\nmodel.learn(\n    total_timesteps,\n    callback=[TensorboardCallback(tag=\"train_metrics\"), eval_callback],\n)\nmodel.save(osp.join(log_path, \"models/latest_model\"))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-01T04:23:10.003419Z","iopub.execute_input":"2023-02-01T04:23:10.003819Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Logging to logs/exp_1/PPO_1\n----------------------------------------------\n| rollout/                        |          |\n|    ep_len_mean                  | 200      |\n|    ep_rew_mean                  | 0        |\n| time/                           |          |\n|    fps                          | 407      |\n|    iterations                   | 1        |\n|    time_elapsed                 | 9        |\n|    total_timesteps              | 4000     |\n| train_metrics/                  |          |\n|    action_queue_updates_success | 145      |\n|    action_queue_updates_total   | 177      |\n|    ice_dug                      | 0        |\n|    water_produced               | 0        |\n----------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.02          |\n| time/                           |               |\n|    fps                          | 548           |\n|    iterations                   | 2             |\n|    time_elapsed                 | 14            |\n|    total_timesteps              | 8000          |\n| train/                          |               |\n|    approx_kl                    | 0.00025716395 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.48         |\n|    explained_variance           | 0.843         |\n|    learning_rate                | 0.0003        |\n|    loss                         | -0.000655     |\n|    n_updates                    | 2             |\n|    policy_gradient_loss         | -0.000534     |\n|    value_loss                   | 0.000729      |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 146           |\n|    action_queue_updates_total   | 176           |\n|    ice_dug                      | 4             |\n|    water_produced               | 0             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.0133        |\n| time/                           |               |\n|    fps                          | 620           |\n|    iterations                   | 3             |\n|    time_elapsed                 | 19            |\n|    total_timesteps              | 12000         |\n| train/                          |               |\n|    approx_kl                    | 9.9918936e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.48         |\n|    explained_variance           | 0.363         |\n|    learning_rate                | 0.0003        |\n|    loss                         | -0.00112      |\n|    n_updates                    | 4             |\n|    policy_gradient_loss         | -0.000431     |\n|    value_loss                   | 0.000874      |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 177           |\n|    ice_dug                      | 0             |\n|    water_produced               | 0             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.075        |\n| time/                           |              |\n|    fps                          | 654          |\n|    iterations                   | 4            |\n|    time_elapsed                 | 24           |\n|    total_timesteps              | 16000        |\n| train/                          |              |\n|    approx_kl                    | 0.0001520467 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.48        |\n|    explained_variance           | 0.945        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00105     |\n|    n_updates                    | 6            |\n|    policy_gradient_loss         | -0.000384    |\n|    value_loss                   | 2.93e-05     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 177          |\n|    ice_dug                      | 1            |\n|    water_produced               | 0.25         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.06          |\n| time/                           |               |\n|    fps                          | 683           |\n|    iterations                   | 5             |\n|    time_elapsed                 | 29            |\n|    total_timesteps              | 20000         |\n| train/                          |               |\n|    approx_kl                    | 0.00014432374 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.48         |\n|    explained_variance           | 0.000437      |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.0395        |\n|    n_updates                    | 8             |\n|    policy_gradient_loss         | -0.00028      |\n|    value_loss                   | 0.0566        |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 140           |\n|    action_queue_updates_total   | 177           |\n|    ice_dug                      | 0             |\n|    water_produced               | 0             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.06         |\n| time/                           |              |\n|    fps                          | 704          |\n|    iterations                   | 6            |\n|    time_elapsed                 | 34           |\n|    total_timesteps              | 24000        |\n| train/                          |              |\n|    approx_kl                    | 7.736915e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.48        |\n|    explained_variance           | 0.926        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00122     |\n|    n_updates                    | 10           |\n|    policy_gradient_loss         | -0.000353    |\n|    value_loss                   | 0.000268     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 149          |\n|    action_queue_updates_total   | 177          |\n|    ice_dug                      | 0            |\n|    water_produced               | 0            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.052         |\n| time/                           |               |\n|    fps                          | 718           |\n|    iterations                   | 7             |\n|    time_elapsed                 | 38            |\n|    total_timesteps              | 28000         |\n| train/                          |               |\n|    approx_kl                    | 0.00023715764 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.48         |\n|    explained_variance           | 0.824         |\n|    learning_rate                | 0.0003        |\n|    loss                         | -0.00399      |\n|    n_updates                    | 12            |\n|    policy_gradient_loss         | -0.00151      |\n|    value_loss                   | 5.99e-05      |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 145           |\n|    action_queue_updates_total   | 177           |\n|    ice_dug                      | 0             |\n|    water_produced               | 0             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.056        |\n| time/                           |              |\n|    fps                          | 729          |\n|    iterations                   | 8            |\n|    time_elapsed                 | 43           |\n|    total_timesteps              | 32000        |\n| train/                          |              |\n|    approx_kl                    | 0.0005054677 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.48        |\n|    explained_variance           | 0.924        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00543     |\n|    n_updates                    | 14           |\n|    policy_gradient_loss         | -0.00253     |\n|    value_loss                   | 1.45e-05     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 138          |\n|    action_queue_updates_total   | 175          |\n|    ice_dug                      | 2            |\n|    water_produced               | 0            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.008         |\n| time/                           |               |\n|    fps                          | 741           |\n|    iterations                   | 9             |\n|    time_elapsed                 | 48            |\n|    total_timesteps              | 36000         |\n| train/                          |               |\n|    approx_kl                    | 0.00033363188 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.48         |\n|    explained_variance           | 0.33          |\n|    learning_rate                | 0.0003        |\n|    loss                         | -0.00186      |\n|    n_updates                    | 16            |\n|    policy_gradient_loss         | -0.000427     |\n|    value_loss                   | 0.000187      |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 175           |\n|    ice_dug                      | 2             |\n|    water_produced               | 0             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.008         |\n| time/                           |               |\n|    fps                          | 748           |\n|    iterations                   | 10            |\n|    time_elapsed                 | 53            |\n|    total_timesteps              | 40000         |\n| train/                          |               |\n|    approx_kl                    | 0.00030649855 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.48         |\n|    explained_variance           | 0.2           |\n|    learning_rate                | 0.0003        |\n|    loss                         | -0.0022       |\n|    n_updates                    | 18            |\n|    policy_gradient_loss         | -0.000971     |\n|    value_loss                   | 0.000312      |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 142           |\n|    action_queue_updates_total   | 175           |\n|    ice_dug                      | 0             |\n|    water_produced               | 0             |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 0.01        |\n| time/                           |             |\n|    fps                          | 752         |\n|    iterations                   | 11          |\n|    time_elapsed                 | 58          |\n|    total_timesteps              | 44000       |\n| train/                          |             |\n|    approx_kl                    | 0.000877353 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.47       |\n|    explained_variance           | 0.931       |\n|    learning_rate                | 0.0003      |\n|    loss                         | -0.00712    |\n|    n_updates                    | 20          |\n|    policy_gradient_loss         | -0.00295    |\n|    value_loss                   | 6.81e-06    |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 132         |\n|    action_queue_updates_total   | 174         |\n|    ice_dug                      | 1           |\n|    water_produced               | 0           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.01         |\n| time/                           |              |\n|    fps                          | 758          |\n|    iterations                   | 12           |\n|    time_elapsed                 | 63           |\n|    total_timesteps              | 48000        |\n| train/                          |              |\n|    approx_kl                    | 0.0011578674 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.46        |\n|    explained_variance           | 0.371        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00181     |\n|    n_updates                    | 22           |\n|    policy_gradient_loss         | -0.000925    |\n|    value_loss                   | 7.56e-05     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 140          |\n|    action_queue_updates_total   | 174          |\n|    ice_dug                      | 0            |\n|    water_produced               | 0            |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 0.006       |\n| time/                           |             |\n|    fps                          | 762         |\n|    iterations                   | 13          |\n|    time_elapsed                 | 68          |\n|    total_timesteps              | 52000       |\n| train/                          |             |\n|    approx_kl                    | 0.002232566 |\n|    clip_fraction                | 0.003       |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.44       |\n|    explained_variance           | 0.898       |\n|    learning_rate                | 0.0003      |\n|    loss                         | -0.0102     |\n|    n_updates                    | 24          |\n|    policy_gradient_loss         | -0.00382    |\n|    value_loss                   | 5.88e-06    |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 132         |\n|    action_queue_updates_total   | 175         |\n|    ice_dug                      | 0           |\n|    water_produced               | 0           |\n-------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 200        |\n|    ep_rew_mean                  | 0.002      |\n| time/                           |            |\n|    fps                          | 766        |\n|    iterations                   | 14         |\n|    time_elapsed                 | 73         |\n|    total_timesteps              | 56000      |\n| train/                          |            |\n|    approx_kl                    | 0.00616081 |\n|    clip_fraction                | 0.0591     |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -2.4       |\n|    explained_variance           | 0.908      |\n|    learning_rate                | 0.0003     |\n|    loss                         | -0.00325   |\n|    n_updates                    | 26         |\n|    policy_gradient_loss         | -0.00513   |\n|    value_loss                   | 3.88e-06   |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 121        |\n|    action_queue_updates_total   | 171        |\n|    ice_dug                      | 0          |\n|    water_produced               | 0          |\n------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.004        |\n| time/                           |              |\n|    fps                          | 772          |\n|    iterations                   | 15           |\n|    time_elapsed                 | 77           |\n|    total_timesteps              | 60000        |\n| train/                          |              |\n|    approx_kl                    | 0.0025576144 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.36        |\n|    explained_variance           | 0.844        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00746     |\n|    n_updates                    | 28           |\n|    policy_gradient_loss         | -0.00235     |\n|    value_loss                   | 4e-06        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 118          |\n|    action_queue_updates_total   | 168          |\n|    ice_dug                      | 1            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.056        |\n| time/                           |              |\n|    fps                          | 776          |\n|    iterations                   | 16           |\n|    time_elapsed                 | 82           |\n|    total_timesteps              | 64000        |\n| train/                          |              |\n|    approx_kl                    | 0.0026448176 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.29        |\n|    explained_variance           | 0.163        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.00305      |\n|    n_updates                    | 30           |\n|    policy_gradient_loss         | 0.000345     |\n|    value_loss                   | 8.15e-05     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 120          |\n|    action_queue_updates_total   | 166          |\n|    ice_dug                      | 2            |\n|    water_produced               | 0.25         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.056         |\n| time/                           |               |\n|    fps                          | 775           |\n|    iterations                   | 17            |\n|    time_elapsed                 | 87            |\n|    total_timesteps              | 68000         |\n| train/                          |               |\n|    approx_kl                    | 0.00060837384 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.29         |\n|    explained_variance           | 0.000745      |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.0319        |\n|    n_updates                    | 32            |\n|    policy_gradient_loss         | -0.00047      |\n|    value_loss                   | 0.0473        |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 118           |\n|    action_queue_updates_total   | 164           |\n|    ice_dug                      | 0             |\n|    water_produced               | 0             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.056        |\n| time/                           |              |\n|    fps                          | 778          |\n|    iterations                   | 18           |\n|    time_elapsed                 | 92           |\n|    total_timesteps              | 72000        |\n| train/                          |              |\n|    approx_kl                    | 0.0003276844 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.24        |\n|    explained_variance           | 0.891        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.000166    |\n|    n_updates                    | 34           |\n|    policy_gradient_loss         | -3.8e-05     |\n|    value_loss                   | 0.000133     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 117          |\n|    action_queue_updates_total   | 165          |\n|    ice_dug                      | 0            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.056        |\n| time/                           |              |\n|    fps                          | 781          |\n|    iterations                   | 19           |\n|    time_elapsed                 | 97           |\n|    total_timesteps              | 76000        |\n| train/                          |              |\n|    approx_kl                    | 0.0017416099 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.31        |\n|    explained_variance           | 0.722        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00485     |\n|    n_updates                    | 36           |\n|    policy_gradient_loss         | -0.00195     |\n|    value_loss                   | 6.42e-05     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 127          |\n|    action_queue_updates_total   | 167          |\n|    ice_dug                      | 0            |\n|    water_produced               | 0            |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 0.054       |\n| time/                           |             |\n|    fps                          | 783         |\n|    iterations                   | 20          |\n|    time_elapsed                 | 102         |\n|    total_timesteps              | 80000       |\n| train/                          |             |\n|    approx_kl                    | 0.003034997 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.32       |\n|    explained_variance           | 0.86        |\n|    learning_rate                | 0.0003      |\n|    loss                         | -0.00174    |\n|    n_updates                    | 38          |\n|    policy_gradient_loss         | -0.00257    |\n|    value_loss                   | 2.84e-05    |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 125         |\n|    action_queue_updates_total   | 168         |\n|    ice_dug                      | 0           |\n|    water_produced               | 0           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.002        |\n| time/                           |              |\n|    fps                          | 784          |\n|    iterations                   | 21           |\n|    time_elapsed                 | 107          |\n|    total_timesteps              | 84000        |\n| train/                          |              |\n|    approx_kl                    | 0.0010488753 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.37        |\n|    explained_variance           | 0.903        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.0017       |\n|    n_updates                    | 40           |\n|    policy_gradient_loss         | 0.00017      |\n|    value_loss                   | 1.63e-05     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 169          |\n|    ice_dug                      | 1            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.002        |\n| time/                           |              |\n|    fps                          | 784          |\n|    iterations                   | 22           |\n|    time_elapsed                 | 112          |\n|    total_timesteps              | 88000        |\n| train/                          |              |\n|    approx_kl                    | 0.0002110318 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.37        |\n|    explained_variance           | 0.491        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.000476    |\n|    n_updates                    | 42           |\n|    policy_gradient_loss         | -0.000178    |\n|    value_loss                   | 8.75e-05     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 169          |\n|    ice_dug                      | 0            |\n|    water_produced               | 0            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 0.006         |\n| time/                           |               |\n|    fps                          | 785           |\n|    iterations                   | 23            |\n|    time_elapsed                 | 117           |\n|    total_timesteps              | 92000         |\n| train/                          |               |\n|    approx_kl                    | 0.00034651073 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.37         |\n|    explained_variance           | 0.894         |\n|    learning_rate                | 0.0003        |\n|    loss                         | -0.00256      |\n|    n_updates                    | 44            |\n|    policy_gradient_loss         | -0.00077      |\n|    value_loss                   | 8.5e-06       |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 168           |\n|    ice_dug                      | 2             |\n|    water_produced               | 0             |\n---------------------------------------------------\nEval num_timesteps=96000, episode_reward=0.04 +/- 0.08\nEpisode length: 301.00 +/- 0.00\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 301           |\n|    mean_reward                  | 0.04          |\n| time/                           |               |\n|    total_timesteps              | 96000         |\n| train/                          |               |\n|    approx_kl                    | 0.00035094516 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.35         |\n|    explained_variance           | 0.468         |\n|    learning_rate                | 0.0003        |\n|    loss                         | -0.00106      |\n|    n_updates                    | 46            |\n|    policy_gradient_loss         | -0.000159     |\n|    value_loss                   | 0.000198      |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 168           |\n|    ice_dug                      | 1             |\n|    water_produced               | 0             |\n---------------------------------------------------\nNew best mean reward!\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 0.008    |\n| time/              |          |\n|    fps             | 763      |\n|    iterations      | 24       |\n|    time_elapsed    | 125      |\n|    total_timesteps | 96000    |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.028        |\n| time/                           |              |\n|    fps                          | 762          |\n|    iterations                   | 25           |\n|    time_elapsed                 | 131          |\n|    total_timesteps              | 100000       |\n| train/                          |              |\n|    approx_kl                    | 0.0008710647 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.37        |\n|    explained_variance           | 0.245        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00485     |\n|    n_updates                    | 48           |\n|    policy_gradient_loss         | -0.00137     |\n|    value_loss                   | 8.9e-05      |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 129          |\n|    action_queue_updates_total   | 166          |\n|    ice_dug                      | 10           |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.034        |\n| time/                           |              |\n|    fps                          | 763          |\n|    iterations                   | 26           |\n|    time_elapsed                 | 136          |\n|    total_timesteps              | 104000       |\n| train/                          |              |\n|    approx_kl                    | 0.0008234091 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.38        |\n|    explained_variance           | 0.131        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.00152      |\n|    n_updates                    | 50           |\n|    policy_gradient_loss         | -0.000816    |\n|    value_loss                   | 0.00182      |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 129          |\n|    action_queue_updates_total   | 167          |\n|    ice_dug                      | 4            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.046        |\n| time/                           |              |\n|    fps                          | 762          |\n|    iterations                   | 27           |\n|    time_elapsed                 | 141          |\n|    total_timesteps              | 108000       |\n| train/                          |              |\n|    approx_kl                    | 0.0008310125 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.37        |\n|    explained_variance           | 0.237        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00501     |\n|    n_updates                    | 52           |\n|    policy_gradient_loss         | -0.00122     |\n|    value_loss                   | 0.000898     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 167          |\n|    ice_dug                      | 6            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.05         |\n| time/                           |              |\n|    fps                          | 762          |\n|    iterations                   | 28           |\n|    time_elapsed                 | 146          |\n|    total_timesteps              | 112000       |\n| train/                          |              |\n|    approx_kl                    | 0.0013095809 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.35        |\n|    explained_variance           | 0.289        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.000849     |\n|    n_updates                    | 54           |\n|    policy_gradient_loss         | -0.00138     |\n|    value_loss                   | 0.000843     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 122          |\n|    action_queue_updates_total   | 166          |\n|    ice_dug                      | 4            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.07         |\n| time/                           |              |\n|    fps                          | 759          |\n|    iterations                   | 29           |\n|    time_elapsed                 | 152          |\n|    total_timesteps              | 116000       |\n| train/                          |              |\n|    approx_kl                    | 0.0017493216 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.32        |\n|    explained_variance           | 0.195        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.000718     |\n|    n_updates                    | 56           |\n|    policy_gradient_loss         | -0.000558    |\n|    value_loss                   | 0.000536     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 129          |\n|    action_queue_updates_total   | 167          |\n|    ice_dug                      | 11           |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.052        |\n| time/                           |              |\n|    fps                          | 759          |\n|    iterations                   | 30           |\n|    time_elapsed                 | 157          |\n|    total_timesteps              | 120000       |\n| train/                          |              |\n|    approx_kl                    | 0.0013841867 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.28        |\n|    explained_variance           | 0.134        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.00214      |\n|    n_updates                    | 58           |\n|    policy_gradient_loss         | -0.000754    |\n|    value_loss                   | 0.00445      |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 169          |\n|    ice_dug                      | 1            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.104        |\n| time/                           |              |\n|    fps                          | 761          |\n|    iterations                   | 31           |\n|    time_elapsed                 | 162          |\n|    total_timesteps              | 124000       |\n| train/                          |              |\n|    approx_kl                    | 0.0016302329 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.27        |\n|    explained_variance           | 0.772        |\n|    learning_rate                | 0.0003       |\n|    loss                         | -0.00248     |\n|    n_updates                    | 60           |\n|    policy_gradient_loss         | -0.00223     |\n|    value_loss                   | 0.000326     |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 163          |\n|    ice_dug                      | 5            |\n|    water_produced               | 0.25         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 0.18        |\n| time/                           |             |\n|    fps                          | 761         |\n|    iterations                   | 32          |\n|    time_elapsed                 | 168         |\n|    total_timesteps              | 128000      |\n| train/                          |             |\n|    approx_kl                    | 0.000806186 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.23       |\n|    explained_variance           | 0.0181      |\n|    learning_rate                | 0.0003      |\n|    loss                         | 0.0375      |\n|    n_updates                    | 62          |\n|    policy_gradient_loss         | 0.000137    |\n|    value_loss                   | 0.0598      |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 140         |\n|    action_queue_updates_total   | 164         |\n|    ice_dug                      | 19          |\n|    water_produced               | 0.25        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 0.318       |\n| time/                           |             |\n|    fps                          | 762         |\n|    iterations                   | 33          |\n|    time_elapsed                 | 173         |\n|    total_timesteps              | 132000      |\n| train/                          |             |\n|    approx_kl                    | 0.001065569 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.22       |\n|    explained_variance           | 0.0271      |\n|    learning_rate                | 0.0003      |\n|    loss                         | 0.0328      |\n|    n_updates                    | 64          |\n|    policy_gradient_loss         | -0.00134    |\n|    value_loss                   | 0.0623      |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 135         |\n|    action_queue_updates_total   | 163         |\n|    ice_dug                      | 48          |\n|    water_produced               | 0.25        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.36         |\n| time/                           |              |\n|    fps                          | 763          |\n|    iterations                   | 34           |\n|    time_elapsed                 | 178          |\n|    total_timesteps              | 136000       |\n| train/                          |              |\n|    approx_kl                    | 0.0014423163 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.21        |\n|    explained_variance           | -0.0109      |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.0462       |\n|    n_updates                    | 66           |\n|    policy_gradient_loss         | -7.75e-05    |\n|    value_loss                   | 0.0869       |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 162          |\n|    ice_dug                      | 32           |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.602        |\n| time/                           |              |\n|    fps                          | 763          |\n|    iterations                   | 35           |\n|    time_elapsed                 | 183          |\n|    total_timesteps              | 140000       |\n| train/                          |              |\n|    approx_kl                    | 0.0017468471 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.16        |\n|    explained_variance           | 0.202        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.00301      |\n|    n_updates                    | 68           |\n|    policy_gradient_loss         | -0.00102     |\n|    value_loss                   | 0.00888      |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 161          |\n|    ice_dug                      | 22           |\n|    water_produced               | 1            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.682        |\n| time/                           |              |\n|    fps                          | 764          |\n|    iterations                   | 36           |\n|    time_elapsed                 | 188          |\n|    total_timesteps              | 144000       |\n| train/                          |              |\n|    approx_kl                    | 0.0016584389 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.09        |\n|    explained_variance           | 0.0387       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.208        |\n|    n_updates                    | 70           |\n|    policy_gradient_loss         | -0.000939    |\n|    value_loss                   | 0.343        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 140          |\n|    action_queue_updates_total   | 158          |\n|    ice_dug                      | 20           |\n|    water_produced               | 0.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.742        |\n| time/                           |              |\n|    fps                          | 765          |\n|    iterations                   | 37           |\n|    time_elapsed                 | 193          |\n|    total_timesteps              | 148000       |\n| train/                          |              |\n|    approx_kl                    | 0.0010056595 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.05        |\n|    explained_variance           | 0.201        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.0601       |\n|    n_updates                    | 72           |\n|    policy_gradient_loss         | -0.000795    |\n|    value_loss                   | 0.128        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 140          |\n|    action_queue_updates_total   | 160          |\n|    ice_dug                      | 24           |\n|    water_produced               | 0.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 0.848        |\n| time/                           |              |\n|    fps                          | 767          |\n|    iterations                   | 38           |\n|    time_elapsed                 | 198          |\n|    total_timesteps              | 152000       |\n| train/                          |              |\n|    approx_kl                    | 0.0016303925 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.06        |\n|    explained_variance           | 0.0882       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.15         |\n|    n_updates                    | 74           |\n|    policy_gradient_loss         | -0.000931    |\n|    value_loss                   | 0.203        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 138          |\n|    action_queue_updates_total   | 158          |\n|    ice_dug                      | 26           |\n|    water_produced               | 1            |\n--------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 200        |\n|    ep_rew_mean                  | 0.898      |\n| time/                           |            |\n|    fps                          | 769        |\n|    iterations                   | 39         |\n|    time_elapsed                 | 202        |\n|    total_timesteps              | 156000     |\n| train/                          |            |\n|    approx_kl                    | 0.00164708 |\n|    clip_fraction                | 0          |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -2.03      |\n|    explained_variance           | 0.02       |\n|    learning_rate                | 0.0003     |\n|    loss                         | 0.107      |\n|    n_updates                    | 76         |\n|    policy_gradient_loss         | -0.00143   |\n|    value_loss                   | 0.235      |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 138        |\n|    action_queue_updates_total   | 157        |\n|    ice_dug                      | 7          |\n|    water_produced               | 0.5        |\n------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 1.8         |\n| time/                           |             |\n|    fps                          | 770         |\n|    iterations                   | 40          |\n|    time_elapsed                 | 207         |\n|    total_timesteps              | 160000      |\n| train/                          |             |\n|    approx_kl                    | 0.003974599 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.92       |\n|    explained_variance           | 0.0994      |\n|    learning_rate                | 0.0003      |\n|    loss                         | 0.103       |\n|    n_updates                    | 78          |\n|    policy_gradient_loss         | -0.00125    |\n|    value_loss                   | 0.23        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 134         |\n|    action_queue_updates_total   | 152         |\n|    ice_dug                      | 75          |\n|    water_produced               | 5           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 3.27         |\n| time/                           |              |\n|    fps                          | 771          |\n|    iterations                   | 41           |\n|    time_elapsed                 | 212          |\n|    total_timesteps              | 164000       |\n| train/                          |              |\n|    approx_kl                    | 0.0026435042 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.88        |\n|    explained_variance           | 0.0301       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 2.19         |\n|    n_updates                    | 80           |\n|    policy_gradient_loss         | -0.00119     |\n|    value_loss                   | 5.14         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 146          |\n|    ice_dug                      | 151          |\n|    water_produced               | 6.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 4.82         |\n| time/                           |              |\n|    fps                          | 771          |\n|    iterations                   | 42           |\n|    time_elapsed                 | 217          |\n|    total_timesteps              | 168000       |\n| train/                          |              |\n|    approx_kl                    | 0.0008459856 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.76        |\n|    explained_variance           | 0.0403       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 3.78         |\n|    n_updates                    | 82           |\n|    policy_gradient_loss         | -0.000208    |\n|    value_loss                   | 8.27         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 123          |\n|    action_queue_updates_total   | 138          |\n|    ice_dug                      | 101          |\n|    water_produced               | 7.5          |\n--------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 200        |\n|    ep_rew_mean                  | 5.5        |\n| time/                           |            |\n|    fps                          | 772        |\n|    iterations                   | 43         |\n|    time_elapsed                 | 222        |\n|    total_timesteps              | 172000     |\n| train/                          |            |\n|    approx_kl                    | 0.00048582 |\n|    clip_fraction                | 0          |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -1.73      |\n|    explained_variance           | 0.0321     |\n|    learning_rate                | 0.0003     |\n|    loss                         | 5.85       |\n|    n_updates                    | 84         |\n|    policy_gradient_loss         | -0.000748  |\n|    value_loss                   | 12.4       |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 120        |\n|    action_queue_updates_total   | 144        |\n|    ice_dug                      | 64         |\n|    water_produced               | 4          |\n------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 5.76          |\n| time/                           |               |\n|    fps                          | 774           |\n|    iterations                   | 44            |\n|    time_elapsed                 | 227           |\n|    total_timesteps              | 176000        |\n| train/                          |               |\n|    approx_kl                    | 0.00069236057 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.8          |\n|    explained_variance           | 0.106         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 2.9           |\n|    n_updates                    | 86            |\n|    policy_gradient_loss         | -0.000522     |\n|    value_loss                   | 4.81          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 148           |\n|    ice_dug                      | 40            |\n|    water_produced               | 1.5           |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 8.15         |\n| time/                           |              |\n|    fps                          | 775          |\n|    iterations                   | 45           |\n|    time_elapsed                 | 232          |\n|    total_timesteps              | 180000       |\n| train/                          |              |\n|    approx_kl                    | 0.0015396948 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.82        |\n|    explained_variance           | 0.17         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 1.29         |\n|    n_updates                    | 88           |\n|    policy_gradient_loss         | -0.000787    |\n|    value_loss                   | 2.14         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 147          |\n|    ice_dug                      | 145          |\n|    water_produced               | 16.2         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 7.65        |\n| time/                           |             |\n|    fps                          | 776         |\n|    iterations                   | 46          |\n|    time_elapsed                 | 237         |\n|    total_timesteps              | 184000      |\n| train/                          |             |\n|    approx_kl                    | 0.000665737 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.86       |\n|    explained_variance           | 0.0594      |\n|    learning_rate                | 0.0003      |\n|    loss                         | 23.4        |\n|    n_updates                    | 90          |\n|    policy_gradient_loss         | -0.000483   |\n|    value_loss                   | 38.3        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 140         |\n|    action_queue_updates_total   | 154         |\n|    ice_dug                      | 75          |\n|    water_produced               | 4.75        |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.28          |\n| time/                           |               |\n|    fps                          | 777           |\n|    iterations                   | 47            |\n|    time_elapsed                 | 241           |\n|    total_timesteps              | 188000        |\n| train/                          |               |\n|    approx_kl                    | 0.00020301346 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.86         |\n|    explained_variance           | 0.0657        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.67          |\n|    n_updates                    | 92            |\n|    policy_gradient_loss         | -0.00022      |\n|    value_loss                   | 7.82          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 141           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 64            |\n|    water_produced               | 1             |\n---------------------------------------------------\nEval num_timesteps=192000, episode_reward=13.16 +/- 20.69\nEpisode length: 313.00 +/- 19.39\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 313           |\n|    mean_reward                  | 13.2          |\n| time/                           |               |\n|    total_timesteps              | 192000        |\n| train/                          |               |\n|    approx_kl                    | 0.00025811998 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.88         |\n|    explained_variance           | 0.466         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.14          |\n|    n_updates                    | 94            |\n|    policy_gradient_loss         | -0.00049      |\n|    value_loss                   | 0.755         |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 64            |\n|    water_produced               | 8             |\n---------------------------------------------------\nNew best mean reward!\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 7.08     |\n| time/              |          |\n|    fps             | 768      |\n|    iterations      | 48       |\n|    time_elapsed    | 249      |\n|    total_timesteps | 192000   |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 7.95         |\n| time/                           |              |\n|    fps                          | 769          |\n|    iterations                   | 49           |\n|    time_elapsed                 | 254          |\n|    total_timesteps              | 196000       |\n| train/                          |              |\n|    approx_kl                    | 8.083284e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.88        |\n|    explained_variance           | 0.0969       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 5.68         |\n|    n_updates                    | 96           |\n|    policy_gradient_loss         | 8.58e-05     |\n|    value_loss                   | 12.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 146          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 75           |\n|    water_produced               | 5.5          |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.44          |\n| time/                           |               |\n|    fps                          | 771           |\n|    iterations                   | 50            |\n|    time_elapsed                 | 259           |\n|    total_timesteps              | 200000        |\n| train/                          |               |\n|    approx_kl                    | 0.00021829375 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.86         |\n|    explained_variance           | 0.121         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.2           |\n|    n_updates                    | 98            |\n|    policy_gradient_loss         | -0.000146     |\n|    value_loss                   | 8.82          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 17            |\n|    water_produced               | 0             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.32          |\n| time/                           |               |\n|    fps                          | 773           |\n|    iterations                   | 51            |\n|    time_elapsed                 | 263           |\n|    total_timesteps              | 204000        |\n| train/                          |               |\n|    approx_kl                    | 0.00073647796 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.8          |\n|    explained_variance           | 0.835         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.109         |\n|    n_updates                    | 100           |\n|    policy_gradient_loss         | 0.000133      |\n|    value_loss                   | 0.237         |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 152           |\n|    ice_dug                      | 139           |\n|    water_produced               | 13.5          |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 7.09        |\n| time/                           |             |\n|    fps                          | 774         |\n|    iterations                   | 52          |\n|    time_elapsed                 | 268         |\n|    total_timesteps              | 208000      |\n| train/                          |             |\n|    approx_kl                    | 0.000326183 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.84       |\n|    explained_variance           | 0.0994      |\n|    learning_rate                | 0.0003      |\n|    loss                         | 9.65        |\n|    n_updates                    | 102         |\n|    policy_gradient_loss         | -0.000251   |\n|    value_loss                   | 26.3        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 136         |\n|    action_queue_updates_total   | 150         |\n|    ice_dug                      | 75          |\n|    water_produced               | 4.75        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 6.27         |\n| time/                           |              |\n|    fps                          | 775          |\n|    iterations                   | 53           |\n|    time_elapsed                 | 273          |\n|    total_timesteps              | 212000       |\n| train/                          |              |\n|    approx_kl                    | 0.0004068938 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.78        |\n|    explained_variance           | 0.176        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 2.15         |\n|    n_updates                    | 104          |\n|    policy_gradient_loss         | -0.000438    |\n|    value_loss                   | 4.45         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 77           |\n|    water_produced               | 3.75         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 5.03         |\n| time/                           |              |\n|    fps                          | 775          |\n|    iterations                   | 54           |\n|    time_elapsed                 | 278          |\n|    total_timesteps              | 216000       |\n| train/                          |              |\n|    approx_kl                    | 0.0001394609 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.81        |\n|    explained_variance           | 0.213        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 2.19         |\n|    n_updates                    | 106          |\n|    policy_gradient_loss         | -0.000283    |\n|    value_loss                   | 4.37         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 6            |\n|    water_produced               | 0            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 5.75          |\n| time/                           |               |\n|    fps                          | 775           |\n|    iterations                   | 55            |\n|    time_elapsed                 | 283           |\n|    total_timesteps              | 220000        |\n| train/                          |               |\n|    approx_kl                    | 0.00019217537 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.78         |\n|    explained_variance           | 0.826         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.0725        |\n|    n_updates                    | 108           |\n|    policy_gradient_loss         | -0.000117     |\n|    value_loss                   | 0.261         |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 134           |\n|    action_queue_updates_total   | 146           |\n|    ice_dug                      | 54            |\n|    water_produced               | 3.25          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 3.51          |\n| time/                           |               |\n|    fps                          | 777           |\n|    iterations                   | 56            |\n|    time_elapsed                 | 288           |\n|    total_timesteps              | 224000        |\n| train/                          |               |\n|    approx_kl                    | 0.00031904533 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.77         |\n|    explained_variance           | 0.264         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 1.16          |\n|    n_updates                    | 110           |\n|    policy_gradient_loss         | -0.0005       |\n|    value_loss                   | 2.38          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 41            |\n|    water_produced               | 3.25          |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 3.84        |\n| time/                           |             |\n|    fps                          | 777         |\n|    iterations                   | 57          |\n|    time_elapsed                 | 293         |\n|    total_timesteps              | 228000      |\n| train/                          |             |\n|    approx_kl                    | 0.001044607 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.74       |\n|    explained_variance           | 0.113       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 2.15        |\n|    n_updates                    | 112         |\n|    policy_gradient_loss         | -0.000722   |\n|    value_loss                   | 3.46        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 137         |\n|    action_queue_updates_total   | 148         |\n|    ice_dug                      | 69          |\n|    water_produced               | 6.5         |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 8.89          |\n| time/                           |               |\n|    fps                          | 778           |\n|    iterations                   | 58            |\n|    time_elapsed                 | 297           |\n|    total_timesteps              | 232000        |\n| train/                          |               |\n|    approx_kl                    | 0.00084565266 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.78         |\n|    explained_variance           | 0.104         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.46          |\n|    n_updates                    | 114           |\n|    policy_gradient_loss         | -0.00025      |\n|    value_loss                   | 10.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 173           |\n|    water_produced               | 28            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 8.9           |\n| time/                           |               |\n|    fps                          | 779           |\n|    iterations                   | 59            |\n|    time_elapsed                 | 302           |\n|    total_timesteps              | 236000        |\n| train/                          |               |\n|    approx_kl                    | 0.00020317236 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.73         |\n|    explained_variance           | 0.0496        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 32.1          |\n|    n_updates                    | 116           |\n|    policy_gradient_loss         | -0.000433     |\n|    value_loss                   | 50.3          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 146           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 11            |\n|    water_produced               | 0             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.6          |\n| time/                           |               |\n|    fps                          | 780           |\n|    iterations                   | 60            |\n|    time_elapsed                 | 307           |\n|    total_timesteps              | 240000        |\n| train/                          |               |\n|    approx_kl                    | 3.3405795e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.71         |\n|    explained_variance           | 0.72          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.211         |\n|    n_updates                    | 118           |\n|    policy_gradient_loss         | 4.94e-05      |\n|    value_loss                   | 0.487         |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 149           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 177           |\n|    water_produced               | 15.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.3          |\n| time/                           |               |\n|    fps                          | 780           |\n|    iterations                   | 61            |\n|    time_elapsed                 | 312           |\n|    total_timesteps              | 244000        |\n| train/                          |               |\n|    approx_kl                    | 1.6553016e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.75         |\n|    explained_variance           | 0.00712       |\n|    learning_rate                | 0.0003        |\n|    loss                         | 8.46          |\n|    n_updates                    | 120           |\n|    policy_gradient_loss         | 2.28e-05      |\n|    value_loss                   | 22.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 152           |\n|    ice_dug                      | 68            |\n|    water_produced               | 1.5           |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.4          |\n| time/                           |               |\n|    fps                          | 780           |\n|    iterations                   | 62            |\n|    time_elapsed                 | 317           |\n|    total_timesteps              | 248000        |\n| train/                          |               |\n|    approx_kl                    | 1.1464402e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.79         |\n|    explained_variance           | 0.361         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.467         |\n|    n_updates                    | 122           |\n|    policy_gradient_loss         | -5.63e-05     |\n|    value_loss                   | 1.36          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 58            |\n|    water_produced               | 7             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 6.77         |\n| time/                           |              |\n|    fps                          | 780          |\n|    iterations                   | 63           |\n|    time_elapsed                 | 322          |\n|    total_timesteps              | 252000       |\n| train/                          |              |\n|    approx_kl                    | 7.817574e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.75        |\n|    explained_variance           | 0.12         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 3.75         |\n|    n_updates                    | 124          |\n|    policy_gradient_loss         | 3.66e-05     |\n|    value_loss                   | 7.74         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 146          |\n|    action_queue_updates_total   | 153          |\n|    ice_dug                      | 48           |\n|    water_produced               | 6            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 9.99          |\n| time/                           |               |\n|    fps                          | 781           |\n|    iterations                   | 64            |\n|    time_elapsed                 | 327           |\n|    total_timesteps              | 256000        |\n| train/                          |               |\n|    approx_kl                    | 0.00025784745 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.77         |\n|    explained_variance           | 0.0496        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 1.8           |\n|    n_updates                    | 126           |\n|    policy_gradient_loss         | 1.64e-05      |\n|    value_loss                   | 7.35          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 142           |\n|    action_queue_updates_total   | 151           |\n|    ice_dug                      | 96            |\n|    water_produced               | 15.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 9.29          |\n| time/                           |               |\n|    fps                          | 782           |\n|    iterations                   | 65            |\n|    time_elapsed                 | 332           |\n|    total_timesteps              | 260000        |\n| train/                          |               |\n|    approx_kl                    | 0.00026554256 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.0365        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 11.8          |\n|    n_updates                    | 128           |\n|    policy_gradient_loss         | -0.000323     |\n|    value_loss                   | 29.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 77            |\n|    water_produced               | 13.3          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 9.34         |\n| time/                           |              |\n|    fps                          | 783          |\n|    iterations                   | 66           |\n|    time_elapsed                 | 336          |\n|    total_timesteps              | 264000       |\n| train/                          |              |\n|    approx_kl                    | 0.0001891165 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.79        |\n|    explained_variance           | 0.0612       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 13.8         |\n|    n_updates                    | 130          |\n|    policy_gradient_loss         | -0.000197    |\n|    value_loss                   | 22.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 145          |\n|    action_queue_updates_total   | 154          |\n|    ice_dug                      | 16           |\n|    water_produced               | 2.25         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 9.57         |\n| time/                           |              |\n|    fps                          | 784          |\n|    iterations                   | 67           |\n|    time_elapsed                 | 341          |\n|    total_timesteps              | 268000       |\n| train/                          |              |\n|    approx_kl                    | 0.0006065965 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.79        |\n|    explained_variance           | 0.0635       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.844        |\n|    n_updates                    | 132          |\n|    policy_gradient_loss         | -0.00108     |\n|    value_loss                   | 2.49         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 146          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 75           |\n|    water_produced               | 8            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.1          |\n| time/                           |               |\n|    fps                          | 785           |\n|    iterations                   | 68            |\n|    time_elapsed                 | 346           |\n|    total_timesteps              | 272000        |\n| train/                          |               |\n|    approx_kl                    | 0.00037927998 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.79         |\n|    explained_variance           | 0.0888        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.53          |\n|    n_updates                    | 134           |\n|    policy_gradient_loss         | -0.00023      |\n|    value_loss                   | 11.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 69            |\n|    water_produced               | 13.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 10.1          |\n| time/                           |               |\n|    fps                          | 785           |\n|    iterations                   | 69            |\n|    time_elapsed                 | 351           |\n|    total_timesteps              | 276000        |\n| train/                          |               |\n|    approx_kl                    | 0.00044146608 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.76         |\n|    explained_variance           | 0.0432        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.2           |\n|    n_updates                    | 136           |\n|    policy_gradient_loss         | -0.000636     |\n|    value_loss                   | 23.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 146           |\n|    action_queue_updates_total   | 160           |\n|    ice_dug                      | 76            |\n|    water_produced               | 10.5          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 9.51          |\n| time/                           |               |\n|    fps                          | 786           |\n|    iterations                   | 70            |\n|    time_elapsed                 | 356           |\n|    total_timesteps              | 280000        |\n| train/                          |               |\n|    approx_kl                    | 0.00086314604 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.79         |\n|    explained_variance           | 0.0969        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 5.88          |\n|    n_updates                    | 138           |\n|    policy_gradient_loss         | -0.00128      |\n|    value_loss                   | 15.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 151           |\n|    action_queue_updates_total   | 160           |\n|    ice_dug                      | 69            |\n|    water_produced               | 10.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 13.6         |\n| time/                           |              |\n|    fps                          | 786          |\n|    iterations                   | 71           |\n|    time_elapsed                 | 360          |\n|    total_timesteps              | 284000       |\n| train/                          |              |\n|    approx_kl                    | 0.0010566121 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.73        |\n|    explained_variance           | 0.0822       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 10.9         |\n|    n_updates                    | 140          |\n|    policy_gradient_loss         | -0.000962    |\n|    value_loss                   | 19.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 154          |\n|    water_produced               | 21.2         |\n--------------------------------------------------\nEval num_timesteps=288000, episode_reward=0.36 +/- 0.72\nEpisode length: 301.00 +/- 0.00\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 301          |\n|    mean_reward                  | 0.36         |\n| time/                           |              |\n|    total_timesteps              | 288000       |\n| train/                          |              |\n|    approx_kl                    | 0.0009082459 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.69        |\n|    explained_variance           | 0.0683       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 21.2         |\n|    n_updates                    | 142          |\n|    policy_gradient_loss         | 0.000231     |\n|    value_loss                   | 42           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 145          |\n|    action_queue_updates_total   | 157          |\n|    ice_dug                      | 48           |\n|    water_produced               | 4            |\n--------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 12.7     |\n| time/              |          |\n|    fps             | 781      |\n|    iterations      | 72       |\n|    time_elapsed    | 368      |\n|    total_timesteps | 288000   |\n---------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.3          |\n| time/                           |               |\n|    fps                          | 781           |\n|    iterations                   | 73            |\n|    time_elapsed                 | 373           |\n|    total_timesteps              | 292000        |\n| train/                          |               |\n|    approx_kl                    | 0.00011291199 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.71         |\n|    explained_variance           | 0.149         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.1           |\n|    n_updates                    | 144           |\n|    policy_gradient_loss         | -9.41e-06     |\n|    value_loss                   | 7.26          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 140           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 118           |\n|    water_produced               | 15.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.2          |\n| time/                           |               |\n|    fps                          | 782           |\n|    iterations                   | 74            |\n|    time_elapsed                 | 378           |\n|    total_timesteps              | 296000        |\n| train/                          |               |\n|    approx_kl                    | 1.2346834e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.67         |\n|    explained_variance           | 0.138         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.3          |\n|    n_updates                    | 146           |\n|    policy_gradient_loss         | 5.54e-05      |\n|    value_loss                   | 23.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 140           |\n|    action_queue_updates_total   | 152           |\n|    ice_dug                      | 75            |\n|    water_produced               | 10            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.9          |\n| time/                           |               |\n|    fps                          | 783           |\n|    iterations                   | 75            |\n|    time_elapsed                 | 383           |\n|    total_timesteps              | 300000        |\n| train/                          |               |\n|    approx_kl                    | 6.1173334e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.127         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 11.3          |\n|    n_updates                    | 148           |\n|    policy_gradient_loss         | -0.000117     |\n|    value_loss                   | 18.3          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 140           |\n|    action_queue_updates_total   | 148           |\n|    ice_dug                      | 68            |\n|    water_produced               | 4             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 8.81          |\n| time/                           |               |\n|    fps                          | 783           |\n|    iterations                   | 76            |\n|    time_elapsed                 | 387           |\n|    total_timesteps              | 304000        |\n| train/                          |               |\n|    approx_kl                    | 0.00012447716 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.67         |\n|    explained_variance           | 0.194         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 4.16          |\n|    n_updates                    | 150           |\n|    policy_gradient_loss         | 1.04e-05      |\n|    value_loss                   | 6.78          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 152           |\n|    ice_dug                      | 48            |\n|    water_produced               | 6.75          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 9.66         |\n| time/                           |              |\n|    fps                          | 784          |\n|    iterations                   | 77           |\n|    time_elapsed                 | 392          |\n|    total_timesteps              | 308000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003696676 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.72        |\n|    explained_variance           | 0.197        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 7.28         |\n|    n_updates                    | 152          |\n|    policy_gradient_loss         | -0.000495    |\n|    value_loss                   | 11.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 147          |\n|    action_queue_updates_total   | 159          |\n|    ice_dug                      | 96           |\n|    water_produced               | 7.75         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 10.2         |\n| time/                           |              |\n|    fps                          | 784          |\n|    iterations                   | 78           |\n|    time_elapsed                 | 397          |\n|    total_timesteps              | 312000       |\n| train/                          |              |\n|    approx_kl                    | 0.0006211189 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.71        |\n|    explained_variance           | 0.158        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 7.65         |\n|    n_updates                    | 154          |\n|    policy_gradient_loss         | 2.98e-05     |\n|    value_loss                   | 12.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 146          |\n|    action_queue_updates_total   | 161          |\n|    ice_dug                      | 114          |\n|    water_produced               | 18.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.2          |\n| time/                           |               |\n|    fps                          | 785           |\n|    iterations                   | 79            |\n|    time_elapsed                 | 402           |\n|    total_timesteps              | 316000        |\n| train/                          |               |\n|    approx_kl                    | 0.00042644804 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.0973        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 18.7          |\n|    n_updates                    | 156           |\n|    policy_gradient_loss         | -0.000432     |\n|    value_loss                   | 31.2          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 145           |\n|    action_queue_updates_total   | 165           |\n|    ice_dug                      | 148           |\n|    water_produced               | 24.5          |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 16.9        |\n| time/                           |             |\n|    fps                          | 785         |\n|    iterations                   | 80          |\n|    time_elapsed                 | 407         |\n|    total_timesteps              | 320000      |\n| train/                          |             |\n|    approx_kl                    | 0.000703286 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.75       |\n|    explained_variance           | 0.0567      |\n|    learning_rate                | 0.0003      |\n|    loss                         | 18.5        |\n|    n_updates                    | 158         |\n|    policy_gradient_loss         | 0.000147    |\n|    value_loss                   | 50.4        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 148         |\n|    action_queue_updates_total   | 167         |\n|    ice_dug                      | 224         |\n|    water_produced               | 20.5        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.7         |\n| time/                           |              |\n|    fps                          | 785          |\n|    iterations                   | 81           |\n|    time_elapsed                 | 412          |\n|    total_timesteps              | 324000       |\n| train/                          |              |\n|    approx_kl                    | 0.0004339232 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.74        |\n|    explained_variance           | 0.159        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.8         |\n|    n_updates                    | 160          |\n|    policy_gradient_loss         | 4.66e-05     |\n|    value_loss                   | 28.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 153          |\n|    action_queue_updates_total   | 167          |\n|    ice_dug                      | 137          |\n|    water_produced               | 20.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 23.4          |\n| time/                           |               |\n|    fps                          | 785           |\n|    iterations                   | 82            |\n|    time_elapsed                 | 417           |\n|    total_timesteps              | 328000        |\n| train/                          |               |\n|    approx_kl                    | 0.00022797105 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.0665        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 19.8          |\n|    n_updates                    | 162           |\n|    policy_gradient_loss         | 8.45e-05      |\n|    value_loss                   | 40.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 147           |\n|    action_queue_updates_total   | 163           |\n|    ice_dug                      | 181           |\n|    water_produced               | 25.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.2          |\n| time/                           |               |\n|    fps                          | 785           |\n|    iterations                   | 83            |\n|    time_elapsed                 | 422           |\n|    total_timesteps              | 332000        |\n| train/                          |               |\n|    approx_kl                    | 0.00024342074 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.71         |\n|    explained_variance           | 0.0773        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 26.9          |\n|    n_updates                    | 164           |\n|    policy_gradient_loss         | 0.000389      |\n|    value_loss                   | 53.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 166           |\n|    ice_dug                      | 44            |\n|    water_produced               | 8.25          |\n---------------------------------------------------\n----------------------------------------------------\n| rollout/                        |                |\n|    ep_len_mean                  | 200            |\n|    ep_rew_mean                  | 16.9           |\n| time/                           |                |\n|    fps                          | 786            |\n|    iterations                   | 84             |\n|    time_elapsed                 | 427            |\n|    total_timesteps              | 336000         |\n| train/                          |                |\n|    approx_kl                    | 0.000119701355 |\n|    clip_fraction                | 0              |\n|    clip_range                   | 0.2            |\n|    entropy_loss                 | -1.72          |\n|    explained_variance           | 0.187          |\n|    learning_rate                | 0.0003         |\n|    loss                         | 4.48           |\n|    n_updates                    | 166            |\n|    policy_gradient_loss         | -9.64e-05      |\n|    value_loss                   | 14.4           |\n| train_metrics/                  |                |\n|    action_queue_updates_success | 151            |\n|    action_queue_updates_total   | 167            |\n|    ice_dug                      | 76             |\n|    water_produced               | 3.75           |\n----------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.1          |\n| time/                           |               |\n|    fps                          | 786           |\n|    iterations                   | 85            |\n|    time_elapsed                 | 432           |\n|    total_timesteps              | 340000        |\n| train/                          |               |\n|    approx_kl                    | 0.00040250184 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.74         |\n|    explained_variance           | 0.377         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 1.91          |\n|    n_updates                    | 168           |\n|    policy_gradient_loss         | -0.000171     |\n|    value_loss                   | 3.99          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 148           |\n|    action_queue_updates_total   | 163           |\n|    ice_dug                      | 143           |\n|    water_produced               | 17.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.8          |\n| time/                           |               |\n|    fps                          | 787           |\n|    iterations                   | 86            |\n|    time_elapsed                 | 437           |\n|    total_timesteps              | 344000        |\n| train/                          |               |\n|    approx_kl                    | 0.00015786414 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.73         |\n|    explained_variance           | 0.157         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 13.6          |\n|    n_updates                    | 170           |\n|    policy_gradient_loss         | -1.67e-05     |\n|    value_loss                   | 30.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 149           |\n|    action_queue_updates_total   | 167           |\n|    ice_dug                      | 69            |\n|    water_produced               | 9.5           |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.1          |\n| time/                           |               |\n|    fps                          | 787           |\n|    iterations                   | 87            |\n|    time_elapsed                 | 441           |\n|    total_timesteps              | 348000        |\n| train/                          |               |\n|    approx_kl                    | 0.00017424766 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.75         |\n|    explained_variance           | 0.159         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.59          |\n|    n_updates                    | 172           |\n|    policy_gradient_loss         | -0.000115     |\n|    value_loss                   | 20.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 149           |\n|    action_queue_updates_total   | 165           |\n|    ice_dug                      | 117           |\n|    water_produced               | 12.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 12.4         |\n| time/                           |              |\n|    fps                          | 788          |\n|    iterations                   | 88           |\n|    time_elapsed                 | 446          |\n|    total_timesteps              | 352000       |\n| train/                          |              |\n|    approx_kl                    | 0.0005022868 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.75        |\n|    explained_variance           | 0.233        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 8.07         |\n|    n_updates                    | 174          |\n|    policy_gradient_loss         | -0.000248    |\n|    value_loss                   | 15.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 162          |\n|    ice_dug                      | 114          |\n|    water_produced               | 13.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 12.3          |\n| time/                           |               |\n|    fps                          | 789           |\n|    iterations                   | 89            |\n|    time_elapsed                 | 451           |\n|    total_timesteps              | 356000        |\n| train/                          |               |\n|    approx_kl                    | 0.00014429663 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.77         |\n|    explained_variance           | 0.313         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.87          |\n|    n_updates                    | 176           |\n|    policy_gradient_loss         | -1.52e-06     |\n|    value_loss                   | 15.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 148           |\n|    action_queue_updates_total   | 163           |\n|    ice_dug                      | 21            |\n|    water_produced               | 4             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 12.3          |\n| time/                           |               |\n|    fps                          | 789           |\n|    iterations                   | 90            |\n|    time_elapsed                 | 456           |\n|    total_timesteps              | 360000        |\n| train/                          |               |\n|    approx_kl                    | 0.00085658545 |\n|    clip_fraction                | 0.000125      |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.76         |\n|    explained_variance           | 0.47          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 1.69          |\n|    n_updates                    | 178           |\n|    policy_gradient_loss         | -0.000119     |\n|    value_loss                   | 5.06          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 144           |\n|    action_queue_updates_total   | 159           |\n|    ice_dug                      | 127           |\n|    water_produced               | 17.5          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 10.8          |\n| time/                           |               |\n|    fps                          | 789           |\n|    iterations                   | 91            |\n|    time_elapsed                 | 461           |\n|    total_timesteps              | 364000        |\n| train/                          |               |\n|    approx_kl                    | 0.00020486079 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.77         |\n|    explained_variance           | 0.227         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.9          |\n|    n_updates                    | 180           |\n|    policy_gradient_loss         | 0.000172      |\n|    value_loss                   | 32.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 157           |\n|    ice_dug                      | 72            |\n|    water_produced               | 1.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 10.7          |\n| time/                           |               |\n|    fps                          | 790           |\n|    iterations                   | 92            |\n|    time_elapsed                 | 465           |\n|    total_timesteps              | 368000        |\n| train/                          |               |\n|    approx_kl                    | 0.00015487775 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.78         |\n|    explained_variance           | 0.734         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 1.16          |\n|    n_updates                    | 182           |\n|    policy_gradient_loss         | -9.3e-06      |\n|    value_loss                   | 2.33          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 154           |\n|    action_queue_updates_total   | 166           |\n|    ice_dug                      | 113           |\n|    water_produced               | 12.2          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 10.6         |\n| time/                           |              |\n|    fps                          | 790          |\n|    iterations                   | 93           |\n|    time_elapsed                 | 470          |\n|    total_timesteps              | 372000       |\n| train/                          |              |\n|    approx_kl                    | 0.0016817044 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.76        |\n|    explained_variance           | 0.266        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 9.1          |\n|    n_updates                    | 184          |\n|    policy_gradient_loss         | -0.0011      |\n|    value_loss                   | 17           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 138          |\n|    action_queue_updates_total   | 148          |\n|    ice_dug                      | 122          |\n|    water_produced               | 13           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 11.7         |\n| time/                           |              |\n|    fps                          | 790          |\n|    iterations                   | 94           |\n|    time_elapsed                 | 475          |\n|    total_timesteps              | 376000       |\n| train/                          |              |\n|    approx_kl                    | 0.0006763857 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.64        |\n|    explained_variance           | 0.15         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 17.4         |\n|    n_updates                    | 186          |\n|    policy_gradient_loss         | -7.96e-05    |\n|    value_loss                   | 22.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 79           |\n|    water_produced               | 9            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11            |\n| time/                           |               |\n|    fps                          | 791           |\n|    iterations                   | 95            |\n|    time_elapsed                 | 480           |\n|    total_timesteps              | 380000        |\n| train/                          |               |\n|    approx_kl                    | 0.00032742237 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.77         |\n|    explained_variance           | 0.224         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 10.9          |\n|    n_updates                    | 188           |\n|    policy_gradient_loss         | -2.02e-05     |\n|    value_loss                   | 20.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 151           |\n|    ice_dug                      | 79            |\n|    water_produced               | 14.2          |\n---------------------------------------------------\nEval num_timesteps=384000, episode_reward=36.56 +/- 73.02\nEpisode length: 336.00 +/- 70.00\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 336           |\n|    mean_reward                  | 36.6          |\n| time/                           |               |\n|    total_timesteps              | 384000        |\n| train/                          |               |\n|    approx_kl                    | 0.00034279362 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.75         |\n|    explained_variance           | 0.197         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.8          |\n|    n_updates                    | 190           |\n|    policy_gradient_loss         | -0.000371     |\n|    value_loss                   | 32.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 153           |\n|    water_produced               | 27.8          |\n---------------------------------------------------\nNew best mean reward!\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 16.3     |\n| time/              |          |\n|    fps             | 786      |\n|    iterations      | 96       |\n|    time_elapsed    | 488      |\n|    total_timesteps | 384000   |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 16.9         |\n| time/                           |              |\n|    fps                          | 786          |\n|    iterations                   | 97           |\n|    time_elapsed                 | 493          |\n|    total_timesteps              | 388000       |\n| train/                          |              |\n|    approx_kl                    | 0.0005015042 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.63        |\n|    explained_variance           | 0.144        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 30.4         |\n|    n_updates                    | 192          |\n|    policy_gradient_loss         | -0.000358    |\n|    value_loss                   | 57.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 144          |\n|    ice_dug                      | 84           |\n|    water_produced               | 15.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.7          |\n| time/                           |               |\n|    fps                          | 787           |\n|    iterations                   | 98            |\n|    time_elapsed                 | 497           |\n|    total_timesteps              | 392000        |\n| train/                          |               |\n|    approx_kl                    | 0.00044463776 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.57         |\n|    explained_variance           | 0.182         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 14.2          |\n|    n_updates                    | 194           |\n|    policy_gradient_loss         | 1.12e-05      |\n|    value_loss                   | 28            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 141           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 78            |\n|    water_produced               | 12.3          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 17.4          |\n| time/                           |               |\n|    fps                          | 787           |\n|    iterations                   | 99            |\n|    time_elapsed                 | 502           |\n|    total_timesteps              | 396000        |\n| train/                          |               |\n|    approx_kl                    | 0.00016963489 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.58         |\n|    explained_variance           | 0.151         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 13.3          |\n|    n_updates                    | 196           |\n|    policy_gradient_loss         | 6.89e-06      |\n|    value_loss                   | 28.2          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 153           |\n|    ice_dug                      | 106           |\n|    water_produced               | 12.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 17.8         |\n| time/                           |              |\n|    fps                          | 787          |\n|    iterations                   | 100          |\n|    time_elapsed                 | 507          |\n|    total_timesteps              | 400000       |\n| train/                          |              |\n|    approx_kl                    | 0.0009872374 |\n|    clip_fraction                | 0.00188      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.69        |\n|    explained_variance           | 0.244        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 8            |\n|    n_updates                    | 198          |\n|    policy_gradient_loss         | -0.00119     |\n|    value_loss                   | 17.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 146          |\n|    ice_dug                      | 95           |\n|    water_produced               | 15.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 13.6         |\n| time/                           |              |\n|    fps                          | 788          |\n|    iterations                   | 101          |\n|    time_elapsed                 | 512          |\n|    total_timesteps              | 404000       |\n| train/                          |              |\n|    approx_kl                    | 0.0030415982 |\n|    clip_fraction                | 0.00388      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.65        |\n|    explained_variance           | 0.167        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 15.4         |\n|    n_updates                    | 200          |\n|    policy_gradient_loss         | -0.0019      |\n|    value_loss                   | 28.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 124          |\n|    action_queue_updates_total   | 135          |\n|    ice_dug                      | 38           |\n|    water_produced               | 8            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 14.5         |\n| time/                           |              |\n|    fps                          | 789          |\n|    iterations                   | 102          |\n|    time_elapsed                 | 516          |\n|    total_timesteps              | 408000       |\n| train/                          |              |\n|    approx_kl                    | 0.0013733355 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.53        |\n|    explained_variance           | 0.136        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 12.2         |\n|    n_updates                    | 202          |\n|    policy_gradient_loss         | 0.000127     |\n|    value_loss                   | 28.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 119          |\n|    action_queue_updates_total   | 132          |\n|    ice_dug                      | 148          |\n|    water_produced               | 19.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 12.6          |\n| time/                           |               |\n|    fps                          | 789           |\n|    iterations                   | 103           |\n|    time_elapsed                 | 521           |\n|    total_timesteps              | 412000        |\n| train/                          |               |\n|    approx_kl                    | 0.00025131094 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.56         |\n|    explained_variance           | 0.153         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 23.7          |\n|    n_updates                    | 204           |\n|    policy_gradient_loss         | -0.00014      |\n|    value_loss                   | 42.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 141           |\n|    ice_dug                      | 47            |\n|    water_produced               | 3             |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 12.7        |\n| time/                           |             |\n|    fps                          | 790         |\n|    iterations                   | 104         |\n|    time_elapsed                 | 526         |\n|    total_timesteps              | 416000      |\n| train/                          |             |\n|    approx_kl                    | 3.66032e-05 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.6        |\n|    explained_variance           | 0.49        |\n|    learning_rate                | 0.0003      |\n|    loss                         | 2.54        |\n|    n_updates                    | 206         |\n|    policy_gradient_loss         | 2.05e-05    |\n|    value_loss                   | 4.84        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 120         |\n|    action_queue_updates_total   | 131         |\n|    ice_dug                      | 84          |\n|    water_produced               | 13.2        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.7         |\n| time/                           |              |\n|    fps                          | 789          |\n|    iterations                   | 105          |\n|    time_elapsed                 | 531          |\n|    total_timesteps              | 420000       |\n| train/                          |              |\n|    approx_kl                    | 0.0001189569 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.58        |\n|    explained_variance           | 0.101        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.7         |\n|    n_updates                    | 208          |\n|    policy_gradient_loss         | -0.000202    |\n|    value_loss                   | 34.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 135          |\n|    ice_dug                      | 276          |\n|    water_produced               | 44           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.6          |\n| time/                           |               |\n|    fps                          | 790           |\n|    iterations                   | 106           |\n|    time_elapsed                 | 536           |\n|    total_timesteps              | 424000        |\n| train/                          |               |\n|    approx_kl                    | 0.00049734936 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.5          |\n|    explained_variance           | 0.0782        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 57.7          |\n|    n_updates                    | 210           |\n|    policy_gradient_loss         | -0.000153     |\n|    value_loss                   | 114           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 118           |\n|    action_queue_updates_total   | 132           |\n|    ice_dug                      | 145           |\n|    water_produced               | 21.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.6         |\n| time/                           |              |\n|    fps                          | 790          |\n|    iterations                   | 107          |\n|    time_elapsed                 | 541          |\n|    total_timesteps              | 428000       |\n| train/                          |              |\n|    approx_kl                    | 0.0004019789 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.51        |\n|    explained_variance           | 0.0964       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 36.6         |\n|    n_updates                    | 212          |\n|    policy_gradient_loss         | -0.000363    |\n|    value_loss                   | 66           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 124          |\n|    action_queue_updates_total   | 133          |\n|    ice_dug                      | 78           |\n|    water_produced               | 9.75         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 26.4          |\n| time/                           |               |\n|    fps                          | 791           |\n|    iterations                   | 108           |\n|    time_elapsed                 | 546           |\n|    total_timesteps              | 432000        |\n| train/                          |               |\n|    approx_kl                    | 3.4632547e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.44         |\n|    explained_variance           | 0.12          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.6           |\n|    n_updates                    | 214           |\n|    policy_gradient_loss         | 0.000113      |\n|    value_loss                   | 19.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 117           |\n|    action_queue_updates_total   | 127           |\n|    ice_dug                      | 179           |\n|    water_produced               | 35.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 26.5          |\n| time/                           |               |\n|    fps                          | 791           |\n|    iterations                   | 109           |\n|    time_elapsed                 | 550           |\n|    total_timesteps              | 436000        |\n| train/                          |               |\n|    approx_kl                    | 4.0246592e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.51         |\n|    explained_variance           | 0.127         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 26.6          |\n|    n_updates                    | 216           |\n|    policy_gradient_loss         | -0.000128     |\n|    value_loss                   | 84.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 116           |\n|    action_queue_updates_total   | 121           |\n|    ice_dug                      | 97            |\n|    water_produced               | 13.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.7          |\n| time/                           |               |\n|    fps                          | 791           |\n|    iterations                   | 110           |\n|    time_elapsed                 | 555           |\n|    total_timesteps              | 440000        |\n| train/                          |               |\n|    approx_kl                    | 0.00016541663 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.43         |\n|    explained_variance           | 0.0892        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 13.6          |\n|    n_updates                    | 218           |\n|    policy_gradient_loss         | -0.000184     |\n|    value_loss                   | 25.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 125           |\n|    action_queue_updates_total   | 136           |\n|    ice_dug                      | 116           |\n|    water_produced               | 16.7          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.2          |\n| time/                           |               |\n|    fps                          | 792           |\n|    iterations                   | 111           |\n|    time_elapsed                 | 560           |\n|    total_timesteps              | 444000        |\n| train/                          |               |\n|    approx_kl                    | 0.00048110232 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.47         |\n|    explained_variance           | 0.119         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 16.8          |\n|    n_updates                    | 220           |\n|    policy_gradient_loss         | 7.64e-05      |\n|    value_loss                   | 32.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 124           |\n|    action_queue_updates_total   | 135           |\n|    ice_dug                      | 61            |\n|    water_produced               | 9.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 17.1          |\n| time/                           |               |\n|    fps                          | 792           |\n|    iterations                   | 112           |\n|    time_elapsed                 | 565           |\n|    total_timesteps              | 448000        |\n| train/                          |               |\n|    approx_kl                    | 0.00021812467 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.53         |\n|    explained_variance           | 0.129         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.3           |\n|    n_updates                    | 222           |\n|    policy_gradient_loss         | -0.00015      |\n|    value_loss                   | 20.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 136           |\n|    ice_dug                      | 26            |\n|    water_produced               | 4.5           |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.4          |\n| time/                           |               |\n|    fps                          | 791           |\n|    iterations                   | 113           |\n|    time_elapsed                 | 570           |\n|    total_timesteps              | 452000        |\n| train/                          |               |\n|    approx_kl                    | 0.00019556319 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.49         |\n|    explained_variance           | 0.199         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 5.6           |\n|    n_updates                    | 224           |\n|    policy_gradient_loss         | -8.39e-05     |\n|    value_loss                   | 10.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 120           |\n|    action_queue_updates_total   | 132           |\n|    ice_dug                      | 107           |\n|    water_produced               | 18            |\n---------------------------------------------------\n----------------------------------------------------\n| rollout/                        |                |\n|    ep_len_mean                  | 200            |\n|    ep_rew_mean                  | 13.9           |\n| time/                           |                |\n|    fps                          | 792            |\n|    iterations                   | 114            |\n|    time_elapsed                 | 575            |\n|    total_timesteps              | 456000         |\n| train/                          |                |\n|    approx_kl                    | 0.000107978274 |\n|    clip_fraction                | 0              |\n|    clip_range                   | 0.2            |\n|    entropy_loss                 | -1.49          |\n|    explained_variance           | 0.158          |\n|    learning_rate                | 0.0003         |\n|    loss                         | 18             |\n|    n_updates                    | 226            |\n|    policy_gradient_loss         | -5.5e-05       |\n|    value_loss                   | 40.1           |\n| train_metrics/                  |                |\n|    action_queue_updates_success | 121            |\n|    action_queue_updates_total   | 127            |\n|    ice_dug                      | 89             |\n|    water_produced               | 16.5           |\n----------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.8          |\n| time/                           |               |\n|    fps                          | 792           |\n|    iterations                   | 115           |\n|    time_elapsed                 | 580           |\n|    total_timesteps              | 460000        |\n| train/                          |               |\n|    approx_kl                    | 0.00019832849 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.44         |\n|    explained_variance           | 0.0649        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 18            |\n|    n_updates                    | 228           |\n|    policy_gradient_loss         | -0.000104     |\n|    value_loss                   | 43            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 120           |\n|    action_queue_updates_total   | 133           |\n|    ice_dug                      | 107           |\n|    water_produced               | 16.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21.8         |\n| time/                           |              |\n|    fps                          | 792          |\n|    iterations                   | 116          |\n|    time_elapsed                 | 585          |\n|    total_timesteps              | 464000       |\n| train/                          |              |\n|    approx_kl                    | 7.485457e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.46        |\n|    explained_variance           | 0.131        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 19.3         |\n|    n_updates                    | 230          |\n|    policy_gradient_loss         | -6.18e-05    |\n|    value_loss                   | 32.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 129          |\n|    action_queue_updates_total   | 139          |\n|    ice_dug                      | 242          |\n|    water_produced               | 48           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 23            |\n| time/                           |               |\n|    fps                          | 793           |\n|    iterations                   | 117           |\n|    time_elapsed                 | 589           |\n|    total_timesteps              | 468000        |\n| train/                          |               |\n|    approx_kl                    | 0.00026079142 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.47         |\n|    explained_variance           | 0.0796        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 52.5          |\n|    n_updates                    | 232           |\n|    policy_gradient_loss         | -0.000543     |\n|    value_loss                   | 120           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 140           |\n|    ice_dug                      | 53            |\n|    water_produced               | 10            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 24.8         |\n| time/                           |              |\n|    fps                          | 793          |\n|    iterations                   | 118          |\n|    time_elapsed                 | 594          |\n|    total_timesteps              | 472000       |\n| train/                          |              |\n|    approx_kl                    | 2.621132e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.46        |\n|    explained_variance           | 0.116        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.3         |\n|    n_updates                    | 234          |\n|    policy_gradient_loss         | 2.88e-05     |\n|    value_loss                   | 22           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 142          |\n|    ice_dug                      | 165          |\n|    water_produced               | 26.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 24.6          |\n| time/                           |               |\n|    fps                          | 793           |\n|    iterations                   | 119           |\n|    time_elapsed                 | 599           |\n|    total_timesteps              | 476000        |\n| train/                          |               |\n|    approx_kl                    | 4.6345533e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.48         |\n|    explained_variance           | 0.0935        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 25.1          |\n|    n_updates                    | 236           |\n|    policy_gradient_loss         | -0.000165     |\n|    value_loss                   | 53.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 144           |\n|    ice_dug                      | 119           |\n|    water_produced               | 15.2          |\n---------------------------------------------------\nEval num_timesteps=480000, episode_reward=13.28 +/- 19.98\nEpisode length: 312.00 +/- 19.60\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 312          |\n|    mean_reward                  | 13.3         |\n| time/                           |              |\n|    total_timesteps              | 480000       |\n| train/                          |              |\n|    approx_kl                    | 0.0002150936 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.56        |\n|    explained_variance           | 0.152        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 12.7         |\n|    n_updates                    | 238          |\n|    policy_gradient_loss         | -0.000182    |\n|    value_loss                   | 26.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 147          |\n|    ice_dug                      | 36           |\n|    water_produced               | 5.25         |\n--------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 22.2     |\n| time/              |          |\n|    fps             | 790      |\n|    iterations      | 120      |\n|    time_elapsed    | 607      |\n|    total_timesteps | 480000   |\n---------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.8          |\n| time/                           |               |\n|    fps                          | 791           |\n|    iterations                   | 121           |\n|    time_elapsed                 | 611           |\n|    total_timesteps              | 484000        |\n| train/                          |               |\n|    approx_kl                    | 0.00047319802 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.53         |\n|    explained_variance           | 0.14          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.24          |\n|    n_updates                    | 240           |\n|    policy_gradient_loss         | 0.000175      |\n|    value_loss                   | 9.69          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 125           |\n|    action_queue_updates_total   | 137           |\n|    ice_dug                      | 43            |\n|    water_produced               | 7.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16            |\n| time/                           |               |\n|    fps                          | 791           |\n|    iterations                   | 122           |\n|    time_elapsed                 | 616           |\n|    total_timesteps              | 488000        |\n| train/                          |               |\n|    approx_kl                    | 0.00042868807 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.55         |\n|    explained_variance           | 0.233         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.49          |\n|    n_updates                    | 242           |\n|    policy_gradient_loss         | -0.000221     |\n|    value_loss                   | 12.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 129           |\n|    action_queue_updates_total   | 139           |\n|    ice_dug                      | 151           |\n|    water_produced               | 20            |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 14.5        |\n| time/                           |             |\n|    fps                          | 792         |\n|    iterations                   | 123         |\n|    time_elapsed                 | 620         |\n|    total_timesteps              | 492000      |\n| train/                          |             |\n|    approx_kl                    | 3.25426e-05 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.52       |\n|    explained_variance           | 0.155       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 25.2        |\n|    n_updates                    | 244         |\n|    policy_gradient_loss         | -3.36e-05   |\n|    value_loss                   | 47.6        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 127         |\n|    action_queue_updates_total   | 138         |\n|    ice_dug                      | 92          |\n|    water_produced               | 19.8        |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.8          |\n| time/                           |               |\n|    fps                          | 792           |\n|    iterations                   | 124           |\n|    time_elapsed                 | 625           |\n|    total_timesteps              | 496000        |\n| train/                          |               |\n|    approx_kl                    | 0.00014352065 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.53         |\n|    explained_variance           | 0.0731        |\n|    learning_rate                | 0.0003        |\n|    loss                         | 19.1          |\n|    n_updates                    | 246           |\n|    policy_gradient_loss         | -0.000165     |\n|    value_loss                   | 46.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 146           |\n|    ice_dug                      | 212           |\n|    water_produced               | 46            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21.4         |\n| time/                           |              |\n|    fps                          | 792          |\n|    iterations                   | 125          |\n|    time_elapsed                 | 630          |\n|    total_timesteps              | 500000       |\n| train/                          |              |\n|    approx_kl                    | 0.0002800067 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.54        |\n|    explained_variance           | 0.0908       |\n|    learning_rate                | 0.0003       |\n|    loss                         | 61.6         |\n|    n_updates                    | 248          |\n|    policy_gradient_loss         | -0.000172    |\n|    value_loss                   | 114          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 62           |\n|    water_produced               | 8            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 22.6         |\n| time/                           |              |\n|    fps                          | 793          |\n|    iterations                   | 126          |\n|    time_elapsed                 | 634          |\n|    total_timesteps              | 504000       |\n| train/                          |              |\n|    approx_kl                    | 7.029156e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.59        |\n|    explained_variance           | 0.223        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 9.77         |\n|    n_updates                    | 250          |\n|    policy_gradient_loss         | -0.000259    |\n|    value_loss                   | 15.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 146          |\n|    ice_dug                      | 93           |\n|    water_produced               | 13           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.9          |\n| time/                           |               |\n|    fps                          | 794           |\n|    iterations                   | 127           |\n|    time_elapsed                 | 639           |\n|    total_timesteps              | 508000        |\n| train/                          |               |\n|    approx_kl                    | 0.00052950566 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.58         |\n|    explained_variance           | 0.168         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 18.8          |\n|    n_updates                    | 252           |\n|    policy_gradient_loss         | 4.88e-05      |\n|    value_loss                   | 24.3          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 144           |\n|    ice_dug                      | 40            |\n|    water_produced               | 2.75          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 20.1         |\n| time/                           |              |\n|    fps                          | 794          |\n|    iterations                   | 128          |\n|    time_elapsed                 | 644          |\n|    total_timesteps              | 512000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003762257 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.57        |\n|    explained_variance           | 0.44         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 1.53         |\n|    n_updates                    | 254          |\n|    policy_gradient_loss         | 0.000105     |\n|    value_loss                   | 3.49         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 141          |\n|    ice_dug                      | 127          |\n|    water_produced               | 25.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 14            |\n| time/                           |               |\n|    fps                          | 794           |\n|    iterations                   | 129           |\n|    time_elapsed                 | 649           |\n|    total_timesteps              | 516000        |\n| train/                          |               |\n|    approx_kl                    | 5.9887097e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.61         |\n|    explained_variance           | 0.102         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 23.8          |\n|    n_updates                    | 256           |\n|    policy_gradient_loss         | 0.000126      |\n|    value_loss                   | 57.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 144           |\n|    ice_dug                      | 140           |\n|    water_produced               | 16.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.2          |\n| time/                           |               |\n|    fps                          | 795           |\n|    iterations                   | 130           |\n|    time_elapsed                 | 653           |\n|    total_timesteps              | 520000        |\n| train/                          |               |\n|    approx_kl                    | 0.00017327846 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.58         |\n|    explained_variance           | 0.131         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.6          |\n|    n_updates                    | 258           |\n|    policy_gradient_loss         | -0.000129     |\n|    value_loss                   | 34.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 219           |\n|    water_produced               | 37.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.2          |\n| time/                           |               |\n|    fps                          | 794           |\n|    iterations                   | 131           |\n|    time_elapsed                 | 659           |\n|    total_timesteps              | 524000        |\n| train/                          |               |\n|    approx_kl                    | 0.00021337035 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.62         |\n|    explained_variance           | 0.118         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 35.5          |\n|    n_updates                    | 260           |\n|    policy_gradient_loss         | -7.44e-05     |\n|    value_loss                   | 66.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 144           |\n|    ice_dug                      | 85            |\n|    water_produced               | 13            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 26.6         |\n| time/                           |              |\n|    fps                          | 795          |\n|    iterations                   | 132          |\n|    time_elapsed                 | 664          |\n|    total_timesteps              | 528000       |\n| train/                          |              |\n|    approx_kl                    | 2.789703e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.61        |\n|    explained_variance           | 0.151        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.3         |\n|    n_updates                    | 262          |\n|    policy_gradient_loss         | -1.9e-05     |\n|    value_loss                   | 30.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 146          |\n|    ice_dug                      | 154          |\n|    water_produced               | 33.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 22.8          |\n| time/                           |               |\n|    fps                          | 795           |\n|    iterations                   | 133           |\n|    time_elapsed                 | 668           |\n|    total_timesteps              | 532000        |\n| train/                          |               |\n|    approx_kl                    | 4.4058357e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.57         |\n|    explained_variance           | 0.109         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 38.5          |\n|    n_updates                    | 264           |\n|    policy_gradient_loss         | -4e-05        |\n|    value_loss                   | 76            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 71            |\n|    water_produced               | 6.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 28.2          |\n| time/                           |               |\n|    fps                          | 795           |\n|    iterations                   | 134           |\n|    time_elapsed                 | 673           |\n|    total_timesteps              | 536000        |\n| train/                          |               |\n|    approx_kl                    | 0.00013034629 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.68         |\n|    explained_variance           | 0.242         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.84          |\n|    n_updates                    | 266           |\n|    policy_gradient_loss         | -2.43e-05     |\n|    value_loss                   | 12.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 134           |\n|    action_queue_updates_total   | 153           |\n|    ice_dug                      | 223           |\n|    water_produced               | 42.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.5          |\n| time/                           |               |\n|    fps                          | 796           |\n|    iterations                   | 135           |\n|    time_elapsed                 | 678           |\n|    total_timesteps              | 540000        |\n| train/                          |               |\n|    approx_kl                    | 0.00014052202 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.17          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 49.4          |\n|    n_updates                    | 268           |\n|    policy_gradient_loss         | 0.000323      |\n|    value_loss                   | 97.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 132           |\n|    action_queue_updates_total   | 151           |\n|    ice_dug                      | 27            |\n|    water_produced               | 6.5           |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21.5         |\n| time/                           |              |\n|    fps                          | 796          |\n|    iterations                   | 136          |\n|    time_elapsed                 | 682          |\n|    total_timesteps              | 544000       |\n| train/                          |              |\n|    approx_kl                    | 9.267153e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.67        |\n|    explained_variance           | 0.224        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 6.89         |\n|    n_updates                    | 270          |\n|    policy_gradient_loss         | 6.66e-05     |\n|    value_loss                   | 17.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 148          |\n|    ice_dug                      | 98           |\n|    water_produced               | 13           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.8          |\n| time/                           |               |\n|    fps                          | 797           |\n|    iterations                   | 137           |\n|    time_elapsed                 | 687           |\n|    total_timesteps              | 548000        |\n| train/                          |               |\n|    approx_kl                    | 0.00077834737 |\n|    clip_fraction                | 0.000125      |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.66         |\n|    explained_variance           | 0.203         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 14.7          |\n|    n_updates                    | 272           |\n|    policy_gradient_loss         | -0.000113     |\n|    value_loss                   | 30.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 31            |\n|    water_produced               | 5.75          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 15.7         |\n| time/                           |              |\n|    fps                          | 797          |\n|    iterations                   | 138          |\n|    time_elapsed                 | 692          |\n|    total_timesteps              | 552000       |\n| train/                          |              |\n|    approx_kl                    | 0.0006762225 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.68        |\n|    explained_variance           | 0.311        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 6.78         |\n|    n_updates                    | 274          |\n|    policy_gradient_loss         | -6.98e-05    |\n|    value_loss                   | 11.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 159          |\n|    ice_dug                      | 57           |\n|    water_produced               | 6.75         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 10.1         |\n| time/                           |              |\n|    fps                          | 797          |\n|    iterations                   | 139          |\n|    time_elapsed                 | 696          |\n|    total_timesteps              | 556000       |\n| train/                          |              |\n|    approx_kl                    | 0.0012264508 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.88        |\n|    explained_variance           | 0.308        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 8.9          |\n|    n_updates                    | 276          |\n|    policy_gradient_loss         | -0.000155    |\n|    value_loss                   | 13.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 153          |\n|    ice_dug                      | 91           |\n|    water_produced               | 15.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 14           |\n| time/                           |              |\n|    fps                          | 797          |\n|    iterations                   | 140          |\n|    time_elapsed                 | 701          |\n|    total_timesteps              | 560000       |\n| train/                          |              |\n|    approx_kl                    | 7.645832e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.85        |\n|    explained_variance           | 0.319        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 12.5         |\n|    n_updates                    | 278          |\n|    policy_gradient_loss         | 4.77e-05     |\n|    value_loss                   | 28.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 143          |\n|    action_queue_updates_total   | 154          |\n|    ice_dug                      | 151          |\n|    water_produced               | 24.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.3          |\n| time/                           |               |\n|    fps                          | 797           |\n|    iterations                   | 141           |\n|    time_elapsed                 | 706           |\n|    total_timesteps              | 564000        |\n| train/                          |               |\n|    approx_kl                    | 0.00021975087 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.73         |\n|    explained_variance           | 0.214         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.2          |\n|    n_updates                    | 280           |\n|    policy_gradient_loss         | -0.000121     |\n|    value_loss                   | 39.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 104           |\n|    water_produced               | 19.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 17.6          |\n| time/                           |               |\n|    fps                          | 798           |\n|    iterations                   | 142           |\n|    time_elapsed                 | 711           |\n|    total_timesteps              | 568000        |\n| train/                          |               |\n|    approx_kl                    | 0.00017151487 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.261         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 26.1          |\n|    n_updates                    | 282           |\n|    policy_gradient_loss         | -0.000131     |\n|    value_loss                   | 42.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 151           |\n|    ice_dug                      | 116           |\n|    water_produced               | 16.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 19.1          |\n| time/                           |               |\n|    fps                          | 798           |\n|    iterations                   | 143           |\n|    time_elapsed                 | 716           |\n|    total_timesteps              | 572000        |\n| train/                          |               |\n|    approx_kl                    | 0.00019878987 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.78         |\n|    explained_variance           | 0.23          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 19.2          |\n|    n_updates                    | 284           |\n|    policy_gradient_loss         | -0.000131     |\n|    value_loss                   | 35.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 104           |\n|    water_produced               | 14            |\n---------------------------------------------------\nEval num_timesteps=576000, episode_reward=0.00 +/- 0.00\nEpisode length: 301.00 +/- 0.00\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 301           |\n|    mean_reward                  | 0             |\n| time/                           |               |\n|    total_timesteps              | 576000        |\n| train/                          |               |\n|    approx_kl                    | 0.00042357476 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.77         |\n|    explained_variance           | 0.296         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 13.4          |\n|    n_updates                    | 286           |\n|    policy_gradient_loss         | -0.000252     |\n|    value_loss                   | 31.2          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 152           |\n|    ice_dug                      | 94            |\n|    water_produced               | 8.5           |\n---------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 17.7     |\n| time/              |          |\n|    fps             | 795      |\n|    iterations      | 144      |\n|    time_elapsed    | 724      |\n|    total_timesteps | 576000   |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 15.6         |\n| time/                           |              |\n|    fps                          | 795          |\n|    iterations                   | 145          |\n|    time_elapsed                 | 729          |\n|    total_timesteps              | 580000       |\n| train/                          |              |\n|    approx_kl                    | 0.0014275485 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.85        |\n|    explained_variance           | 0.499        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 6.3          |\n|    n_updates                    | 288          |\n|    policy_gradient_loss         | -0.000848    |\n|    value_loss                   | 10.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 91           |\n|    water_produced               | 14.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 16           |\n| time/                           |              |\n|    fps                          | 795          |\n|    iterations                   | 146          |\n|    time_elapsed                 | 734          |\n|    total_timesteps              | 584000       |\n| train/                          |              |\n|    approx_kl                    | 0.0005172007 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.65        |\n|    explained_variance           | 0.301        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 18.3         |\n|    n_updates                    | 290          |\n|    policy_gradient_loss         | -0.000417    |\n|    value_loss                   | 28.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 155          |\n|    ice_dug                      | 188          |\n|    water_produced               | 21           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.6         |\n| time/                           |              |\n|    fps                          | 795          |\n|    iterations                   | 147          |\n|    time_elapsed                 | 739          |\n|    total_timesteps              | 588000       |\n| train/                          |              |\n|    approx_kl                    | 0.0008119313 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.71        |\n|    explained_variance           | 0.279        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 24.7         |\n|    n_updates                    | 292          |\n|    policy_gradient_loss         | -0.000112    |\n|    value_loss                   | 44.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 130          |\n|    water_produced               | 28.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 17.2         |\n| time/                           |              |\n|    fps                          | 795          |\n|    iterations                   | 148          |\n|    time_elapsed                 | 743          |\n|    total_timesteps              | 592000       |\n| train/                          |              |\n|    approx_kl                    | 0.0005473661 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.74        |\n|    explained_variance           | 0.247        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 31.2         |\n|    n_updates                    | 294          |\n|    policy_gradient_loss         | -0.000323    |\n|    value_loss                   | 56.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 40           |\n|    water_produced               | 7.75         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 19.7          |\n| time/                           |               |\n|    fps                          | 796           |\n|    iterations                   | 149           |\n|    time_elapsed                 | 748           |\n|    total_timesteps              | 596000        |\n| train/                          |               |\n|    approx_kl                    | 0.00036622933 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.402         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.83          |\n|    n_updates                    | 296           |\n|    policy_gradient_loss         | 0.000144      |\n|    value_loss                   | 20            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 156           |\n|    ice_dug                      | 136           |\n|    water_produced               | 20.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21           |\n| time/                           |              |\n|    fps                          | 795          |\n|    iterations                   | 150          |\n|    time_elapsed                 | 753          |\n|    total_timesteps              | 600000       |\n| train/                          |              |\n|    approx_kl                    | 0.0002547471 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.77        |\n|    explained_variance           | 0.301        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 20.6         |\n|    n_updates                    | 298          |\n|    policy_gradient_loss         | 0.000133     |\n|    value_loss                   | 50.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 154          |\n|    ice_dug                      | 116          |\n|    water_produced               | 21           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.7          |\n| time/                           |               |\n|    fps                          | 796           |\n|    iterations                   | 151           |\n|    time_elapsed                 | 758           |\n|    total_timesteps              | 604000        |\n| train/                          |               |\n|    approx_kl                    | 0.00018713754 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.78         |\n|    explained_variance           | 0.31          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.9          |\n|    n_updates                    | 300           |\n|    policy_gradient_loss         | -5.62e-05     |\n|    value_loss                   | 49.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 121           |\n|    water_produced               | 25            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.2          |\n| time/                           |               |\n|    fps                          | 796           |\n|    iterations                   | 152           |\n|    time_elapsed                 | 763           |\n|    total_timesteps              | 608000        |\n| train/                          |               |\n|    approx_kl                    | 0.00012084496 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.71         |\n|    explained_variance           | 0.288         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 44.4          |\n|    n_updates                    | 302           |\n|    policy_gradient_loss         | -9.99e-05     |\n|    value_loss                   | 68.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 129           |\n|    action_queue_updates_total   | 145           |\n|    ice_dug                      | 186           |\n|    water_produced               | 20.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.9          |\n| time/                           |               |\n|    fps                          | 796           |\n|    iterations                   | 153           |\n|    time_elapsed                 | 768           |\n|    total_timesteps              | 612000        |\n| train/                          |               |\n|    approx_kl                    | 0.00012577753 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.65         |\n|    explained_variance           | 0.36          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.3          |\n|    n_updates                    | 304           |\n|    policy_gradient_loss         | 1.96e-05      |\n|    value_loss                   | 39.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 134           |\n|    action_queue_updates_total   | 157           |\n|    ice_dug                      | 146           |\n|    water_produced               | 15.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.8          |\n| time/                           |               |\n|    fps                          | 796           |\n|    iterations                   | 154           |\n|    time_elapsed                 | 773           |\n|    total_timesteps              | 616000        |\n| train/                          |               |\n|    approx_kl                    | 0.00021348354 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.78         |\n|    explained_variance           | 0.4           |\n|    learning_rate                | 0.0003        |\n|    loss                         | 14.2          |\n|    n_updates                    | 306           |\n|    policy_gradient_loss         | 0.000114      |\n|    value_loss                   | 29.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 148           |\n|    ice_dug                      | 140           |\n|    water_produced               | 19.8          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 20.4         |\n| time/                           |              |\n|    fps                          | 796          |\n|    iterations                   | 155          |\n|    time_elapsed                 | 778          |\n|    total_timesteps              | 620000       |\n| train/                          |              |\n|    approx_kl                    | 0.0011422962 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.71        |\n|    explained_variance           | 0.439        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 13.3         |\n|    n_updates                    | 308          |\n|    policy_gradient_loss         | -0.000333    |\n|    value_loss                   | 30.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 86           |\n|    water_produced               | 14.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 17.4         |\n| time/                           |              |\n|    fps                          | 796          |\n|    iterations                   | 156          |\n|    time_elapsed                 | 783          |\n|    total_timesteps              | 624000       |\n| train/                          |              |\n|    approx_kl                    | 0.0018914065 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.86        |\n|    explained_variance           | 0.511        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.6         |\n|    n_updates                    | 310          |\n|    policy_gradient_loss         | 1.37e-05     |\n|    value_loss                   | 25.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 157          |\n|    ice_dug                      | 51           |\n|    water_produced               | 10.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 17.9          |\n| time/                           |               |\n|    fps                          | 796           |\n|    iterations                   | 157           |\n|    time_elapsed                 | 788           |\n|    total_timesteps              | 628000        |\n| train/                          |               |\n|    approx_kl                    | 0.00040925355 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.92         |\n|    explained_variance           | 0.603         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.77          |\n|    n_updates                    | 312           |\n|    policy_gradient_loss         | -0.000366     |\n|    value_loss                   | 15.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 153           |\n|    ice_dug                      | 132           |\n|    water_produced               | 23.8          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.6         |\n| time/                           |              |\n|    fps                          | 797          |\n|    iterations                   | 158          |\n|    time_elapsed                 | 792          |\n|    total_timesteps              | 632000       |\n| train/                          |              |\n|    approx_kl                    | 0.0014935824 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.73        |\n|    explained_variance           | 0.427        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 21.4         |\n|    n_updates                    | 314          |\n|    policy_gradient_loss         | -0.000123    |\n|    value_loss                   | 50           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 114          |\n|    water_produced               | 19           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.2          |\n| time/                           |               |\n|    fps                          | 797           |\n|    iterations                   | 159           |\n|    time_elapsed                 | 797           |\n|    total_timesteps              | 636000        |\n| train/                          |               |\n|    approx_kl                    | 5.0288927e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.64         |\n|    explained_variance           | 0.321         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.3          |\n|    n_updates                    | 316           |\n|    policy_gradient_loss         | 1.61e-05      |\n|    value_loss                   | 37.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 140           |\n|    ice_dug                      | 23            |\n|    water_produced               | 4             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.5          |\n| time/                           |               |\n|    fps                          | 798           |\n|    iterations                   | 160           |\n|    time_elapsed                 | 801           |\n|    total_timesteps              | 640000        |\n| train/                          |               |\n|    approx_kl                    | 0.00052962865 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.59         |\n|    explained_variance           | 0.564         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 2.88          |\n|    n_updates                    | 318           |\n|    policy_gradient_loss         | -0.00019      |\n|    value_loss                   | 10.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 147           |\n|    ice_dug                      | 88            |\n|    water_produced               | 15.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.9          |\n| time/                           |               |\n|    fps                          | 798           |\n|    iterations                   | 161           |\n|    time_elapsed                 | 806           |\n|    total_timesteps              | 644000        |\n| train/                          |               |\n|    approx_kl                    | 0.00015061807 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.69         |\n|    explained_variance           | 0.367         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.5          |\n|    n_updates                    | 320           |\n|    policy_gradient_loss         | -0.000371     |\n|    value_loss                   | 36.3          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 143           |\n|    ice_dug                      | 76            |\n|    water_produced               | 12.5          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 14.2          |\n| time/                           |               |\n|    fps                          | 799           |\n|    iterations                   | 162           |\n|    time_elapsed                 | 810           |\n|    total_timesteps              | 648000        |\n| train/                          |               |\n|    approx_kl                    | 0.00020718158 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.58         |\n|    explained_variance           | 0.339         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 12.1          |\n|    n_updates                    | 322           |\n|    policy_gradient_loss         | -0.000387     |\n|    value_loss                   | 31            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 123           |\n|    action_queue_updates_total   | 144           |\n|    ice_dug                      | 75            |\n|    water_produced               | 15.7          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.7          |\n| time/                           |               |\n|    fps                          | 799           |\n|    iterations                   | 163           |\n|    time_elapsed                 | 815           |\n|    total_timesteps              | 652000        |\n| train/                          |               |\n|    approx_kl                    | 0.00020122866 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.69         |\n|    explained_variance           | 0.43          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 17.4          |\n|    n_updates                    | 324           |\n|    policy_gradient_loss         | -0.000158     |\n|    value_loss                   | 31            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 123           |\n|    action_queue_updates_total   | 139           |\n|    ice_dug                      | 55            |\n|    water_produced               | 7.25          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.7          |\n| time/                           |               |\n|    fps                          | 799           |\n|    iterations                   | 164           |\n|    time_elapsed                 | 820           |\n|    total_timesteps              | 656000        |\n| train/                          |               |\n|    approx_kl                    | 0.00018896077 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.63         |\n|    explained_variance           | 0.513         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 5.26          |\n|    n_updates                    | 326           |\n|    policy_gradient_loss         | 2.01e-05      |\n|    value_loss                   | 10.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 146           |\n|    ice_dug                      | 159           |\n|    water_produced               | 22.5          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.6          |\n| time/                           |               |\n|    fps                          | 800           |\n|    iterations                   | 165           |\n|    time_elapsed                 | 824           |\n|    total_timesteps              | 660000        |\n| train/                          |               |\n|    approx_kl                    | 0.00017486085 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.67         |\n|    explained_variance           | 0.266         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 34.5          |\n|    n_updates                    | 328           |\n|    policy_gradient_loss         | -0.000219     |\n|    value_loss                   | 62.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 125           |\n|    action_queue_updates_total   | 141           |\n|    ice_dug                      | 85            |\n|    water_produced               | 20.2          |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 17          |\n| time/                           |             |\n|    fps                          | 800         |\n|    iterations                   | 166         |\n|    time_elapsed                 | 829         |\n|    total_timesteps              | 664000      |\n| train/                          |             |\n|    approx_kl                    | 0.001538135 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.6        |\n|    explained_variance           | 0.223       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 33.3        |\n|    n_updates                    | 330         |\n|    policy_gradient_loss         | -0.000144   |\n|    value_loss                   | 55.4        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 130         |\n|    action_queue_updates_total   | 141         |\n|    ice_dug                      | 81          |\n|    water_produced               | 14.8        |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.1          |\n| time/                           |               |\n|    fps                          | 800           |\n|    iterations                   | 167           |\n|    time_elapsed                 | 834           |\n|    total_timesteps              | 668000        |\n| train/                          |               |\n|    approx_kl                    | 0.00019238265 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.55         |\n|    explained_variance           | 0.306         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.8          |\n|    n_updates                    | 332           |\n|    policy_gradient_loss         | -0.000152     |\n|    value_loss                   | 33.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 142           |\n|    ice_dug                      | 118           |\n|    water_produced               | 21            |\n---------------------------------------------------\nEval num_timesteps=672000, episode_reward=5.24 +/- 8.85\nEpisode length: 305.00 +/- 8.00\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 305           |\n|    mean_reward                  | 5.24          |\n| time/                           |               |\n|    total_timesteps              | 672000        |\n| train/                          |               |\n|    approx_kl                    | 0.00022197426 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.64         |\n|    explained_variance           | 0.314         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 21.6          |\n|    n_updates                    | 334           |\n|    policy_gradient_loss         | -0.000167     |\n|    value_loss                   | 38            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 121           |\n|    action_queue_updates_total   | 135           |\n|    ice_dug                      | 82            |\n|    water_produced               | 13.5          |\n---------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 19.5     |\n| time/              |          |\n|    fps             | 798      |\n|    iterations      | 168      |\n|    time_elapsed    | 841      |\n|    total_timesteps | 672000   |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.8         |\n| time/                           |              |\n|    fps                          | 799          |\n|    iterations                   | 169          |\n|    time_elapsed                 | 845          |\n|    total_timesteps              | 676000       |\n| train/                          |              |\n|    approx_kl                    | 0.0006194165 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.58        |\n|    explained_variance           | 0.306        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 15.8         |\n|    n_updates                    | 336          |\n|    policy_gradient_loss         | -2.08e-05    |\n|    value_loss                   | 30.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 122          |\n|    water_produced               | 24.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 19.6          |\n| time/                           |               |\n|    fps                          | 799           |\n|    iterations                   | 170           |\n|    time_elapsed                 | 850           |\n|    total_timesteps              | 680000        |\n| train/                          |               |\n|    approx_kl                    | 0.00012223788 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.6          |\n|    explained_variance           | 0.286         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 26.5          |\n|    n_updates                    | 338           |\n|    policy_gradient_loss         | -0.000216     |\n|    value_loss                   | 45.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 141           |\n|    ice_dug                      | 108           |\n|    water_produced               | 19.3          |\n---------------------------------------------------\n----------------------------------------------------\n| rollout/                        |                |\n|    ep_len_mean                  | 200            |\n|    ep_rew_mean                  | 25.4           |\n| time/                           |                |\n|    fps                          | 800            |\n|    iterations                   | 171            |\n|    time_elapsed                 | 854            |\n|    total_timesteps              | 684000         |\n| train/                          |                |\n|    approx_kl                    | 0.000103289916 |\n|    clip_fraction                | 0              |\n|    clip_range                   | 0.2            |\n|    entropy_loss                 | -1.56          |\n|    explained_variance           | 0.256          |\n|    learning_rate                | 0.0003         |\n|    loss                         | 18.5           |\n|    n_updates                    | 340            |\n|    policy_gradient_loss         | -0.000325      |\n|    value_loss                   | 36.8           |\n| train_metrics/                  |                |\n|    action_queue_updates_success | 136            |\n|    action_queue_updates_total   | 147            |\n|    ice_dug                      | 207            |\n|    water_produced               | 42.2           |\n----------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 23.6          |\n| time/                           |               |\n|    fps                          | 801           |\n|    iterations                   | 172           |\n|    time_elapsed                 | 858           |\n|    total_timesteps              | 688000        |\n| train/                          |               |\n|    approx_kl                    | 0.00017056346 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.59         |\n|    explained_variance           | 0.269         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 41.8          |\n|    n_updates                    | 342           |\n|    policy_gradient_loss         | 7.64e-05      |\n|    value_loss                   | 81.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 138           |\n|    ice_dug                      | 62            |\n|    water_produced               | 12.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.5          |\n| time/                           |               |\n|    fps                          | 801           |\n|    iterations                   | 173           |\n|    time_elapsed                 | 862           |\n|    total_timesteps              | 692000        |\n| train/                          |               |\n|    approx_kl                    | 2.4299025e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.48         |\n|    explained_variance           | 0.315         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.8          |\n|    n_updates                    | 344           |\n|    policy_gradient_loss         | 3.53e-05      |\n|    value_loss                   | 27            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 25            |\n|    water_produced               | 3.75          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 23.4         |\n| time/                           |              |\n|    fps                          | 802          |\n|    iterations                   | 174          |\n|    time_elapsed                 | 867          |\n|    total_timesteps              | 696000       |\n| train/                          |              |\n|    approx_kl                    | 0.0011129864 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.7         |\n|    explained_variance           | 0.577        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 2.59         |\n|    n_updates                    | 346          |\n|    policy_gradient_loss         | -0.000458    |\n|    value_loss                   | 6.08         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 150          |\n|    ice_dug                      | 192          |\n|    water_produced               | 33.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 22           |\n| time/                           |              |\n|    fps                          | 803          |\n|    iterations                   | 175          |\n|    time_elapsed                 | 871          |\n|    total_timesteps              | 700000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003357281 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.7         |\n|    explained_variance           | 0.276        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 22.8         |\n|    n_updates                    | 348          |\n|    policy_gradient_loss         | -0.000153    |\n|    value_loss                   | 72           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 154          |\n|    ice_dug                      | 109          |\n|    water_produced               | 12           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.2          |\n| time/                           |               |\n|    fps                          | 803           |\n|    iterations                   | 176           |\n|    time_elapsed                 | 875           |\n|    total_timesteps              | 704000        |\n| train/                          |               |\n|    approx_kl                    | 0.00026154923 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.8          |\n|    explained_variance           | 0.318         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 11.9          |\n|    n_updates                    | 350           |\n|    policy_gradient_loss         | -0.000216     |\n|    value_loss                   | 28.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 49            |\n|    water_produced               | 9.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15            |\n| time/                           |               |\n|    fps                          | 803           |\n|    iterations                   | 177           |\n|    time_elapsed                 | 880           |\n|    total_timesteps              | 708000        |\n| train/                          |               |\n|    approx_kl                    | 0.00026825207 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.74         |\n|    explained_variance           | 0.428         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.13          |\n|    n_updates                    | 352           |\n|    policy_gradient_loss         | 0.00012       |\n|    value_loss                   | 12.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 70            |\n|    water_produced               | 12            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.8          |\n| time/                           |               |\n|    fps                          | 804           |\n|    iterations                   | 178           |\n|    time_elapsed                 | 885           |\n|    total_timesteps              | 712000        |\n| train/                          |               |\n|    approx_kl                    | 0.00066508556 |\n|    clip_fraction                | 0.00137       |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.76         |\n|    explained_variance           | 0.359         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 14.6          |\n|    n_updates                    | 354           |\n|    policy_gradient_loss         | 0.000122      |\n|    value_loss                   | 22.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 158           |\n|    ice_dug                      | 117           |\n|    water_produced               | 21.7          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.3          |\n| time/                           |               |\n|    fps                          | 804           |\n|    iterations                   | 179           |\n|    time_elapsed                 | 889           |\n|    total_timesteps              | 716000        |\n| train/                          |               |\n|    approx_kl                    | 0.00093234255 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.88         |\n|    explained_variance           | 0.368         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.5          |\n|    n_updates                    | 356           |\n|    policy_gradient_loss         | 0.000118      |\n|    value_loss                   | 42.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 109           |\n|    water_produced               | 21.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 14.7         |\n| time/                           |              |\n|    fps                          | 805          |\n|    iterations                   | 180          |\n|    time_elapsed                 | 893          |\n|    total_timesteps              | 720000       |\n| train/                          |              |\n|    approx_kl                    | 0.0011250575 |\n|    clip_fraction                | 0.00075      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.79        |\n|    explained_variance           | 0.31         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 22           |\n|    n_updates                    | 358          |\n|    policy_gradient_loss         | -0.000466    |\n|    value_loss                   | 54.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 157          |\n|    ice_dug                      | 28           |\n|    water_produced               | 5            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.4         |\n| time/                           |              |\n|    fps                          | 805          |\n|    iterations                   | 181          |\n|    time_elapsed                 | 898          |\n|    total_timesteps              | 724000       |\n| train/                          |              |\n|    approx_kl                    | 0.0038592597 |\n|    clip_fraction                | 0.0161       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.93        |\n|    explained_variance           | 0.741        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 2.37         |\n|    n_updates                    | 360          |\n|    policy_gradient_loss         | -0.000127    |\n|    value_loss                   | 5.41         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 147          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 152          |\n|    water_produced               | 31.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.3          |\n| time/                           |               |\n|    fps                          | 806           |\n|    iterations                   | 182           |\n|    time_elapsed                 | 902           |\n|    total_timesteps              | 728000        |\n| train/                          |               |\n|    approx_kl                    | 0.00011359898 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.68         |\n|    explained_variance           | 0.308         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 37.3          |\n|    n_updates                    | 362           |\n|    policy_gradient_loss         | -4.03e-05     |\n|    value_loss                   | 59.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 147           |\n|    action_queue_updates_total   | 162           |\n|    ice_dug                      | 128           |\n|    water_produced               | 16.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.1          |\n| time/                           |               |\n|    fps                          | 806           |\n|    iterations                   | 183           |\n|    time_elapsed                 | 907           |\n|    total_timesteps              | 732000        |\n| train/                          |               |\n|    approx_kl                    | 6.9310365e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.83         |\n|    explained_variance           | 0.397         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 16.5          |\n|    n_updates                    | 364           |\n|    policy_gradient_loss         | 7.25e-06      |\n|    value_loss                   | 33.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 143           |\n|    action_queue_updates_total   | 156           |\n|    ice_dug                      | 70            |\n|    water_produced               | 11            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21.9         |\n| time/                           |              |\n|    fps                          | 807          |\n|    iterations                   | 184          |\n|    time_elapsed                 | 911          |\n|    total_timesteps              | 736000       |\n| train/                          |              |\n|    approx_kl                    | 0.0009790689 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.8         |\n|    explained_variance           | 0.48         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 10.1         |\n|    n_updates                    | 366          |\n|    policy_gradient_loss         | 0.000889     |\n|    value_loss                   | 18.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 161          |\n|    ice_dug                      | 238          |\n|    water_produced               | 39.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 23.2         |\n| time/                           |              |\n|    fps                          | 807          |\n|    iterations                   | 185          |\n|    time_elapsed                 | 916          |\n|    total_timesteps              | 740000       |\n| train/                          |              |\n|    approx_kl                    | 0.0015198134 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.88        |\n|    explained_variance           | 0.389        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 43.4         |\n|    n_updates                    | 368          |\n|    policy_gradient_loss         | -0.000326    |\n|    value_loss                   | 72.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 137          |\n|    action_queue_updates_total   | 160          |\n|    ice_dug                      | 138          |\n|    water_produced               | 10.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 19            |\n| time/                           |               |\n|    fps                          | 808           |\n|    iterations                   | 186           |\n|    time_elapsed                 | 920           |\n|    total_timesteps              | 744000        |\n| train/                          |               |\n|    approx_kl                    | 0.00016919918 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.91         |\n|    explained_variance           | 0.623         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.58          |\n|    n_updates                    | 370           |\n|    policy_gradient_loss         | -0.000173     |\n|    value_loss                   | 15.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 163           |\n|    ice_dug                      | 68            |\n|    water_produced               | 11.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.8         |\n| time/                           |              |\n|    fps                          | 808          |\n|    iterations                   | 187          |\n|    time_elapsed                 | 925          |\n|    total_timesteps              | 748000       |\n| train/                          |              |\n|    approx_kl                    | 0.0016052086 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.98        |\n|    explained_variance           | 0.625        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 10.4         |\n|    n_updates                    | 372          |\n|    policy_gradient_loss         | -3.54e-05    |\n|    value_loss                   | 18.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 154          |\n|    ice_dug                      | 102          |\n|    water_produced               | 20.5         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 19.8        |\n| time/                           |             |\n|    fps                          | 808         |\n|    iterations                   | 188         |\n|    time_elapsed                 | 929         |\n|    total_timesteps              | 752000      |\n| train/                          |             |\n|    approx_kl                    | 0.000202163 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.83       |\n|    explained_variance           | 0.482       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 15.2        |\n|    n_updates                    | 374         |\n|    policy_gradient_loss         | 0.000234    |\n|    value_loss                   | 35.6        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 138         |\n|    action_queue_updates_total   | 162         |\n|    ice_dug                      | 79          |\n|    water_produced               | 10.8        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 13.8         |\n| time/                           |              |\n|    fps                          | 808          |\n|    iterations                   | 189          |\n|    time_elapsed                 | 934          |\n|    total_timesteps              | 756000       |\n| train/                          |              |\n|    approx_kl                    | 0.0021645874 |\n|    clip_fraction                | 0.000375     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.96        |\n|    explained_variance           | 0.524        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 7.79         |\n|    n_updates                    | 376          |\n|    policy_gradient_loss         | 3.79e-05     |\n|    value_loss                   | 23.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 137          |\n|    action_queue_updates_total   | 154          |\n|    ice_dug                      | 66           |\n|    water_produced               | 11.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.4          |\n| time/                           |               |\n|    fps                          | 808           |\n|    iterations                   | 190           |\n|    time_elapsed                 | 939           |\n|    total_timesteps              | 760000        |\n| train/                          |               |\n|    approx_kl                    | 0.00049861905 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.86         |\n|    explained_variance           | 0.602         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 11.7          |\n|    n_updates                    | 378           |\n|    policy_gradient_loss         | -4.28e-05     |\n|    value_loss                   | 21.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 142           |\n|    action_queue_updates_total   | 157           |\n|    ice_dug                      | 40            |\n|    water_produced               | 9             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 17.1          |\n| time/                           |               |\n|    fps                          | 808           |\n|    iterations                   | 191           |\n|    time_elapsed                 | 944           |\n|    total_timesteps              | 764000        |\n| train/                          |               |\n|    approx_kl                    | 0.00056018506 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.83         |\n|    explained_variance           | 0.633         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 11            |\n|    n_updates                    | 380           |\n|    policy_gradient_loss         | -0.000282     |\n|    value_loss                   | 17.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 156           |\n|    ice_dug                      | 139           |\n|    water_produced               | 29.2          |\n---------------------------------------------------\nEval num_timesteps=768000, episode_reward=0.00 +/- 0.00\nEpisode length: 301.00 +/- 0.00\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 301          |\n|    mean_reward                  | 0            |\n| time/                           |              |\n|    total_timesteps              | 768000       |\n| train/                          |              |\n|    approx_kl                    | 0.0015369231 |\n|    clip_fraction                | 0.00262      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.87        |\n|    explained_variance           | 0.458        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 32           |\n|    n_updates                    | 382          |\n|    policy_gradient_loss         | -0.000529    |\n|    value_loss                   | 57.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 146          |\n|    ice_dug                      | 158          |\n|    water_produced               | 29.2         |\n--------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 18.9     |\n| time/              |          |\n|    fps             | 806      |\n|    iterations      | 192      |\n|    time_elapsed    | 952      |\n|    total_timesteps | 768000   |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.3         |\n| time/                           |              |\n|    fps                          | 806          |\n|    iterations                   | 193          |\n|    time_elapsed                 | 957          |\n|    total_timesteps              | 772000       |\n| train/                          |              |\n|    approx_kl                    | 0.0011260008 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.7         |\n|    explained_variance           | 0.402        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 34.6         |\n|    n_updates                    | 384          |\n|    policy_gradient_loss         | -2.04e-05    |\n|    value_loss                   | 61.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 42           |\n|    water_produced               | 8            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 21.5          |\n| time/                           |               |\n|    fps                          | 806           |\n|    iterations                   | 194           |\n|    time_elapsed                 | 962           |\n|    total_timesteps              | 776000        |\n| train/                          |               |\n|    approx_kl                    | 0.00096253026 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.79         |\n|    explained_variance           | 0.604         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.78          |\n|    n_updates                    | 386           |\n|    policy_gradient_loss         | 0.000483      |\n|    value_loss                   | 14.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 153           |\n|    ice_dug                      | 133           |\n|    water_produced               | 26.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 25.5          |\n| time/                           |               |\n|    fps                          | 806           |\n|    iterations                   | 195           |\n|    time_elapsed                 | 967           |\n|    total_timesteps              | 780000        |\n| train/                          |               |\n|    approx_kl                    | 0.00034384383 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.8          |\n|    explained_variance           | 0.485         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 21.4          |\n|    n_updates                    | 388           |\n|    policy_gradient_loss         | -6.56e-05     |\n|    value_loss                   | 43.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 151           |\n|    ice_dug                      | 137           |\n|    water_produced               | 28.2          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21.5         |\n| time/                           |              |\n|    fps                          | 806          |\n|    iterations                   | 196          |\n|    time_elapsed                 | 972          |\n|    total_timesteps              | 784000       |\n| train/                          |              |\n|    approx_kl                    | 0.0021298998 |\n|    clip_fraction                | 0.00212      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.76        |\n|    explained_variance           | 0.362        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 34.3         |\n|    n_updates                    | 390          |\n|    policy_gradient_loss         | -0.000806    |\n|    value_loss                   | 57           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 140          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 58           |\n|    water_produced               | 10           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 19.2          |\n| time/                           |               |\n|    fps                          | 806           |\n|    iterations                   | 197           |\n|    time_elapsed                 | 977           |\n|    total_timesteps              | 788000        |\n| train/                          |               |\n|    approx_kl                    | 0.00014952707 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.81         |\n|    explained_variance           | 0.471         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 9             |\n|    n_updates                    | 392           |\n|    policy_gradient_loss         | -0.000124     |\n|    value_loss                   | 24.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 148           |\n|    ice_dug                      | 108           |\n|    water_produced               | 18            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.7          |\n| time/                           |               |\n|    fps                          | 806           |\n|    iterations                   | 198           |\n|    time_elapsed                 | 982           |\n|    total_timesteps              | 792000        |\n| train/                          |               |\n|    approx_kl                    | 0.00022887201 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.69         |\n|    explained_variance           | 0.469         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 11.4          |\n|    n_updates                    | 394           |\n|    policy_gradient_loss         | -4.75e-05     |\n|    value_loss                   | 28.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 148           |\n|    ice_dug                      | 126           |\n|    water_produced               | 14.7          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 20.9         |\n| time/                           |              |\n|    fps                          | 806          |\n|    iterations                   | 199          |\n|    time_elapsed                 | 986          |\n|    total_timesteps              | 796000       |\n| train/                          |              |\n|    approx_kl                    | 0.0001751883 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.67        |\n|    explained_variance           | 0.319        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 18.6         |\n|    n_updates                    | 396          |\n|    policy_gradient_loss         | -0.000129    |\n|    value_loss                   | 41.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 148          |\n|    ice_dug                      | 121          |\n|    water_produced               | 27.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.1         |\n| time/                           |              |\n|    fps                          | 806          |\n|    iterations                   | 200          |\n|    time_elapsed                 | 991          |\n|    total_timesteps              | 800000       |\n| train/                          |              |\n|    approx_kl                    | 0.0015025137 |\n|    clip_fraction                | 0.000875     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.75        |\n|    explained_variance           | 0.349        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 30.4         |\n|    n_updates                    | 398          |\n|    policy_gradient_loss         | -0.000833    |\n|    value_loss                   | 62.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 111          |\n|    water_produced               | 20           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 17.6         |\n| time/                           |              |\n|    fps                          | 807          |\n|    iterations                   | 201          |\n|    time_elapsed                 | 996          |\n|    total_timesteps              | 804000       |\n| train/                          |              |\n|    approx_kl                    | 0.0001693565 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.64        |\n|    explained_variance           | 0.396        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 22.1         |\n|    n_updates                    | 400          |\n|    policy_gradient_loss         | 9.85e-05     |\n|    value_loss                   | 35.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 39           |\n|    water_produced               | 2.25         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 20.2         |\n| time/                           |              |\n|    fps                          | 807          |\n|    iterations                   | 202          |\n|    time_elapsed                 | 1000         |\n|    total_timesteps              | 808000       |\n| train/                          |              |\n|    approx_kl                    | 0.0015583395 |\n|    clip_fraction                | 0.000125     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.68        |\n|    explained_variance           | 0.712        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 2.72         |\n|    n_updates                    | 402          |\n|    policy_gradient_loss         | -1.11e-05    |\n|    value_loss                   | 6.37         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 144          |\n|    ice_dug                      | 173          |\n|    water_produced               | 30.7         |\n--------------------------------------------------\n----------------------------------------------------\n| rollout/                        |                |\n|    ep_len_mean                  | 200            |\n|    ep_rew_mean                  | 22.8           |\n| time/                           |                |\n|    fps                          | 807            |\n|    iterations                   | 203            |\n|    time_elapsed                 | 1005           |\n|    total_timesteps              | 812000         |\n| train/                          |                |\n|    approx_kl                    | 0.000101483776 |\n|    clip_fraction                | 0              |\n|    clip_range                   | 0.2            |\n|    entropy_loss                 | -1.65          |\n|    explained_variance           | 0.353          |\n|    learning_rate                | 0.0003         |\n|    loss                         | 36.9           |\n|    n_updates                    | 404            |\n|    policy_gradient_loss         | -7.24e-05      |\n|    value_loss                   | 64             |\n| train_metrics/                  |                |\n|    action_queue_updates_success | 130            |\n|    action_queue_updates_total   | 144            |\n|    ice_dug                      | 128            |\n|    water_produced               | 27.7           |\n----------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 22.9         |\n| time/                           |              |\n|    fps                          | 807          |\n|    iterations                   | 204          |\n|    time_elapsed                 | 1010         |\n|    total_timesteps              | 816000       |\n| train/                          |              |\n|    approx_kl                    | 0.0007318262 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.63        |\n|    explained_variance           | 0.333        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 31.7         |\n|    n_updates                    | 406          |\n|    policy_gradient_loss         | -0.000503    |\n|    value_loss                   | 65.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 140          |\n|    action_queue_updates_total   | 153          |\n|    ice_dug                      | 145          |\n|    water_produced               | 27.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 26.9          |\n| time/                           |               |\n|    fps                          | 807           |\n|    iterations                   | 205           |\n|    time_elapsed                 | 1015          |\n|    total_timesteps              | 820000        |\n| train/                          |               |\n|    approx_kl                    | 0.00013486322 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.68         |\n|    explained_variance           | 0.363         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 33.7          |\n|    n_updates                    | 408           |\n|    policy_gradient_loss         | -0.000349     |\n|    value_loss                   | 70.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 134           |\n|    action_queue_updates_total   | 144           |\n|    ice_dug                      | 212           |\n|    water_produced               | 39.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 28.9          |\n| time/                           |               |\n|    fps                          | 807           |\n|    iterations                   | 206           |\n|    time_elapsed                 | 1020          |\n|    total_timesteps              | 824000        |\n| train/                          |               |\n|    approx_kl                    | 0.00041726144 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.55         |\n|    explained_variance           | 0.317         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 42.5          |\n|    n_updates                    | 410           |\n|    policy_gradient_loss         | -0.000502     |\n|    value_loss                   | 92.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 146           |\n|    ice_dug                      | 58            |\n|    water_produced               | 12            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 26.3          |\n| time/                           |               |\n|    fps                          | 807           |\n|    iterations                   | 207           |\n|    time_elapsed                 | 1025          |\n|    total_timesteps              | 828000        |\n| train/                          |               |\n|    approx_kl                    | 0.00026639557 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.64         |\n|    explained_variance           | 0.41          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 14.5          |\n|    n_updates                    | 412           |\n|    policy_gradient_loss         | -2.44e-05     |\n|    value_loss                   | 31.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 126           |\n|    action_queue_updates_total   | 140           |\n|    ice_dug                      | 99            |\n|    water_produced               | 18.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 26.5          |\n| time/                           |               |\n|    fps                          | 807           |\n|    iterations                   | 208           |\n|    time_elapsed                 | 1030          |\n|    total_timesteps              | 832000        |\n| train/                          |               |\n|    approx_kl                    | 0.00089280866 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.59         |\n|    explained_variance           | 0.315         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 27            |\n|    n_updates                    | 414           |\n|    policy_gradient_loss         | -8.18e-05     |\n|    value_loss                   | 51.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 143           |\n|    ice_dug                      | 177           |\n|    water_produced               | 28.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 25.5          |\n| time/                           |               |\n|    fps                          | 807           |\n|    iterations                   | 209           |\n|    time_elapsed                 | 1034          |\n|    total_timesteps              | 836000        |\n| train/                          |               |\n|    approx_kl                    | 0.00011344401 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.61         |\n|    explained_variance           | 0.33          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 36            |\n|    n_updates                    | 416           |\n|    policy_gradient_loss         | -0.000251     |\n|    value_loss                   | 86.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 145           |\n|    ice_dug                      | 131           |\n|    water_produced               | 22.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.9          |\n| time/                           |               |\n|    fps                          | 807           |\n|    iterations                   | 210           |\n|    time_elapsed                 | 1039          |\n|    total_timesteps              | 840000        |\n| train/                          |               |\n|    approx_kl                    | 0.00014708556 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.58         |\n|    explained_variance           | 0.368         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 29.8          |\n|    n_updates                    | 418           |\n|    policy_gradient_loss         | -0.000244     |\n|    value_loss                   | 50.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 120           |\n|    action_queue_updates_total   | 133           |\n|    ice_dug                      | 82            |\n|    water_produced               | 7.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.3          |\n| time/                           |               |\n|    fps                          | 807           |\n|    iterations                   | 211           |\n|    time_elapsed                 | 1044          |\n|    total_timesteps              | 844000        |\n| train/                          |               |\n|    approx_kl                    | 0.00051919126 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.54         |\n|    explained_variance           | 0.522         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 8.22          |\n|    n_updates                    | 420           |\n|    policy_gradient_loss         | 0.000174      |\n|    value_loss                   | 19.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 145           |\n|    ice_dug                      | 74            |\n|    water_produced               | 8.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.6          |\n| time/                           |               |\n|    fps                          | 808           |\n|    iterations                   | 212           |\n|    time_elapsed                 | 1049          |\n|    total_timesteps              | 848000        |\n| train/                          |               |\n|    approx_kl                    | 0.00045394036 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.65         |\n|    explained_variance           | 0.478         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 11.6          |\n|    n_updates                    | 422           |\n|    policy_gradient_loss         | 0.000123      |\n|    value_loss                   | 22.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 125           |\n|    action_queue_updates_total   | 137           |\n|    ice_dug                      | 117           |\n|    water_produced               | 9.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 14.7          |\n| time/                           |               |\n|    fps                          | 808           |\n|    iterations                   | 213           |\n|    time_elapsed                 | 1053          |\n|    total_timesteps              | 852000        |\n| train/                          |               |\n|    approx_kl                    | 0.00015867902 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.63         |\n|    explained_variance           | 0.435         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 10.1          |\n|    n_updates                    | 424           |\n|    policy_gradient_loss         | -0.000133     |\n|    value_loss                   | 24.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 132           |\n|    action_queue_updates_total   | 148           |\n|    ice_dug                      | 96            |\n|    water_produced               | 19.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 12.1         |\n| time/                           |              |\n|    fps                          | 808          |\n|    iterations                   | 214          |\n|    time_elapsed                 | 1058         |\n|    total_timesteps              | 856000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003449091 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.72        |\n|    explained_variance           | 0.364        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 27.4         |\n|    n_updates                    | 426          |\n|    policy_gradient_loss         | -0.000261    |\n|    value_loss                   | 47.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 122          |\n|    action_queue_updates_total   | 137          |\n|    ice_dug                      | 48           |\n|    water_produced               | 10.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.1          |\n| time/                           |               |\n|    fps                          | 809           |\n|    iterations                   | 215           |\n|    time_elapsed                 | 1063          |\n|    total_timesteps              | 860000        |\n| train/                          |               |\n|    approx_kl                    | 0.00028756118 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.61         |\n|    explained_variance           | 0.424         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 10.6          |\n|    n_updates                    | 428           |\n|    policy_gradient_loss         | -6.61e-05     |\n|    value_loss                   | 22.2          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 132           |\n|    action_queue_updates_total   | 143           |\n|    ice_dug                      | 179           |\n|    water_produced               | 21.5          |\n---------------------------------------------------\nEval num_timesteps=864000, episode_reward=92.56 +/- 51.51\nEpisode length: 389.00 +/- 49.05\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 389          |\n|    mean_reward                  | 92.6         |\n| time/                           |              |\n|    total_timesteps              | 864000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003211095 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.61        |\n|    explained_variance           | 0.336        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 23.3         |\n|    n_updates                    | 430          |\n|    policy_gradient_loss         | 6.88e-05     |\n|    value_loss                   | 50.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 156          |\n|    water_produced               | 27           |\n--------------------------------------------------\nNew best mean reward!\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 18.9     |\n| time/              |          |\n|    fps             | 806      |\n|    iterations      | 216      |\n|    time_elapsed    | 1070     |\n|    total_timesteps | 864000   |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.2         |\n| time/                           |              |\n|    fps                          | 807          |\n|    iterations                   | 217          |\n|    time_elapsed                 | 1075         |\n|    total_timesteps              | 868000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003713021 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.7         |\n|    explained_variance           | 0.35         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 37           |\n|    n_updates                    | 432          |\n|    policy_gradient_loss         | 6.79e-05     |\n|    value_loss                   | 79.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 139          |\n|    ice_dug                      | 59           |\n|    water_produced               | 11.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.7         |\n| time/                           |              |\n|    fps                          | 807          |\n|    iterations                   | 218          |\n|    time_elapsed                 | 1079         |\n|    total_timesteps              | 872000       |\n| train/                          |              |\n|    approx_kl                    | 0.0007994392 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.59        |\n|    explained_variance           | 0.432        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 12.1         |\n|    n_updates                    | 434          |\n|    policy_gradient_loss         | -0.000415    |\n|    value_loss                   | 25.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 130          |\n|    water_produced               | 22           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 19.7          |\n| time/                           |               |\n|    fps                          | 807           |\n|    iterations                   | 219           |\n|    time_elapsed                 | 1084          |\n|    total_timesteps              | 876000        |\n| train/                          |               |\n|    approx_kl                    | 4.7599304e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.395         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 23.5          |\n|    n_updates                    | 436           |\n|    policy_gradient_loss         | 8.82e-05      |\n|    value_loss                   | 71            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 123           |\n|    action_queue_updates_total   | 147           |\n|    ice_dug                      | 48            |\n|    water_produced               | 10.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 20           |\n| time/                           |              |\n|    fps                          | 808          |\n|    iterations                   | 220          |\n|    time_elapsed                 | 1089         |\n|    total_timesteps              | 880000       |\n| train/                          |              |\n|    approx_kl                    | 9.761705e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.85        |\n|    explained_variance           | 0.455        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 15.2         |\n|    n_updates                    | 438          |\n|    policy_gradient_loss         | 1.25e-07     |\n|    value_loss                   | 28.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 119          |\n|    action_queue_updates_total   | 131          |\n|    ice_dug                      | 132          |\n|    water_produced               | 23.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 16.3         |\n| time/                           |              |\n|    fps                          | 808          |\n|    iterations                   | 221          |\n|    time_elapsed                 | 1093         |\n|    total_timesteps              | 884000       |\n| train/                          |              |\n|    approx_kl                    | 0.0008234276 |\n|    clip_fraction                | 0.00075      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.57        |\n|    explained_variance           | 0.366        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 24.9         |\n|    n_updates                    | 440          |\n|    policy_gradient_loss         | -0.000351    |\n|    value_loss                   | 52.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 124          |\n|    action_queue_updates_total   | 137          |\n|    ice_dug                      | 56           |\n|    water_produced               | 9.5          |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 17.7          |\n| time/                           |               |\n|    fps                          | 808           |\n|    iterations                   | 222           |\n|    time_elapsed                 | 1097          |\n|    total_timesteps              | 888000        |\n| train/                          |               |\n|    approx_kl                    | 0.00032900774 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.63         |\n|    explained_variance           | 0.36          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 14.9          |\n|    n_updates                    | 442           |\n|    policy_gradient_loss         | -0.000486     |\n|    value_loss                   | 23.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 142           |\n|    ice_dug                      | 116           |\n|    water_produced               | 18.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.4          |\n| time/                           |               |\n|    fps                          | 808           |\n|    iterations                   | 223           |\n|    time_elapsed                 | 1102          |\n|    total_timesteps              | 892000        |\n| train/                          |               |\n|    approx_kl                    | 0.00010123962 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.65         |\n|    explained_variance           | 0.413         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 20.8          |\n|    n_updates                    | 444           |\n|    policy_gradient_loss         | -0.000133     |\n|    value_loss                   | 36.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 141           |\n|    ice_dug                      | 127           |\n|    water_produced               | 25.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 24.1          |\n| time/                           |               |\n|    fps                          | 809           |\n|    iterations                   | 224           |\n|    time_elapsed                 | 1107          |\n|    total_timesteps              | 896000        |\n| train/                          |               |\n|    approx_kl                    | 0.00043650725 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.66         |\n|    explained_variance           | 0.384         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 27.5          |\n|    n_updates                    | 446           |\n|    policy_gradient_loss         | 5.24e-05      |\n|    value_loss                   | 59.3          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 134           |\n|    action_queue_updates_total   | 141           |\n|    ice_dug                      | 221           |\n|    water_produced               | 37.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 28.7         |\n| time/                           |              |\n|    fps                          | 809          |\n|    iterations                   | 225          |\n|    time_elapsed                 | 1111         |\n|    total_timesteps              | 900000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003205806 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.54        |\n|    explained_variance           | 0.304        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 47.2         |\n|    n_updates                    | 448          |\n|    policy_gradient_loss         | -0.000135    |\n|    value_loss                   | 107          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 125          |\n|    action_queue_updates_total   | 139          |\n|    ice_dug                      | 226          |\n|    water_produced               | 45.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 32.9         |\n| time/                           |              |\n|    fps                          | 809          |\n|    iterations                   | 226          |\n|    time_elapsed                 | 1116         |\n|    total_timesteps              | 904000       |\n| train/                          |              |\n|    approx_kl                    | 0.0010600372 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.58        |\n|    explained_variance           | 0.397        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 46.3         |\n|    n_updates                    | 450          |\n|    policy_gradient_loss         | 4.57e-06     |\n|    value_loss                   | 97           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 127          |\n|    action_queue_updates_total   | 146          |\n|    ice_dug                      | 132          |\n|    water_produced               | 29.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 38.3          |\n| time/                           |               |\n|    fps                          | 810           |\n|    iterations                   | 227           |\n|    time_elapsed                 | 1120          |\n|    total_timesteps              | 908000        |\n| train/                          |               |\n|    approx_kl                    | 0.00043743983 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.7          |\n|    explained_variance           | 0.382         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 35.5          |\n|    n_updates                    | 452           |\n|    policy_gradient_loss         | 5.05e-05      |\n|    value_loss                   | 76.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 140           |\n|    ice_dug                      | 209           |\n|    water_produced               | 44            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 36.1          |\n| time/                           |               |\n|    fps                          | 810           |\n|    iterations                   | 228           |\n|    time_elapsed                 | 1125          |\n|    total_timesteps              | 912000        |\n| train/                          |               |\n|    approx_kl                    | 0.00015562904 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.56         |\n|    explained_variance           | 0.33          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 50.8          |\n|    n_updates                    | 454           |\n|    policy_gradient_loss         | -0.000223     |\n|    value_loss                   | 108           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 124           |\n|    action_queue_updates_total   | 127           |\n|    ice_dug                      | 90            |\n|    water_produced               | 14.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 33.2          |\n| time/                           |               |\n|    fps                          | 810           |\n|    iterations                   | 229           |\n|    time_elapsed                 | 1129          |\n|    total_timesteps              | 916000        |\n| train/                          |               |\n|    approx_kl                    | 6.7222936e-05 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.34         |\n|    explained_variance           | 0.315         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.5          |\n|    n_updates                    | 456           |\n|    policy_gradient_loss         | -0.000196     |\n|    value_loss                   | 33.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 112           |\n|    action_queue_updates_total   | 118           |\n|    ice_dug                      | 111           |\n|    water_produced               | 24.2          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 23.9          |\n| time/                           |               |\n|    fps                          | 811           |\n|    iterations                   | 230           |\n|    time_elapsed                 | 1134          |\n|    total_timesteps              | 920000        |\n| train/                          |               |\n|    approx_kl                    | 0.00039507946 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.38         |\n|    explained_variance           | 0.311         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 29.9          |\n|    n_updates                    | 458           |\n|    policy_gradient_loss         | -0.000379     |\n|    value_loss                   | 62            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 141           |\n|    ice_dug                      | 34            |\n|    water_produced               | 1             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 22.5         |\n| time/                           |              |\n|    fps                          | 811          |\n|    iterations                   | 231          |\n|    time_elapsed                 | 1138         |\n|    total_timesteps              | 924000       |\n| train/                          |              |\n|    approx_kl                    | 0.0003045352 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.59        |\n|    explained_variance           | 0.743        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 2.34         |\n|    n_updates                    | 460          |\n|    policy_gradient_loss         | -7.69e-05    |\n|    value_loss                   | 7.32         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 157          |\n|    ice_dug                      | 181          |\n|    water_produced               | 22.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 15.5         |\n| time/                           |              |\n|    fps                          | 811          |\n|    iterations                   | 232          |\n|    time_elapsed                 | 1143         |\n|    total_timesteps              | 928000       |\n| train/                          |              |\n|    approx_kl                    | 0.0005018158 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.87        |\n|    explained_variance           | 0.431        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 21           |\n|    n_updates                    | 462          |\n|    policy_gradient_loss         | -5.04e-05    |\n|    value_loss                   | 43.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 109          |\n|    water_produced               | 10           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.5         |\n| time/                           |              |\n|    fps                          | 812          |\n|    iterations                   | 233          |\n|    time_elapsed                 | 1147         |\n|    total_timesteps              | 932000       |\n| train/                          |              |\n|    approx_kl                    | 0.0007851451 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.7         |\n|    explained_variance           | 0.507        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 13.6         |\n|    n_updates                    | 464          |\n|    policy_gradient_loss         | 1.68e-05     |\n|    value_loss                   | 23.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 122          |\n|    action_queue_updates_total   | 137          |\n|    ice_dug                      | 149          |\n|    water_produced               | 29           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21.1         |\n| time/                           |              |\n|    fps                          | 812          |\n|    iterations                   | 234          |\n|    time_elapsed                 | 1152         |\n|    total_timesteps              | 936000       |\n| train/                          |              |\n|    approx_kl                    | 8.262141e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.64        |\n|    explained_variance           | 0.413        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 35.8         |\n|    n_updates                    | 466          |\n|    policy_gradient_loss         | -6.09e-05    |\n|    value_loss                   | 55.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 126          |\n|    action_queue_updates_total   | 142          |\n|    ice_dug                      | 169          |\n|    water_produced               | 36.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 24.4         |\n| time/                           |              |\n|    fps                          | 812          |\n|    iterations                   | 235          |\n|    time_elapsed                 | 1156         |\n|    total_timesteps              | 940000       |\n| train/                          |              |\n|    approx_kl                    | 0.0005505717 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.65        |\n|    explained_variance           | 0.395        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 44.4         |\n|    n_updates                    | 468          |\n|    policy_gradient_loss         | -0.000464    |\n|    value_loss                   | 90.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 158          |\n|    water_produced               | 16.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 23.8         |\n| time/                           |              |\n|    fps                          | 812          |\n|    iterations                   | 236          |\n|    time_elapsed                 | 1161         |\n|    total_timesteps              | 944000       |\n| train/                          |              |\n|    approx_kl                    | 0.0020641664 |\n|    clip_fraction                | 0.00663      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.61        |\n|    explained_variance           | 0.407        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 18.1         |\n|    n_updates                    | 470          |\n|    policy_gradient_loss         | 0.000386     |\n|    value_loss                   | 36.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 142          |\n|    ice_dug                      | 111          |\n|    water_produced               | 20           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 31.7          |\n| time/                           |               |\n|    fps                          | 812           |\n|    iterations                   | 237           |\n|    time_elapsed                 | 1166          |\n|    total_timesteps              | 948000        |\n| train/                          |               |\n|    approx_kl                    | 0.00040785372 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.6          |\n|    explained_variance           | 0.391         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.2          |\n|    n_updates                    | 472           |\n|    policy_gradient_loss         | -3.13e-05     |\n|    value_loss                   | 41.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 255           |\n|    water_produced               | 48            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 36.1         |\n| time/                           |              |\n|    fps                          | 812          |\n|    iterations                   | 238          |\n|    time_elapsed                 | 1171         |\n|    total_timesteps              | 952000       |\n| train/                          |              |\n|    approx_kl                    | 0.0017728902 |\n|    clip_fraction                | 0.00425      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.61        |\n|    explained_variance           | 0.393        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 46.6         |\n|    n_updates                    | 474          |\n|    policy_gradient_loss         | 2.96e-05     |\n|    value_loss                   | 98.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 220          |\n|    water_produced               | 50           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 29.2         |\n| time/                           |              |\n|    fps                          | 813          |\n|    iterations                   | 239          |\n|    time_elapsed                 | 1175         |\n|    total_timesteps              | 956000       |\n| train/                          |              |\n|    approx_kl                    | 0.0019152559 |\n|    clip_fraction                | 0.000625     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.59        |\n|    explained_variance           | 0.367        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 53.5         |\n|    n_updates                    | 476          |\n|    policy_gradient_loss         | -0.000712    |\n|    value_loss                   | 122          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 122          |\n|    action_queue_updates_total   | 139          |\n|    ice_dug                      | 18           |\n|    water_produced               | 3.75         |\n--------------------------------------------------\nEval num_timesteps=960000, episode_reward=37.60 +/- 75.00\nEpisode length: 337.00 +/- 72.00\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 337          |\n|    mean_reward                  | 37.6         |\n| time/                           |              |\n|    total_timesteps              | 960000       |\n| train/                          |              |\n|    approx_kl                    | 0.0005348517 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.62        |\n|    explained_variance           | 0.668        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 4.98         |\n|    n_updates                    | 478          |\n|    policy_gradient_loss         | 0.000477     |\n|    value_loss                   | 10.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 309          |\n|    water_produced               | 56.5         |\n--------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 37.5     |\n| time/              |          |\n|    fps             | 811      |\n|    iterations      | 240      |\n|    time_elapsed    | 1183     |\n|    total_timesteps | 960000   |\n---------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 40.6        |\n| time/                           |             |\n|    fps                          | 811         |\n|    iterations                   | 241         |\n|    time_elapsed                 | 1188        |\n|    total_timesteps              | 964000      |\n| train/                          |             |\n|    approx_kl                    | 3.21278e-05 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.53       |\n|    explained_variance           | 0.356       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 68.6        |\n|    n_updates                    | 480         |\n|    policy_gradient_loss         | -5.95e-05   |\n|    value_loss                   | 138         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 126         |\n|    action_queue_updates_total   | 134         |\n|    ice_dug                      | 180         |\n|    water_produced               | 35          |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 32.1          |\n| time/                           |               |\n|    fps                          | 811           |\n|    iterations                   | 242           |\n|    time_elapsed                 | 1192          |\n|    total_timesteps              | 968000        |\n| train/                          |               |\n|    approx_kl                    | 0.00023631603 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.45         |\n|    explained_variance           | 0.362         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 41.5          |\n|    n_updates                    | 482           |\n|    policy_gradient_loss         | 3.24e-05      |\n|    value_loss                   | 91.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 125           |\n|    action_queue_updates_total   | 140           |\n|    ice_dug                      | 85            |\n|    water_produced               | 7.25          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 23           |\n| time/                           |              |\n|    fps                          | 811          |\n|    iterations                   | 243          |\n|    time_elapsed                 | 1197         |\n|    total_timesteps              | 972000       |\n| train/                          |              |\n|    approx_kl                    | 0.0011677453 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.64        |\n|    explained_variance           | 0.6          |\n|    learning_rate                | 0.0003       |\n|    loss                         | 8.09         |\n|    n_updates                    | 484          |\n|    policy_gradient_loss         | 0.000343     |\n|    value_loss                   | 18.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 141          |\n|    ice_dug                      | 44           |\n|    water_produced               | 6            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 28.7         |\n| time/                           |              |\n|    fps                          | 811          |\n|    iterations                   | 244          |\n|    time_elapsed                 | 1202         |\n|    total_timesteps              | 976000       |\n| train/                          |              |\n|    approx_kl                    | 0.0010476264 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.64        |\n|    explained_variance           | 0.626        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 5.5          |\n|    n_updates                    | 486          |\n|    policy_gradient_loss         | 0.000564     |\n|    value_loss                   | 11.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 140          |\n|    ice_dug                      | 154          |\n|    water_produced               | 31.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.7          |\n| time/                           |               |\n|    fps                          | 812           |\n|    iterations                   | 245           |\n|    time_elapsed                 | 1206          |\n|    total_timesteps              | 980000        |\n| train/                          |               |\n|    approx_kl                    | 0.00020335914 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.6          |\n|    explained_variance           | 0.431         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 35.5          |\n|    n_updates                    | 488           |\n|    policy_gradient_loss         | -0.000159     |\n|    value_loss                   | 61.2          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 153           |\n|    ice_dug                      | 78            |\n|    water_produced               | 18.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.7         |\n| time/                           |              |\n|    fps                          | 812          |\n|    iterations                   | 246          |\n|    time_elapsed                 | 1211         |\n|    total_timesteps              | 984000       |\n| train/                          |              |\n|    approx_kl                    | 0.0029804884 |\n|    clip_fraction                | 0.013        |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.79        |\n|    explained_variance           | 0.443        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 23.8         |\n|    n_updates                    | 490          |\n|    policy_gradient_loss         | -0.000349    |\n|    value_loss                   | 56.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 129          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 150          |\n|    water_produced               | 30.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 20.4          |\n| time/                           |               |\n|    fps                          | 812           |\n|    iterations                   | 247           |\n|    time_elapsed                 | 1215          |\n|    total_timesteps              | 988000        |\n| train/                          |               |\n|    approx_kl                    | 0.00021298481 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.79         |\n|    explained_variance           | 0.441         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 48.7          |\n|    n_updates                    | 492           |\n|    policy_gradient_loss         | -0.000215     |\n|    value_loss                   | 88            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 129           |\n|    action_queue_updates_total   | 147           |\n|    ice_dug                      | 60            |\n|    water_produced               | 10.8          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 23.9         |\n| time/                           |              |\n|    fps                          | 812          |\n|    iterations                   | 248          |\n|    time_elapsed                 | 1220         |\n|    total_timesteps              | 992000       |\n| train/                          |              |\n|    approx_kl                    | 0.0011334476 |\n|    clip_fraction                | 0.00025      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.76        |\n|    explained_variance           | 0.543        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 11           |\n|    n_updates                    | 494          |\n|    policy_gradient_loss         | 0.000349     |\n|    value_loss                   | 22.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 148          |\n|    ice_dug                      | 113          |\n|    water_produced               | 23           |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 20.1        |\n| time/                           |             |\n|    fps                          | 813         |\n|    iterations                   | 249         |\n|    time_elapsed                 | 1224        |\n|    total_timesteps              | 996000      |\n| train/                          |             |\n|    approx_kl                    | 0.000395175 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.76       |\n|    explained_variance           | 0.536       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 23          |\n|    n_updates                    | 496         |\n|    policy_gradient_loss         | -0.000337   |\n|    value_loss                   | 51.6        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 123         |\n|    action_queue_updates_total   | 136         |\n|    ice_dug                      | 72          |\n|    water_produced               | 13          |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 17.6         |\n| time/                           |              |\n|    fps                          | 813          |\n|    iterations                   | 250          |\n|    time_elapsed                 | 1229         |\n|    total_timesteps              | 1000000      |\n| train/                          |              |\n|    approx_kl                    | 0.0004987839 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.58        |\n|    explained_variance           | 0.515        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 13.6         |\n|    n_updates                    | 498          |\n|    policy_gradient_loss         | -1.82e-05    |\n|    value_loss                   | 28.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 160          |\n|    ice_dug                      | 61           |\n|    water_produced               | 6.25         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 15.5         |\n| time/                           |              |\n|    fps                          | 813          |\n|    iterations                   | 251          |\n|    time_elapsed                 | 1234         |\n|    total_timesteps              | 1004000      |\n| train/                          |              |\n|    approx_kl                    | 0.0014642596 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2           |\n|    explained_variance           | 0.636        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 10.1         |\n|    n_updates                    | 500          |\n|    policy_gradient_loss         | 0.000265     |\n|    value_loss                   | 17.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 158          |\n|    ice_dug                      | 98           |\n|    water_produced               | 20.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 19.8         |\n| time/                           |              |\n|    fps                          | 813          |\n|    iterations                   | 252          |\n|    time_elapsed                 | 1238         |\n|    total_timesteps              | 1008000      |\n| train/                          |              |\n|    approx_kl                    | 0.0010990673 |\n|    clip_fraction                | 0.0015       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.92        |\n|    explained_variance           | 0.509        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 30.2         |\n|    n_updates                    | 502          |\n|    policy_gradient_loss         | -0.000299    |\n|    value_loss                   | 47.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 171          |\n|    water_produced               | 31.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 23.9          |\n| time/                           |               |\n|    fps                          | 814           |\n|    iterations                   | 253           |\n|    time_elapsed                 | 1243          |\n|    total_timesteps              | 1012000       |\n| train/                          |               |\n|    approx_kl                    | 0.00031601347 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.73         |\n|    explained_variance           | 0.458         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 32.6          |\n|    n_updates                    | 504           |\n|    policy_gradient_loss         | 0.000166      |\n|    value_loss                   | 72.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 198           |\n|    water_produced               | 42.8          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 24.9         |\n| time/                           |              |\n|    fps                          | 814          |\n|    iterations                   | 254          |\n|    time_elapsed                 | 1247         |\n|    total_timesteps              | 1016000      |\n| train/                          |              |\n|    approx_kl                    | 0.0003452353 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.7         |\n|    explained_variance           | 0.436        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 48.7         |\n|    n_updates                    | 506          |\n|    policy_gradient_loss         | -9.78e-05    |\n|    value_loss                   | 107          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 79           |\n|    water_produced               | 18           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 25.9         |\n| time/                           |              |\n|    fps                          | 814          |\n|    iterations                   | 255          |\n|    time_elapsed                 | 1252         |\n|    total_timesteps              | 1020000      |\n| train/                          |              |\n|    approx_kl                    | 0.0005792126 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.84        |\n|    explained_variance           | 0.505        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 21           |\n|    n_updates                    | 508          |\n|    policy_gradient_loss         | 0.000189     |\n|    value_loss                   | 50.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 142          |\n|    ice_dug                      | 55           |\n|    water_produced               | 11.3         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 26.5         |\n| time/                           |              |\n|    fps                          | 814          |\n|    iterations                   | 256          |\n|    time_elapsed                 | 1256         |\n|    total_timesteps              | 1024000      |\n| train/                          |              |\n|    approx_kl                    | 0.0009578364 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.67        |\n|    explained_variance           | 0.54         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.6         |\n|    n_updates                    | 510          |\n|    policy_gradient_loss         | -0.00029     |\n|    value_loss                   | 30.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 153          |\n|    ice_dug                      | 148          |\n|    water_produced               | 22.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 27.2         |\n| time/                           |              |\n|    fps                          | 814          |\n|    iterations                   | 257          |\n|    time_elapsed                 | 1261         |\n|    total_timesteps              | 1028000      |\n| train/                          |              |\n|    approx_kl                    | 0.0005047376 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.79        |\n|    explained_variance           | 0.472        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 23.2         |\n|    n_updates                    | 512          |\n|    policy_gradient_loss         | -1.06e-05    |\n|    value_loss                   | 66.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 137          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 210          |\n|    water_produced               | 34.5         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 24.9        |\n| time/                           |             |\n|    fps                          | 815         |\n|    iterations                   | 258         |\n|    time_elapsed                 | 1266        |\n|    total_timesteps              | 1032000     |\n| train/                          |             |\n|    approx_kl                    | 0.000624693 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.76       |\n|    explained_variance           | 0.459       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 30.3        |\n|    n_updates                    | 514         |\n|    policy_gradient_loss         | -4.47e-05   |\n|    value_loss                   | 90.4        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 132         |\n|    action_queue_updates_total   | 142         |\n|    ice_dug                      | 199         |\n|    water_produced               | 31.2        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 22.5         |\n| time/                           |              |\n|    fps                          | 815          |\n|    iterations                   | 259          |\n|    time_elapsed                 | 1270         |\n|    total_timesteps              | 1036000      |\n| train/                          |              |\n|    approx_kl                    | 0.0010614784 |\n|    clip_fraction                | 0.00125      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.63        |\n|    explained_variance           | 0.463        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 39.1         |\n|    n_updates                    | 516          |\n|    policy_gradient_loss         | 0.000329     |\n|    value_loss                   | 72.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 34           |\n|    water_produced               | 6.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 25.7         |\n| time/                           |              |\n|    fps                          | 815          |\n|    iterations                   | 260          |\n|    time_elapsed                 | 1275         |\n|    total_timesteps              | 1040000      |\n| train/                          |              |\n|    approx_kl                    | 0.0020411522 |\n|    clip_fraction                | 0.00288      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.77        |\n|    explained_variance           | 0.592        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 9.72         |\n|    n_updates                    | 518          |\n|    policy_gradient_loss         | 0.000491     |\n|    value_loss                   | 21.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 126          |\n|    water_produced               | 26.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 27.3          |\n| time/                           |               |\n|    fps                          | 815           |\n|    iterations                   | 261           |\n|    time_elapsed                 | 1279          |\n|    total_timesteps              | 1044000       |\n| train/                          |               |\n|    approx_kl                    | 0.00035780176 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.81         |\n|    explained_variance           | 0.577         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 26.8          |\n|    n_updates                    | 520           |\n|    policy_gradient_loss         | -0.000354     |\n|    value_loss                   | 56.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 146           |\n|    ice_dug                      | 132           |\n|    water_produced               | 30.5          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 22.8          |\n| time/                           |               |\n|    fps                          | 816           |\n|    iterations                   | 262           |\n|    time_elapsed                 | 1284          |\n|    total_timesteps              | 1048000       |\n| train/                          |               |\n|    approx_kl                    | 0.00031716534 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.8          |\n|    explained_variance           | 0.505         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 51.5          |\n|    n_updates                    | 522           |\n|    policy_gradient_loss         | 5.17e-05      |\n|    value_loss                   | 90.3          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 150           |\n|    ice_dug                      | 71            |\n|    water_produced               | 13.8          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 22.5         |\n| time/                           |              |\n|    fps                          | 816          |\n|    iterations                   | 263          |\n|    time_elapsed                 | 1288         |\n|    total_timesteps              | 1052000      |\n| train/                          |              |\n|    approx_kl                    | 7.243102e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.81        |\n|    explained_variance           | 0.559        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 20           |\n|    n_updates                    | 524          |\n|    policy_gradient_loss         | 3e-05        |\n|    value_loss                   | 39.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 158          |\n|    ice_dug                      | 142          |\n|    water_produced               | 30.2         |\n--------------------------------------------------\nEval num_timesteps=1056000, episode_reward=26.40 +/- 52.80\nEpisode length: 326.00 +/- 50.00\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 326           |\n|    mean_reward                  | 26.4          |\n| time/                           |               |\n|    total_timesteps              | 1056000       |\n| train/                          |               |\n|    approx_kl                    | 0.00055517023 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.89         |\n|    explained_variance           | 0.57          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 31.5          |\n|    n_updates                    | 526           |\n|    policy_gradient_loss         | -0.00036      |\n|    value_loss                   | 70.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 130           |\n|    action_queue_updates_total   | 142           |\n|    ice_dug                      | 136           |\n|    water_produced               | 29.8          |\n---------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 27.4     |\n| time/              |          |\n|    fps             | 814      |\n|    iterations      | 264      |\n|    time_elapsed    | 1296     |\n|    total_timesteps | 1056000  |\n---------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 26.8          |\n| time/                           |               |\n|    fps                          | 814           |\n|    iterations                   | 265           |\n|    time_elapsed                 | 1301          |\n|    total_timesteps              | 1060000       |\n| train/                          |               |\n|    approx_kl                    | 0.00027754993 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.64         |\n|    explained_variance           | 0.526         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 34            |\n|    n_updates                    | 528           |\n|    policy_gradient_loss         | -7.44e-05     |\n|    value_loss                   | 70.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 129           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 120           |\n|    water_produced               | 23.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 22.1         |\n| time/                           |              |\n|    fps                          | 814          |\n|    iterations                   | 266          |\n|    time_elapsed                 | 1305         |\n|    total_timesteps              | 1064000      |\n| train/                          |              |\n|    approx_kl                    | 0.0002875963 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.93        |\n|    explained_variance           | 0.551        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 27.7         |\n|    n_updates                    | 530          |\n|    policy_gradient_loss         | -6.99e-05    |\n|    value_loss                   | 55.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 124          |\n|    action_queue_updates_total   | 143          |\n|    ice_dug                      | 54           |\n|    water_produced               | 8.25         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 21.1         |\n| time/                           |              |\n|    fps                          | 815          |\n|    iterations                   | 267          |\n|    time_elapsed                 | 1310         |\n|    total_timesteps              | 1068000      |\n| train/                          |              |\n|    approx_kl                    | 0.0004328352 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.75        |\n|    explained_variance           | 0.701        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 12.6         |\n|    n_updates                    | 532          |\n|    policy_gradient_loss         | -0.000385    |\n|    value_loss                   | 19.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 150          |\n|    ice_dug                      | 71           |\n|    water_produced               | 8.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 20.3         |\n| time/                           |              |\n|    fps                          | 815          |\n|    iterations                   | 268          |\n|    time_elapsed                 | 1314         |\n|    total_timesteps              | 1072000      |\n| train/                          |              |\n|    approx_kl                    | 0.0006023676 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.87        |\n|    explained_variance           | 0.629        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 22.1         |\n|    n_updates                    | 534          |\n|    policy_gradient_loss         | -0.000301    |\n|    value_loss                   | 27.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 144          |\n|    action_queue_updates_total   | 157          |\n|    ice_dug                      | 144          |\n|    water_produced               | 26.3         |\n--------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 200        |\n|    ep_rew_mean                  | 18.2       |\n| time/                           |            |\n|    fps                          | 815        |\n|    iterations                   | 269        |\n|    time_elapsed                 | 1319       |\n|    total_timesteps              | 1076000    |\n| train/                          |            |\n|    approx_kl                    | 0.00113333 |\n|    clip_fraction                | 0.000625   |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -1.81      |\n|    explained_variance           | 0.532      |\n|    learning_rate                | 0.0003     |\n|    loss                         | 26.5       |\n|    n_updates                    | 536        |\n|    policy_gradient_loss         | -0.000391  |\n|    value_loss                   | 51.8       |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 143        |\n|    action_queue_updates_total   | 159        |\n|    ice_dug                      | 121        |\n|    water_produced               | 19.5       |\n------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 16.5         |\n| time/                           |              |\n|    fps                          | 815          |\n|    iterations                   | 270          |\n|    time_elapsed                 | 1324         |\n|    total_timesteps              | 1080000      |\n| train/                          |              |\n|    approx_kl                    | 0.0020851283 |\n|    clip_fraction                | 0.0035       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.9         |\n|    explained_variance           | 0.473        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 21.1         |\n|    n_updates                    | 538          |\n|    policy_gradient_loss         | 0.000479     |\n|    value_loss                   | 49.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 68           |\n|    water_produced               | 15.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.7          |\n| time/                           |               |\n|    fps                          | 815           |\n|    iterations                   | 271           |\n|    time_elapsed                 | 1328          |\n|    total_timesteps              | 1084000       |\n| train/                          |               |\n|    approx_kl                    | 0.00051485084 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.92         |\n|    explained_variance           | 0.591         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 10.2          |\n|    n_updates                    | 540           |\n|    policy_gradient_loss         | 0.000279      |\n|    value_loss                   | 32.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 161           |\n|    ice_dug                      | 49            |\n|    water_produced               | 9.25          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.8         |\n| time/                           |              |\n|    fps                          | 815          |\n|    iterations                   | 272          |\n|    time_elapsed                 | 1333         |\n|    total_timesteps              | 1088000      |\n| train/                          |              |\n|    approx_kl                    | 0.0016538281 |\n|    clip_fraction                | 0.00025      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.01        |\n|    explained_variance           | 0.593        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 9.26         |\n|    n_updates                    | 542          |\n|    policy_gradient_loss         | 0.000684     |\n|    value_loss                   | 21.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 138          |\n|    action_queue_updates_total   | 161          |\n|    ice_dug                      | 103          |\n|    water_produced               | 18.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.2          |\n| time/                           |               |\n|    fps                          | 816           |\n|    iterations                   | 273           |\n|    time_elapsed                 | 1337          |\n|    total_timesteps              | 1092000       |\n| train/                          |               |\n|    approx_kl                    | 0.00017962238 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.02         |\n|    explained_variance           | 0.566         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 23.8          |\n|    n_updates                    | 544           |\n|    policy_gradient_loss         | 0.000107      |\n|    value_loss                   | 47.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 154           |\n|    ice_dug                      | 131           |\n|    water_produced               | 23.8          |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 16.4        |\n| time/                           |             |\n|    fps                          | 816         |\n|    iterations                   | 274         |\n|    time_elapsed                 | 1342        |\n|    total_timesteps              | 1096000     |\n| train/                          |             |\n|    approx_kl                    | 0.000459477 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.85       |\n|    explained_variance           | 0.541       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 22.6        |\n|    n_updates                    | 546         |\n|    policy_gradient_loss         | 0.00017     |\n|    value_loss                   | 48.3        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 134         |\n|    action_queue_updates_total   | 153         |\n|    ice_dug                      | 51          |\n|    water_produced               | 10.8        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 16.5         |\n| time/                           |              |\n|    fps                          | 816          |\n|    iterations                   | 275          |\n|    time_elapsed                 | 1346         |\n|    total_timesteps              | 1100000      |\n| train/                          |              |\n|    approx_kl                    | 0.0014787544 |\n|    clip_fraction                | 0.000625     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.86        |\n|    explained_variance           | 0.71         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 7.43         |\n|    n_updates                    | 548          |\n|    policy_gradient_loss         | 0.000458     |\n|    value_loss                   | 15.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 129          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 85           |\n|    water_produced               | 16.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.3          |\n| time/                           |               |\n|    fps                          | 816           |\n|    iterations                   | 276           |\n|    time_elapsed                 | 1351          |\n|    total_timesteps              | 1104000       |\n| train/                          |               |\n|    approx_kl                    | 0.00017726843 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.96         |\n|    explained_variance           | 0.631         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.3          |\n|    n_updates                    | 550           |\n|    policy_gradient_loss         | 0.000169      |\n|    value_loss                   | 33.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 159           |\n|    ice_dug                      | 53            |\n|    water_produced               | 8             |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 14.7        |\n| time/                           |             |\n|    fps                          | 816         |\n|    iterations                   | 277         |\n|    time_elapsed                 | 1356        |\n|    total_timesteps              | 1108000     |\n| train/                          |             |\n|    approx_kl                    | 0.002192241 |\n|    clip_fraction                | 0.000875    |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.99       |\n|    explained_variance           | 0.706       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 12          |\n|    n_updates                    | 552         |\n|    policy_gradient_loss         | 0.000555    |\n|    value_loss                   | 18.7        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 134         |\n|    action_queue_updates_total   | 165         |\n|    ice_dug                      | 77          |\n|    water_produced               | 10.7        |\n-------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.4          |\n| time/                           |               |\n|    fps                          | 817           |\n|    iterations                   | 278           |\n|    time_elapsed                 | 1360          |\n|    total_timesteps              | 1112000       |\n| train/                          |               |\n|    approx_kl                    | 0.00027118268 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.15         |\n|    explained_variance           | 0.655         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 9.21          |\n|    n_updates                    | 554           |\n|    policy_gradient_loss         | 0.000142      |\n|    value_loss                   | 21.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 134           |\n|    action_queue_updates_total   | 156           |\n|    ice_dug                      | 125           |\n|    water_produced               | 27.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 15.2         |\n| time/                           |              |\n|    fps                          | 817          |\n|    iterations                   | 279          |\n|    time_elapsed                 | 1365         |\n|    total_timesteps              | 1116000      |\n| train/                          |              |\n|    approx_kl                    | 0.0005089751 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.96        |\n|    explained_variance           | 0.581        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 28.1         |\n|    n_updates                    | 556          |\n|    policy_gradient_loss         | 0.000359     |\n|    value_loss                   | 57.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 160          |\n|    ice_dug                      | 52           |\n|    water_produced               | 9.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 14.2         |\n| time/                           |              |\n|    fps                          | 817          |\n|    iterations                   | 280          |\n|    time_elapsed                 | 1369         |\n|    total_timesteps              | 1120000      |\n| train/                          |              |\n|    approx_kl                    | 0.0006080299 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.05        |\n|    explained_variance           | 0.68         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.4         |\n|    n_updates                    | 558          |\n|    policy_gradient_loss         | 0.00016      |\n|    value_loss                   | 26.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 128          |\n|    action_queue_updates_total   | 157          |\n|    ice_dug                      | 62           |\n|    water_produced               | 11.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 15.7          |\n| time/                           |               |\n|    fps                          | 817           |\n|    iterations                   | 281           |\n|    time_elapsed                 | 1374          |\n|    total_timesteps              | 1124000       |\n| train/                          |               |\n|    approx_kl                    | 0.00016689653 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.06         |\n|    explained_variance           | 0.726         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 10.3          |\n|    n_updates                    | 560           |\n|    policy_gradient_loss         | -6.36e-05     |\n|    value_loss                   | 20.4          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 131           |\n|    action_queue_updates_total   | 159           |\n|    ice_dug                      | 80            |\n|    water_produced               | 15.2          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 14.2         |\n| time/                           |              |\n|    fps                          | 818          |\n|    iterations                   | 282          |\n|    time_elapsed                 | 1378         |\n|    total_timesteps              | 1128000      |\n| train/                          |              |\n|    approx_kl                    | 0.0007549894 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.02        |\n|    explained_variance           | 0.705        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 15           |\n|    n_updates                    | 562          |\n|    policy_gradient_loss         | -0.000409    |\n|    value_loss                   | 27           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 124          |\n|    action_queue_updates_total   | 164          |\n|    ice_dug                      | 17           |\n|    water_produced               | 3.75         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 10.9        |\n| time/                           |             |\n|    fps                          | 818         |\n|    iterations                   | 283         |\n|    time_elapsed                 | 1383        |\n|    total_timesteps              | 1132000     |\n| train/                          |             |\n|    approx_kl                    | 0.000624061 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2.17       |\n|    explained_variance           | 0.792       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 4.12        |\n|    n_updates                    | 564         |\n|    policy_gradient_loss         | -0.000774   |\n|    value_loss                   | 8.38        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 130         |\n|    action_queue_updates_total   | 164         |\n|    ice_dug                      | 60          |\n|    water_produced               | 12          |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 10.5         |\n| time/                           |              |\n|    fps                          | 818          |\n|    iterations                   | 284          |\n|    time_elapsed                 | 1388         |\n|    total_timesteps              | 1136000      |\n| train/                          |              |\n|    approx_kl                    | 0.0003749789 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.16        |\n|    explained_variance           | 0.693        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 14.8         |\n|    n_updates                    | 566          |\n|    policy_gradient_loss         | -0.000243    |\n|    value_loss                   | 31.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 137          |\n|    action_queue_updates_total   | 166          |\n|    ice_dug                      | 42           |\n|    water_produced               | 7.5          |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 9.92         |\n| time/                           |              |\n|    fps                          | 818          |\n|    iterations                   | 285          |\n|    time_elapsed                 | 1393         |\n|    total_timesteps              | 1140000      |\n| train/                          |              |\n|    approx_kl                    | 0.0017598458 |\n|    clip_fraction                | 0.000125     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.16        |\n|    explained_variance           | 0.721        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 6.79         |\n|    n_updates                    | 568          |\n|    policy_gradient_loss         | 0.000238     |\n|    value_loss                   | 15.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 161          |\n|    ice_dug                      | 59           |\n|    water_produced               | 8.5          |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.71          |\n| time/                           |               |\n|    fps                          | 818           |\n|    iterations                   | 286           |\n|    time_elapsed                 | 1397          |\n|    total_timesteps              | 1144000       |\n| train/                          |               |\n|    approx_kl                    | 0.00020297637 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.1          |\n|    explained_variance           | 0.754         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.99          |\n|    n_updates                    | 570           |\n|    policy_gradient_loss         | 0.000163      |\n|    value_loss                   | 15.3          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 135           |\n|    action_queue_updates_total   | 169           |\n|    ice_dug                      | 2             |\n|    water_produced               | 0             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 7.02         |\n| time/                           |              |\n|    fps                          | 818          |\n|    iterations                   | 287          |\n|    time_elapsed                 | 1402         |\n|    total_timesteps              | 1148000      |\n| train/                          |              |\n|    approx_kl                    | 0.0020938807 |\n|    clip_fraction                | 0.00413      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.2         |\n|    explained_variance           | 0.822        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 1.5          |\n|    n_updates                    | 572          |\n|    policy_gradient_loss         | 0.000942     |\n|    value_loss                   | 3.31         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 142          |\n|    action_queue_updates_total   | 170          |\n|    ice_dug                      | 46           |\n|    water_produced               | 5            |\n--------------------------------------------------\nEval num_timesteps=1152000, episode_reward=1.08 +/- 2.16\nEpisode length: 302.00 +/- 2.00\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 302          |\n|    mean_reward                  | 1.08         |\n| time/                           |              |\n|    total_timesteps              | 1152000      |\n| train/                          |              |\n|    approx_kl                    | 0.0008510974 |\n|    clip_fraction                | 0.000875     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.22        |\n|    explained_variance           | 0.682        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 6.03         |\n|    n_updates                    | 574          |\n|    policy_gradient_loss         | 0.000453     |\n|    value_loss                   | 12           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 171          |\n|    ice_dug                      | 77           |\n|    water_produced               | 12.2         |\n--------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 7.1      |\n| time/              |          |\n|    fps             | 817      |\n|    iterations      | 288      |\n|    time_elapsed    | 1410     |\n|    total_timesteps | 1152000  |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 8.72         |\n| time/                           |              |\n|    fps                          | 817          |\n|    iterations                   | 289          |\n|    time_elapsed                 | 1414         |\n|    total_timesteps              | 1156000      |\n| train/                          |              |\n|    approx_kl                    | 0.0005210367 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.22        |\n|    explained_variance           | 0.715        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 12.5         |\n|    n_updates                    | 576          |\n|    policy_gradient_loss         | 2.57e-05     |\n|    value_loss                   | 21           |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 169          |\n|    ice_dug                      | 77           |\n|    water_produced               | 15.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 8.8           |\n| time/                           |               |\n|    fps                          | 817           |\n|    iterations                   | 290           |\n|    time_elapsed                 | 1419          |\n|    total_timesteps              | 1160000       |\n| train/                          |               |\n|    approx_kl                    | 0.00044472553 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.22         |\n|    explained_variance           | 0.69          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 13.5          |\n|    n_updates                    | 578           |\n|    policy_gradient_loss         | 0.000183      |\n|    value_loss                   | 25.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 158           |\n|    ice_dug                      | 49            |\n|    water_produced               | 9             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.5          |\n| time/                           |               |\n|    fps                          | 817           |\n|    iterations                   | 291           |\n|    time_elapsed                 | 1424          |\n|    total_timesteps              | 1164000       |\n| train/                          |               |\n|    approx_kl                    | 0.00062468735 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.01         |\n|    explained_variance           | 0.74          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 9.14          |\n|    n_updates                    | 580           |\n|    policy_gradient_loss         | 0.000129      |\n|    value_loss                   | 16.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 159           |\n|    ice_dug                      | 75            |\n|    water_produced               | 12.8          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11.4          |\n| time/                           |               |\n|    fps                          | 817           |\n|    iterations                   | 292           |\n|    time_elapsed                 | 1429          |\n|    total_timesteps              | 1168000       |\n| train/                          |               |\n|    approx_kl                    | 0.00031951623 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.02         |\n|    explained_variance           | 0.632         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 14.1          |\n|    n_updates                    | 582           |\n|    policy_gradient_loss         | -0.000335     |\n|    value_loss                   | 23.7          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 125           |\n|    action_queue_updates_total   | 170           |\n|    ice_dug                      | 20            |\n|    water_produced               | 4.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 11            |\n| time/                           |               |\n|    fps                          | 817           |\n|    iterations                   | 293           |\n|    time_elapsed                 | 1433          |\n|    total_timesteps              | 1172000       |\n| train/                          |               |\n|    approx_kl                    | 0.00011024615 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.25         |\n|    explained_variance           | 0.78          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.8           |\n|    n_updates                    | 584           |\n|    policy_gradient_loss         | -0.000296     |\n|    value_loss                   | 8.43          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 138           |\n|    action_queue_updates_total   | 172           |\n|    ice_dug                      | 43            |\n|    water_produced               | 10.5          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.2          |\n| time/                           |               |\n|    fps                          | 817           |\n|    iterations                   | 294           |\n|    time_elapsed                 | 1438          |\n|    total_timesteps              | 1176000       |\n| train/                          |               |\n|    approx_kl                    | 0.00043498678 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.22         |\n|    explained_variance           | 0.67          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 7.84          |\n|    n_updates                    | 586           |\n|    policy_gradient_loss         | 0.000312      |\n|    value_loss                   | 17.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 168           |\n|    ice_dug                      | 159           |\n|    water_produced               | 25.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 15.9         |\n| time/                           |              |\n|    fps                          | 817          |\n|    iterations                   | 295          |\n|    time_elapsed                 | 1443         |\n|    total_timesteps              | 1180000      |\n| train/                          |              |\n|    approx_kl                    | 0.0010667145 |\n|    clip_fraction                | 0.000375     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.08        |\n|    explained_variance           | 0.683        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 25.5         |\n|    n_updates                    | 588          |\n|    policy_gradient_loss         | 0.000221     |\n|    value_loss                   | 55.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 166          |\n|    ice_dug                      | 103          |\n|    water_produced               | 22.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 15.6         |\n| time/                           |              |\n|    fps                          | 817          |\n|    iterations                   | 296          |\n|    time_elapsed                 | 1447         |\n|    total_timesteps              | 1184000      |\n| train/                          |              |\n|    approx_kl                    | 0.0008863056 |\n|    clip_fraction                | 0.00075      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.12        |\n|    explained_variance           | 0.693        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 15.3         |\n|    n_updates                    | 590          |\n|    policy_gradient_loss         | -9.44e-05    |\n|    value_loss                   | 41.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 129          |\n|    action_queue_updates_total   | 161          |\n|    ice_dug                      | 79           |\n|    water_produced               | 11           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 19.3          |\n| time/                           |               |\n|    fps                          | 818           |\n|    iterations                   | 297           |\n|    time_elapsed                 | 1452          |\n|    total_timesteps              | 1188000       |\n| train/                          |               |\n|    approx_kl                    | 0.00032515073 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.01         |\n|    explained_variance           | 0.795         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 10.3          |\n|    n_updates                    | 592           |\n|    policy_gradient_loss         | -0.000181     |\n|    value_loss                   | 20.2          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 171           |\n|    ice_dug                      | 104           |\n|    water_produced               | 22.2          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.7         |\n| time/                           |              |\n|    fps                          | 818          |\n|    iterations                   | 298          |\n|    time_elapsed                 | 1456         |\n|    total_timesteps              | 1192000      |\n| train/                          |              |\n|    approx_kl                    | 0.0004766675 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.19        |\n|    explained_variance           | 0.684        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 29.4         |\n|    n_updates                    | 594          |\n|    policy_gradient_loss         | 9.84e-05     |\n|    value_loss                   | 59.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 115          |\n|    action_queue_updates_total   | 155          |\n|    ice_dug                      | 34           |\n|    water_produced               | 7.5          |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 13.3          |\n| time/                           |               |\n|    fps                          | 818           |\n|    iterations                   | 299           |\n|    time_elapsed                 | 1461          |\n|    total_timesteps              | 1196000       |\n| train/                          |               |\n|    approx_kl                    | 0.00021486692 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.05         |\n|    explained_variance           | 0.82          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 9.98          |\n|    n_updates                    | 596           |\n|    policy_gradient_loss         | -3.63e-05     |\n|    value_loss                   | 18.1          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 123           |\n|    action_queue_updates_total   | 169           |\n|    ice_dug                      | 8             |\n|    water_produced               | 0             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 13.3         |\n| time/                           |              |\n|    fps                          | 818          |\n|    iterations                   | 300          |\n|    time_elapsed                 | 1466         |\n|    total_timesteps              | 1200000      |\n| train/                          |              |\n|    approx_kl                    | 0.0004866078 |\n|    clip_fraction                | 0.00025      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.23        |\n|    explained_variance           | 0.86         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 1.61         |\n|    n_updates                    | 598          |\n|    policy_gradient_loss         | -0.000574    |\n|    value_loss                   | 4.02         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 161          |\n|    ice_dug                      | 120          |\n|    water_produced               | 22.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 12.5          |\n| time/                           |               |\n|    fps                          | 818           |\n|    iterations                   | 301           |\n|    time_elapsed                 | 1470          |\n|    total_timesteps              | 1204000       |\n| train/                          |               |\n|    approx_kl                    | 0.00045920373 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.96         |\n|    explained_variance           | 0.687         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 22.6          |\n|    n_updates                    | 600           |\n|    policy_gradient_loss         | -0.000318     |\n|    value_loss                   | 49.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 128           |\n|    action_queue_updates_total   | 162           |\n|    ice_dug                      | 30            |\n|    water_produced               | 7.25          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 8.25         |\n| time/                           |              |\n|    fps                          | 818          |\n|    iterations                   | 302          |\n|    time_elapsed                 | 1475         |\n|    total_timesteps              | 1208000      |\n| train/                          |              |\n|    approx_kl                    | 0.0005177533 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.07        |\n|    explained_variance           | 0.764        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 8.45         |\n|    n_updates                    | 602          |\n|    policy_gradient_loss         | -0.000422    |\n|    value_loss                   | 18.4         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 168          |\n|    ice_dug                      | 10           |\n|    water_produced               | 2            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 7.44          |\n| time/                           |               |\n|    fps                          | 818           |\n|    iterations                   | 303           |\n|    time_elapsed                 | 1480          |\n|    total_timesteps              | 1212000       |\n| train/                          |               |\n|    approx_kl                    | 0.00035707653 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.2          |\n|    explained_variance           | 0.845         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.43          |\n|    n_updates                    | 604           |\n|    policy_gradient_loss         | -4.12e-05     |\n|    value_loss                   | 6.32          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 168           |\n|    ice_dug                      | 26            |\n|    water_produced               | 3.5           |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 12.4         |\n| time/                           |              |\n|    fps                          | 819          |\n|    iterations                   | 304          |\n|    time_elapsed                 | 1484         |\n|    total_timesteps              | 1216000      |\n| train/                          |              |\n|    approx_kl                    | 9.287884e-05 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.2         |\n|    explained_variance           | 0.824        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 3.74         |\n|    n_updates                    | 606          |\n|    policy_gradient_loss         | -0.000353    |\n|    value_loss                   | 7.66         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 140          |\n|    action_queue_updates_total   | 162          |\n|    ice_dug                      | 134          |\n|    water_produced               | 23.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 10.3          |\n| time/                           |               |\n|    fps                          | 819           |\n|    iterations                   | 305           |\n|    time_elapsed                 | 1489          |\n|    total_timesteps              | 1220000       |\n| train/                          |               |\n|    approx_kl                    | 0.00015605132 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.04         |\n|    explained_variance           | 0.596         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 34.3          |\n|    n_updates                    | 608           |\n|    policy_gradient_loss         | -0.000346     |\n|    value_loss                   | 75            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 134           |\n|    action_queue_updates_total   | 169           |\n|    ice_dug                      | 53            |\n|    water_produced               | 12.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 13.8         |\n| time/                           |              |\n|    fps                          | 819          |\n|    iterations                   | 306          |\n|    time_elapsed                 | 1493         |\n|    total_timesteps              | 1224000      |\n| train/                          |              |\n|    approx_kl                    | 0.0002167614 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.19        |\n|    explained_variance           | 0.684        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 16.4         |\n|    n_updates                    | 610          |\n|    policy_gradient_loss         | 0.000101     |\n|    value_loss                   | 30.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 170          |\n|    ice_dug                      | 97           |\n|    water_produced               | 24.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 16.7         |\n| time/                           |              |\n|    fps                          | 819          |\n|    iterations                   | 307          |\n|    time_elapsed                 | 1498         |\n|    total_timesteps              | 1228000      |\n| train/                          |              |\n|    approx_kl                    | 0.0002460958 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.2         |\n|    explained_variance           | 0.631        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 26.6         |\n|    n_updates                    | 612          |\n|    policy_gradient_loss         | 0.000139     |\n|    value_loss                   | 66.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 144          |\n|    action_queue_updates_total   | 167          |\n|    ice_dug                      | 86           |\n|    water_produced               | 16           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 18.8          |\n| time/                           |               |\n|    fps                          | 819           |\n|    iterations                   | 308           |\n|    time_elapsed                 | 1502          |\n|    total_timesteps              | 1232000       |\n| train/                          |               |\n|    approx_kl                    | 0.00046287593 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.04         |\n|    explained_variance           | 0.68          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 12.7          |\n|    n_updates                    | 614           |\n|    policy_gradient_loss         | 0.000273      |\n|    value_loss                   | 31            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 136           |\n|    action_queue_updates_total   | 167           |\n|    ice_dug                      | 60            |\n|    water_produced               | 13.2          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 17.3         |\n| time/                           |              |\n|    fps                          | 819          |\n|    iterations                   | 309          |\n|    time_elapsed                 | 1507         |\n|    total_timesteps              | 1236000      |\n| train/                          |              |\n|    approx_kl                    | 0.0006344596 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.16        |\n|    explained_variance           | 0.655        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 33.3         |\n|    n_updates                    | 616          |\n|    policy_gradient_loss         | -0.000188    |\n|    value_loss                   | 44.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 127          |\n|    action_queue_updates_total   | 170          |\n|    ice_dug                      | 71           |\n|    water_produced               | 17           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18.2         |\n| time/                           |              |\n|    fps                          | 820          |\n|    iterations                   | 310          |\n|    time_elapsed                 | 1512         |\n|    total_timesteps              | 1240000      |\n| train/                          |              |\n|    approx_kl                    | 0.0002316423 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.2         |\n|    explained_variance           | 0.739        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 9.85         |\n|    n_updates                    | 618          |\n|    policy_gradient_loss         | 7.57e-06     |\n|    value_loss                   | 30.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 137          |\n|    action_queue_updates_total   | 165          |\n|    ice_dug                      | 115          |\n|    water_produced               | 16.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 18           |\n| time/                           |              |\n|    fps                          | 820          |\n|    iterations                   | 311          |\n|    time_elapsed                 | 1516         |\n|    total_timesteps              | 1244000      |\n| train/                          |              |\n|    approx_kl                    | 0.0009554931 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.09        |\n|    explained_variance           | 0.669        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 22.2         |\n|    n_updates                    | 620          |\n|    policy_gradient_loss         | 0.000797     |\n|    value_loss                   | 44.9         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 136          |\n|    action_queue_updates_total   | 164          |\n|    ice_dug                      | 103          |\n|    water_produced               | 23           |\n--------------------------------------------------\nEval num_timesteps=1248000, episode_reward=1.88 +/- 3.76\nEpisode length: 301.00 +/- 0.00\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 301           |\n|    mean_reward                  | 1.88          |\n| time/                           |               |\n|    total_timesteps              | 1248000       |\n| train/                          |               |\n|    approx_kl                    | 0.00045636072 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.06         |\n|    explained_variance           | 0.676         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 35.6          |\n|    n_updates                    | 622           |\n|    policy_gradient_loss         | -0.000332     |\n|    value_loss                   | 53.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 132           |\n|    action_queue_updates_total   | 156           |\n|    ice_dug                      | 21            |\n|    water_produced               | 2.25          |\n---------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 15.1     |\n| time/              |          |\n|    fps             | 819      |\n|    iterations      | 312      |\n|    time_elapsed    | 1523     |\n|    total_timesteps | 1248000  |\n---------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 16.8        |\n| time/                           |             |\n|    fps                          | 819         |\n|    iterations                   | 313         |\n|    time_elapsed                 | 1528        |\n|    total_timesteps              | 1252000     |\n| train/                          |             |\n|    approx_kl                    | 0.002095736 |\n|    clip_fraction                | 0.00188     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -2          |\n|    explained_variance           | 0.841       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 4.02        |\n|    n_updates                    | 624         |\n|    policy_gradient_loss         | -0.000134   |\n|    value_loss                   | 9           |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 135         |\n|    action_queue_updates_total   | 159         |\n|    ice_dug                      | 136         |\n|    water_produced               | 21          |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 13.2         |\n| time/                           |              |\n|    fps                          | 819          |\n|    iterations                   | 314          |\n|    time_elapsed                 | 1532         |\n|    total_timesteps              | 1256000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011950241 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.98        |\n|    explained_variance           | 0.691        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 21.5         |\n|    n_updates                    | 626          |\n|    policy_gradient_loss         | -0.000476    |\n|    value_loss                   | 52.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 130          |\n|    action_queue_updates_total   | 175          |\n|    ice_dug                      | 0            |\n|    water_produced               | 0            |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 10.5          |\n| time/                           |               |\n|    fps                          | 819           |\n|    iterations                   | 315           |\n|    time_elapsed                 | 1537          |\n|    total_timesteps              | 1260000       |\n| train/                          |               |\n|    approx_kl                    | 0.00046619904 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.32         |\n|    explained_variance           | 0.849         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.98          |\n|    n_updates                    | 628           |\n|    policy_gradient_loss         | -0.000144     |\n|    value_loss                   | 2.52          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 127           |\n|    action_queue_updates_total   | 177           |\n|    ice_dug                      | 16            |\n|    water_produced               | 3.25          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 5.86          |\n| time/                           |               |\n|    fps                          | 819           |\n|    iterations                   | 316           |\n|    time_elapsed                 | 1542          |\n|    total_timesteps              | 1264000       |\n| train/                          |               |\n|    approx_kl                    | 0.00029291006 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.28         |\n|    explained_variance           | 0.828         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 2.18          |\n|    n_updates                    | 630           |\n|    policy_gradient_loss         | -0.000677     |\n|    value_loss                   | 6.11          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 132           |\n|    action_queue_updates_total   | 171           |\n|    ice_dug                      | 5             |\n|    water_produced               | 1             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 7.63         |\n| time/                           |              |\n|    fps                          | 819          |\n|    iterations                   | 317          |\n|    time_elapsed                 | 1547         |\n|    total_timesteps              | 1268000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011448052 |\n|    clip_fraction                | 0.00225      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.23        |\n|    explained_variance           | 0.843        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 1.62         |\n|    n_updates                    | 632          |\n|    policy_gradient_loss         | 0.000466     |\n|    value_loss                   | 3.45         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 139          |\n|    action_queue_updates_total   | 176          |\n|    ice_dug                      | 56           |\n|    water_produced               | 10.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.98          |\n| time/                           |               |\n|    fps                          | 819           |\n|    iterations                   | 318           |\n|    time_elapsed                 | 1551          |\n|    total_timesteps              | 1272000       |\n| train/                          |               |\n|    approx_kl                    | 0.00020211484 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.23         |\n|    explained_variance           | 0.684         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 12.6          |\n|    n_updates                    | 634           |\n|    policy_gradient_loss         | -0.000292     |\n|    value_loss                   | 24            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 172           |\n|    ice_dug                      | 36            |\n|    water_produced               | 8.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.6           |\n| time/                           |               |\n|    fps                          | 819           |\n|    iterations                   | 319           |\n|    time_elapsed                 | 1556          |\n|    total_timesteps              | 1276000       |\n| train/                          |               |\n|    approx_kl                    | 0.00010213704 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.19         |\n|    explained_variance           | 0.706         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 10.6          |\n|    n_updates                    | 636           |\n|    policy_gradient_loss         | -0.000202     |\n|    value_loss                   | 21.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 173           |\n|    ice_dug                      | 38            |\n|    water_produced               | 7.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 5.92          |\n| time/                           |               |\n|    fps                          | 820           |\n|    iterations                   | 320           |\n|    time_elapsed                 | 1560          |\n|    total_timesteps              | 1280000       |\n| train/                          |               |\n|    approx_kl                    | 0.00031237624 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.22         |\n|    explained_variance           | 0.772         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.48          |\n|    n_updates                    | 638           |\n|    policy_gradient_loss         | 0.000327      |\n|    value_loss                   | 11            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 133           |\n|    action_queue_updates_total   | 172           |\n|    ice_dug                      | 0             |\n|    water_produced               | 0             |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 5.71         |\n| time/                           |              |\n|    fps                          | 820          |\n|    iterations                   | 321          |\n|    time_elapsed                 | 1564         |\n|    total_timesteps              | 1284000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011285143 |\n|    clip_fraction                | 0.000625     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.17        |\n|    explained_variance           | 0.861        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 1.23         |\n|    n_updates                    | 640          |\n|    policy_gradient_loss         | 0.000108     |\n|    value_loss                   | 2.8          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 131          |\n|    action_queue_updates_total   | 177          |\n|    ice_dug                      | 0            |\n|    water_produced               | 0            |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 5.71         |\n| time/                           |              |\n|    fps                          | 820          |\n|    iterations                   | 322          |\n|    time_elapsed                 | 1569         |\n|    total_timesteps              | 1288000      |\n| train/                          |              |\n|    approx_kl                    | 0.0006486765 |\n|    clip_fraction                | 0.0015       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.34        |\n|    explained_variance           | 0.85         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.279        |\n|    n_updates                    | 642          |\n|    policy_gradient_loss         | 0.000494     |\n|    value_loss                   | 0.772        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 146          |\n|    action_queue_updates_total   | 179          |\n|    ice_dug                      | 58           |\n|    water_produced               | 10.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 5.09         |\n| time/                           |              |\n|    fps                          | 821          |\n|    iterations                   | 323          |\n|    time_elapsed                 | 1573         |\n|    total_timesteps              | 1292000      |\n| train/                          |              |\n|    approx_kl                    | 0.0002622258 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.25        |\n|    explained_variance           | 0.713        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 8.06         |\n|    n_updates                    | 644          |\n|    policy_gradient_loss         | -0.000331    |\n|    value_loss                   | 16.8         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 137          |\n|    action_queue_updates_total   | 177          |\n|    ice_dug                      | 25           |\n|    water_produced               | 5.75         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.19          |\n| time/                           |               |\n|    fps                          | 821           |\n|    iterations                   | 324           |\n|    time_elapsed                 | 1578          |\n|    total_timesteps              | 1296000       |\n| train/                          |               |\n|    approx_kl                    | 0.00036467708 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.29         |\n|    explained_variance           | 0.688         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.82          |\n|    n_updates                    | 646           |\n|    policy_gradient_loss         | 0.000228      |\n|    value_loss                   | 15            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 144           |\n|    action_queue_updates_total   | 174           |\n|    ice_dug                      | 14            |\n|    water_produced               | 3.5           |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 4.67         |\n| time/                           |              |\n|    fps                          | 821          |\n|    iterations                   | 325          |\n|    time_elapsed                 | 1582         |\n|    total_timesteps              | 1300000      |\n| train/                          |              |\n|    approx_kl                    | 0.0012498202 |\n|    clip_fraction                | 0.00188      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.27        |\n|    explained_variance           | 0.745        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 5.83         |\n|    n_updates                    | 648          |\n|    policy_gradient_loss         | 0.000499     |\n|    value_loss                   | 7.78         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 135          |\n|    action_queue_updates_total   | 176          |\n|    ice_dug                      | 12           |\n|    water_produced               | 2.25         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.79          |\n| time/                           |               |\n|    fps                          | 821           |\n|    iterations                   | 326           |\n|    time_elapsed                 | 1587          |\n|    total_timesteps              | 1304000       |\n| train/                          |               |\n|    approx_kl                    | 0.00040356535 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.33         |\n|    explained_variance           | 0.785         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 1.41          |\n|    n_updates                    | 650           |\n|    policy_gradient_loss         | -0.000147     |\n|    value_loss                   | 2.61          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 147           |\n|    action_queue_updates_total   | 174           |\n|    ice_dug                      | 61            |\n|    water_produced               | 10            |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.94          |\n| time/                           |               |\n|    fps                          | 821           |\n|    iterations                   | 327           |\n|    time_elapsed                 | 1591          |\n|    total_timesteps              | 1308000       |\n| train/                          |               |\n|    approx_kl                    | 0.00036326153 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.25         |\n|    explained_variance           | 0.609         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 15.5          |\n|    n_updates                    | 652           |\n|    policy_gradient_loss         | -0.00071      |\n|    value_loss                   | 23            |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 145           |\n|    action_queue_updates_total   | 176           |\n|    ice_dug                      | 8             |\n|    water_produced               | 2             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 5.37          |\n| time/                           |               |\n|    fps                          | 822           |\n|    iterations                   | 328           |\n|    time_elapsed                 | 1596          |\n|    total_timesteps              | 1312000       |\n| train/                          |               |\n|    approx_kl                    | 0.00085185573 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.27         |\n|    explained_variance           | 0.75          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 2.36          |\n|    n_updates                    | 654           |\n|    policy_gradient_loss         | -0.000123     |\n|    value_loss                   | 4.03          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 177           |\n|    ice_dug                      | 38            |\n|    water_produced               | 7.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.69          |\n| time/                           |               |\n|    fps                          | 822           |\n|    iterations                   | 329           |\n|    time_elapsed                 | 1600          |\n|    total_timesteps              | 1316000       |\n| train/                          |               |\n|    approx_kl                    | 0.00029530743 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.31         |\n|    explained_variance           | 0.666         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.53          |\n|    n_updates                    | 656           |\n|    policy_gradient_loss         | 0.000131      |\n|    value_loss                   | 19.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 142           |\n|    action_queue_updates_total   | 176           |\n|    ice_dug                      | 1             |\n|    water_produced               | 0.25          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 8.1           |\n| time/                           |               |\n|    fps                          | 822           |\n|    iterations                   | 330           |\n|    time_elapsed                 | 1604          |\n|    total_timesteps              | 1320000       |\n| train/                          |               |\n|    approx_kl                    | 0.00031200075 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.27         |\n|    explained_variance           | 0.901         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.397         |\n|    n_updates                    | 658           |\n|    policy_gradient_loss         | 0.00057       |\n|    value_loss                   | 1.23          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 144           |\n|    action_queue_updates_total   | 173           |\n|    ice_dug                      | 94            |\n|    water_produced               | 18.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 6.09         |\n| time/                           |              |\n|    fps                          | 822          |\n|    iterations                   | 331          |\n|    time_elapsed                 | 1609         |\n|    total_timesteps              | 1324000      |\n| train/                          |              |\n|    approx_kl                    | 0.0013709879 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.26        |\n|    explained_variance           | 0.67         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 20.4         |\n|    n_updates                    | 660          |\n|    policy_gradient_loss         | -0.000149    |\n|    value_loss                   | 41.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 142          |\n|    action_queue_updates_total   | 177          |\n|    ice_dug                      | 4            |\n|    water_produced               | 0.5          |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 7.66          |\n| time/                           |               |\n|    fps                          | 822           |\n|    iterations                   | 332           |\n|    time_elapsed                 | 1613          |\n|    total_timesteps              | 1328000       |\n| train/                          |               |\n|    approx_kl                    | 0.00023335408 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.33         |\n|    explained_variance           | 0.79          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.576         |\n|    n_updates                    | 662           |\n|    policy_gradient_loss         | 0.000437      |\n|    value_loss                   | 1.34          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 147           |\n|    action_queue_updates_total   | 178           |\n|    ice_dug                      | 41            |\n|    water_produced               | 9.5           |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.24          |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 333           |\n|    time_elapsed                 | 1618          |\n|    total_timesteps              | 1332000       |\n| train/                          |               |\n|    approx_kl                    | 0.00015113055 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.23         |\n|    explained_variance           | 0.648         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 8.93          |\n|    n_updates                    | 664           |\n|    policy_gradient_loss         | 1.84e-05      |\n|    value_loss                   | 22.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 178           |\n|    ice_dug                      | 7             |\n|    water_produced               | 1             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 7.13          |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 334           |\n|    time_elapsed                 | 1622          |\n|    total_timesteps              | 1336000       |\n| train/                          |               |\n|    approx_kl                    | 0.00029954236 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.29         |\n|    explained_variance           | 0.837         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.535         |\n|    n_updates                    | 666           |\n|    policy_gradient_loss         | -0.000175     |\n|    value_loss                   | 1.15          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 147           |\n|    action_queue_updates_total   | 172           |\n|    ice_dug                      | 20            |\n|    water_produced               | 4.5           |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.35          |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 335           |\n|    time_elapsed                 | 1626          |\n|    total_timesteps              | 1340000       |\n| train/                          |               |\n|    approx_kl                    | 0.00051399204 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.27         |\n|    explained_variance           | 0.714         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.04          |\n|    n_updates                    | 668           |\n|    policy_gradient_loss         | 0.000122      |\n|    value_loss                   | 6.67          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 150           |\n|    action_queue_updates_total   | 175           |\n|    ice_dug                      | 29            |\n|    water_produced               | 5.25          |\n---------------------------------------------------\nEval num_timesteps=1344000, episode_reward=0.00 +/- 0.00\nEpisode length: 301.00 +/- 0.00\n---------------------------------------------------\n| eval/                           |               |\n|    mean_ep_length               | 301           |\n|    mean_reward                  | 0             |\n| time/                           |               |\n|    total_timesteps              | 1344000       |\n| train/                          |               |\n|    approx_kl                    | 0.00013514272 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.28         |\n|    explained_variance           | 0.712         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.1           |\n|    n_updates                    | 670           |\n|    policy_gradient_loss         | -6.45e-05     |\n|    value_loss                   | 7.63          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 145           |\n|    action_queue_updates_total   | 175           |\n|    ice_dug                      | 11            |\n|    water_produced               | 1.5           |\n---------------------------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 4.57     |\n| time/              |          |\n|    fps             | 822      |\n|    iterations      | 336      |\n|    time_elapsed    | 1633     |\n|    total_timesteps | 1344000  |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 2.96         |\n| time/                           |              |\n|    fps                          | 822          |\n|    iterations                   | 337          |\n|    time_elapsed                 | 1638         |\n|    total_timesteps              | 1348000      |\n| train/                          |              |\n|    approx_kl                    | 0.0007206025 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.27        |\n|    explained_variance           | 0.771        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 1.27         |\n|    n_updates                    | 672          |\n|    policy_gradient_loss         | 0.00075      |\n|    value_loss                   | 2.98         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 151          |\n|    action_queue_updates_total   | 176          |\n|    ice_dug                      | 11           |\n|    water_produced               | 1.75         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 2.74          |\n| time/                           |               |\n|    fps                          | 822           |\n|    iterations                   | 338           |\n|    time_elapsed                 | 1642          |\n|    total_timesteps              | 1352000       |\n| train/                          |               |\n|    approx_kl                    | 0.00046364925 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.33         |\n|    explained_variance           | 0.706         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 1.01          |\n|    n_updates                    | 674           |\n|    policy_gradient_loss         | 0.000686      |\n|    value_loss                   | 2.67          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 148           |\n|    action_queue_updates_total   | 176           |\n|    ice_dug                      | 0             |\n|    water_produced               | 0             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 1.97          |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 339           |\n|    time_elapsed                 | 1647          |\n|    total_timesteps              | 1356000       |\n| train/                          |               |\n|    approx_kl                    | 0.00054471765 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.34         |\n|    explained_variance           | 0.786         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.183         |\n|    n_updates                    | 676           |\n|    policy_gradient_loss         | -0.000361     |\n|    value_loss                   | 0.426         |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 157           |\n|    action_queue_updates_total   | 178           |\n|    ice_dug                      | 8             |\n|    water_produced               | 0.75          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.75          |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 340           |\n|    time_elapsed                 | 1651          |\n|    total_timesteps              | 1360000       |\n| train/                          |               |\n|    approx_kl                    | 0.00054264773 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.34         |\n|    explained_variance           | 0.456         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.568         |\n|    n_updates                    | 678           |\n|    policy_gradient_loss         | -2.82e-05     |\n|    value_loss                   | 0.885         |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 159           |\n|    action_queue_updates_total   | 174           |\n|    ice_dug                      | 95            |\n|    water_produced               | 18.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 7            |\n| time/                           |              |\n|    fps                          | 823          |\n|    iterations                   | 341          |\n|    time_elapsed                 | 1655         |\n|    total_timesteps              | 1364000      |\n| train/                          |              |\n|    approx_kl                    | 0.0038056516 |\n|    clip_fraction                | 0.0174       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.28        |\n|    explained_variance           | 0.579        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 13.9         |\n|    n_updates                    | 680          |\n|    policy_gradient_loss         | -0.00272     |\n|    value_loss                   | 30.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 153          |\n|    action_queue_updates_total   | 169          |\n|    ice_dug                      | 63           |\n|    water_produced               | 12.3         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.79          |\n| time/                           |               |\n|    fps                          | 823           |\n|    iterations                   | 342           |\n|    time_elapsed                 | 1660          |\n|    total_timesteps              | 1368000       |\n| train/                          |               |\n|    approx_kl                    | 0.00065781863 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.22         |\n|    explained_variance           | 0.547         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 12.9          |\n|    n_updates                    | 682           |\n|    policy_gradient_loss         | -0.00121      |\n|    value_loss                   | 29.9          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 155           |\n|    action_queue_updates_total   | 174           |\n|    ice_dug                      | 4             |\n|    water_produced               | 0.75          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 7.12         |\n| time/                           |              |\n|    fps                          | 824          |\n|    iterations                   | 343          |\n|    time_elapsed                 | 1664         |\n|    total_timesteps              | 1372000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011751932 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.3         |\n|    explained_variance           | 0.753        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 0.363        |\n|    n_updates                    | 684          |\n|    policy_gradient_loss         | -6.03e-05    |\n|    value_loss                   | 0.897        |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 159          |\n|    action_queue_updates_total   | 172          |\n|    ice_dug                      | 15           |\n|    water_produced               | 1.5          |\n--------------------------------------------------\n------------------------------------------------\n| rollout/                        |            |\n|    ep_len_mean                  | 200        |\n|    ep_rew_mean                  | 7.11       |\n| time/                           |            |\n|    fps                          | 824        |\n|    iterations                   | 344        |\n|    time_elapsed                 | 1669       |\n|    total_timesteps              | 1376000    |\n| train/                          |            |\n|    approx_kl                    | 0.00062722 |\n|    clip_fraction                | 0          |\n|    clip_range                   | 0.2        |\n|    entropy_loss                 | -2.28      |\n|    explained_variance           | 0.51       |\n|    learning_rate                | 0.0003     |\n|    loss                         | 1.68       |\n|    n_updates                    | 686        |\n|    policy_gradient_loss         | 0.000408   |\n|    value_loss                   | 2.47       |\n| train_metrics/                  |            |\n|    action_queue_updates_success | 150        |\n|    action_queue_updates_total   | 172        |\n|    ice_dug                      | 4          |\n|    water_produced               | 0.75       |\n------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 3.44          |\n| time/                           |               |\n|    fps                          | 824           |\n|    iterations                   | 345           |\n|    time_elapsed                 | 1673          |\n|    total_timesteps              | 1380000       |\n| train/                          |               |\n|    approx_kl                    | 0.00057513604 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.29         |\n|    explained_variance           | 0.758         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.232         |\n|    n_updates                    | 688           |\n|    policy_gradient_loss         | 0.00033       |\n|    value_loss                   | 0.693         |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 160           |\n|    action_queue_updates_total   | 172           |\n|    ice_dug                      | 11            |\n|    water_produced               | 1             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 2.13          |\n| time/                           |               |\n|    fps                          | 824           |\n|    iterations                   | 346           |\n|    time_elapsed                 | 1678          |\n|    total_timesteps              | 1384000       |\n| train/                          |               |\n|    approx_kl                    | 0.00044133878 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.29         |\n|    explained_variance           | 0.638         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 0.76          |\n|    n_updates                    | 690           |\n|    policy_gradient_loss         | 0.000335      |\n|    value_loss                   | 1.4           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 160           |\n|    action_queue_updates_total   | 173           |\n|    ice_dug                      | 32            |\n|    water_produced               | 6             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 2             |\n| time/                           |               |\n|    fps                          | 824           |\n|    iterations                   | 347           |\n|    time_elapsed                 | 1682          |\n|    total_timesteps              | 1388000       |\n| train/                          |               |\n|    approx_kl                    | 0.00040451757 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.29         |\n|    explained_variance           | 0.724         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 3.55          |\n|    n_updates                    | 692           |\n|    policy_gradient_loss         | -0.00046      |\n|    value_loss                   | 7.14          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 161           |\n|    action_queue_updates_total   | 177           |\n|    ice_dug                      | 13            |\n|    water_produced               | 0             |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 2.39          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 348           |\n|    time_elapsed                 | 1686          |\n|    total_timesteps              | 1392000       |\n| train/                          |               |\n|    approx_kl                    | 0.00014301736 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.32         |\n|    explained_variance           | 0.635         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 2             |\n|    n_updates                    | 694           |\n|    policy_gradient_loss         | -0.000138     |\n|    value_loss                   | 2.84          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 155           |\n|    action_queue_updates_total   | 173           |\n|    ice_dug                      | 37            |\n|    water_produced               | 3.25          |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 3.02          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 349           |\n|    time_elapsed                 | 1691          |\n|    total_timesteps              | 1396000       |\n| train/                          |               |\n|    approx_kl                    | 0.00038259962 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.28         |\n|    explained_variance           | 0.633         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 2.54          |\n|    n_updates                    | 696           |\n|    policy_gradient_loss         | -0.00137      |\n|    value_loss                   | 6.75          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 152           |\n|    action_queue_updates_total   | 173           |\n|    ice_dug                      | 40            |\n|    water_produced               | 3.5           |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 4.15          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 350           |\n|    time_elapsed                 | 1695          |\n|    total_timesteps              | 1400000       |\n| train/                          |               |\n|    approx_kl                    | 0.00021880404 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.24         |\n|    explained_variance           | 0.683         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.58          |\n|    n_updates                    | 698           |\n|    policy_gradient_loss         | -0.000548     |\n|    value_loss                   | 9.2           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 158           |\n|    action_queue_updates_total   | 176           |\n|    ice_dug                      | 29            |\n|    water_produced               | 6.5           |\n---------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 6.19          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 351           |\n|    time_elapsed                 | 1700          |\n|    total_timesteps              | 1404000       |\n| train/                          |               |\n|    approx_kl                    | 0.00025216496 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.25         |\n|    explained_variance           | 0.586         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 6.37          |\n|    n_updates                    | 700           |\n|    policy_gradient_loss         | -0.000681     |\n|    value_loss                   | 18.8          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 159           |\n|    action_queue_updates_total   | 173           |\n|    ice_dug                      | 74            |\n|    water_produced               | 15.8          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 11.2         |\n| time/                           |              |\n|    fps                          | 825          |\n|    iterations                   | 352          |\n|    time_elapsed                 | 1704         |\n|    total_timesteps              | 1408000      |\n| train/                          |              |\n|    approx_kl                    | 0.0010115396 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.23        |\n|    explained_variance           | 0.577        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 17.8         |\n|    n_updates                    | 702          |\n|    policy_gradient_loss         | -0.000462    |\n|    value_loss                   | 49.6         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 167          |\n|    action_queue_updates_total   | 172          |\n|    ice_dug                      | 114          |\n|    water_produced               | 24           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 16.3          |\n| time/                           |               |\n|    fps                          | 826           |\n|    iterations                   | 353           |\n|    time_elapsed                 | 1709          |\n|    total_timesteps              | 1412000       |\n| train/                          |               |\n|    approx_kl                    | 0.00075706496 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -2.14         |\n|    explained_variance           | 0.49          |\n|    learning_rate                | 0.0003        |\n|    loss                         | 69.6          |\n|    n_updates                    | 704           |\n|    policy_gradient_loss         | -2.89e-05     |\n|    value_loss                   | 94.5          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 155           |\n|    action_queue_updates_total   | 169           |\n|    ice_dug                      | 170           |\n|    water_produced               | 27.5          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 23.7         |\n| time/                           |              |\n|    fps                          | 826          |\n|    iterations                   | 354          |\n|    time_elapsed                 | 1713         |\n|    total_timesteps              | 1416000      |\n| train/                          |              |\n|    approx_kl                    | 0.0032165137 |\n|    clip_fraction                | 0.0149       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -2.09        |\n|    explained_variance           | 0.565        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 34.9         |\n|    n_updates                    | 706          |\n|    policy_gradient_loss         | -0.000829    |\n|    value_loss                   | 79.1         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 164          |\n|    action_queue_updates_total   | 168          |\n|    ice_dug                      | 225          |\n|    water_produced               | 38.8         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 28.4        |\n| time/                           |             |\n|    fps                          | 826         |\n|    iterations                   | 355         |\n|    time_elapsed                 | 1717        |\n|    total_timesteps              | 1420000     |\n| train/                          |             |\n|    approx_kl                    | 0.005030956 |\n|    clip_fraction                | 0.0268      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.96       |\n|    explained_variance           | 0.577       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 38.1        |\n|    n_updates                    | 708         |\n|    policy_gradient_loss         | -0.00144    |\n|    value_loss                   | 72.9        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 147         |\n|    action_queue_updates_total   | 161         |\n|    ice_dug                      | 206         |\n|    water_produced               | 28.2        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 42.2        |\n| time/                           |             |\n|    fps                          | 826         |\n|    iterations                   | 356         |\n|    time_elapsed                 | 1721        |\n|    total_timesteps              | 1424000     |\n| train/                          |             |\n|    approx_kl                    | 0.004134828 |\n|    clip_fraction                | 0.0155      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.93       |\n|    explained_variance           | 0.592       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 48.7        |\n|    n_updates                    | 710         |\n|    policy_gradient_loss         | -0.000834   |\n|    value_loss                   | 73.2        |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 146         |\n|    action_queue_updates_total   | 162         |\n|    ice_dug                      | 412         |\n|    water_produced               | 81.2        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 46.9         |\n| time/                           |              |\n|    fps                          | 827          |\n|    iterations                   | 357          |\n|    time_elapsed                 | 1726         |\n|    total_timesteps              | 1428000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011539629 |\n|    clip_fraction                | 0.000125     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.83        |\n|    explained_variance           | 0.524        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 126          |\n|    n_updates                    | 712          |\n|    policy_gradient_loss         | -0.000592    |\n|    value_loss                   | 331          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 150          |\n|    ice_dug                      | 253          |\n|    water_produced               | 46           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 49           |\n| time/                           |              |\n|    fps                          | 827          |\n|    iterations                   | 358          |\n|    time_elapsed                 | 1731         |\n|    total_timesteps              | 1432000      |\n| train/                          |              |\n|    approx_kl                    | 0.0038175047 |\n|    clip_fraction                | 0.0161       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.69        |\n|    explained_variance           | 0.502        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 102          |\n|    n_updates                    | 714          |\n|    policy_gradient_loss         | -0.00116     |\n|    value_loss                   | 161          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 132          |\n|    action_queue_updates_total   | 142          |\n|    ice_dug                      | 285          |\n|    water_produced               | 37           |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 49.8          |\n| time/                           |               |\n|    fps                          | 827           |\n|    iterations                   | 359           |\n|    time_elapsed                 | 1736          |\n|    total_timesteps              | 1436000       |\n| train/                          |               |\n|    approx_kl                    | 0.00055099506 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.59         |\n|    explained_variance           | 0.555         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 65.4          |\n|    n_updates                    | 716           |\n|    policy_gradient_loss         | -0.000637     |\n|    value_loss                   | 106           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 139           |\n|    action_queue_updates_total   | 143           |\n|    ice_dug                      | 209           |\n|    water_produced               | 42.7          |\n---------------------------------------------------\nEval num_timesteps=1440000, episode_reward=100.96 +/- 131.28\nEpisode length: 398.00 +/- 126.24\n-------------------------------------------------\n| eval/                           |             |\n|    mean_ep_length               | 398         |\n|    mean_reward                  | 101         |\n| time/                           |             |\n|    total_timesteps              | 1440000     |\n| train/                          |             |\n|    approx_kl                    | 0.001383229 |\n|    clip_fraction                | 0.00075     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.6        |\n|    explained_variance           | 0.518       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 44.7        |\n|    n_updates                    | 718         |\n|    policy_gradient_loss         | -0.000926   |\n|    value_loss                   | 101         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 141         |\n|    action_queue_updates_total   | 151         |\n|    ice_dug                      | 185         |\n|    water_produced               | 30.8        |\n-------------------------------------------------\nNew best mean reward!\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 50.2     |\n| time/              |          |\n|    fps             | 826      |\n|    iterations      | 360      |\n|    time_elapsed    | 1743     |\n|    total_timesteps | 1440000  |\n---------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 51.2          |\n| time/                           |               |\n|    fps                          | 826           |\n|    iterations                   | 361           |\n|    time_elapsed                 | 1747          |\n|    total_timesteps              | 1444000       |\n| train/                          |               |\n|    approx_kl                    | 0.00087729207 |\n|    clip_fraction                | 0.00363       |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.76         |\n|    explained_variance           | 0.538         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 51            |\n|    n_updates                    | 720           |\n|    policy_gradient_loss         | -0.000333     |\n|    value_loss                   | 92.6          |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 151           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 411           |\n|    water_produced               | 86.2          |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 55.3         |\n| time/                           |              |\n|    fps                          | 826          |\n|    iterations                   | 362          |\n|    time_elapsed                 | 1752         |\n|    total_timesteps              | 1448000      |\n| train/                          |              |\n|    approx_kl                    | 0.0047625764 |\n|    clip_fraction                | 0.0252       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.66        |\n|    explained_variance           | 0.536        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 138          |\n|    n_updates                    | 722          |\n|    policy_gradient_loss         | 0.000488     |\n|    value_loss                   | 244          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 137          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 338          |\n|    water_produced               | 65.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 60.7         |\n| time/                           |              |\n|    fps                          | 826          |\n|    iterations                   | 363          |\n|    time_elapsed                 | 1756         |\n|    total_timesteps              | 1452000      |\n| train/                          |              |\n|    approx_kl                    | 0.0066222725 |\n|    clip_fraction                | 0.0409       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.65        |\n|    explained_variance           | 0.467        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 130          |\n|    n_updates                    | 724          |\n|    policy_gradient_loss         | -0.000971    |\n|    value_loss                   | 239          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 123          |\n|    action_queue_updates_total   | 127          |\n|    ice_dug                      | 289          |\n|    water_produced               | 64           |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 68.9        |\n| time/                           |             |\n|    fps                          | 826         |\n|    iterations                   | 364         |\n|    time_elapsed                 | 1761        |\n|    total_timesteps              | 1456000     |\n| train/                          |             |\n|    approx_kl                    | 0.001119815 |\n|    clip_fraction                | 0.000625    |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.3        |\n|    explained_variance           | 0.451       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 108         |\n|    n_updates                    | 726         |\n|    policy_gradient_loss         | 0.000359    |\n|    value_loss                   | 207         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 137         |\n|    action_queue_updates_total   | 148         |\n|    ice_dug                      | 459         |\n|    water_produced               | 81.3        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 74.4         |\n| time/                           |              |\n|    fps                          | 826          |\n|    iterations                   | 365          |\n|    time_elapsed                 | 1766         |\n|    total_timesteps              | 1460000      |\n| train/                          |              |\n|    approx_kl                    | 0.0010913487 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.45        |\n|    explained_variance           | 0.431        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 162          |\n|    n_updates                    | 728          |\n|    policy_gradient_loss         | -0.000715    |\n|    value_loss                   | 307          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 118          |\n|    action_queue_updates_total   | 128          |\n|    ice_dug                      | 255          |\n|    water_produced               | 57.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 62.1         |\n| time/                           |              |\n|    fps                          | 826          |\n|    iterations                   | 366          |\n|    time_elapsed                 | 1770         |\n|    total_timesteps              | 1464000      |\n| train/                          |              |\n|    approx_kl                    | 0.0001394096 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.29        |\n|    explained_variance           | 0.447        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 76.5         |\n|    n_updates                    | 730          |\n|    policy_gradient_loss         | -0.000304    |\n|    value_loss                   | 189          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 123          |\n|    action_queue_updates_total   | 127          |\n|    ice_dug                      | 129          |\n|    water_produced               | 27.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 65.5         |\n| time/                           |              |\n|    fps                          | 826          |\n|    iterations                   | 367          |\n|    time_elapsed                 | 1775         |\n|    total_timesteps              | 1468000      |\n| train/                          |              |\n|    approx_kl                    | 0.0029549673 |\n|    clip_fraction                | 0.0136       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.37        |\n|    explained_variance           | 0.421        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 67.7         |\n|    n_updates                    | 732          |\n|    policy_gradient_loss         | -0.000472    |\n|    value_loss                   | 123          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 134          |\n|    action_queue_updates_total   | 145          |\n|    ice_dug                      | 416          |\n|    water_produced               | 81.8         |\n--------------------------------------------------\n----------------------------------------------------\n| rollout/                        |                |\n|    ep_len_mean                  | 200            |\n|    ep_rew_mean                  | 68.5           |\n| time/                           |                |\n|    fps                          | 826            |\n|    iterations                   | 368            |\n|    time_elapsed                 | 1780           |\n|    total_timesteps              | 1472000        |\n| train/                          |                |\n|    approx_kl                    | 0.000114817034 |\n|    clip_fraction                | 0              |\n|    clip_range                   | 0.2            |\n|    entropy_loss                 | -1.47          |\n|    explained_variance           | 0.48           |\n|    learning_rate                | 0.0003         |\n|    loss                         | 127            |\n|    n_updates                    | 734            |\n|    policy_gradient_loss         | -9.22e-05      |\n|    value_loss                   | 289            |\n| train_metrics/                  |                |\n|    action_queue_updates_success | 133            |\n|    action_queue_updates_total   | 140            |\n|    ice_dug                      | 397            |\n|    water_produced               | 77.5           |\n----------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 65.7        |\n| time/                           |             |\n|    fps                          | 826         |\n|    iterations                   | 369         |\n|    time_elapsed                 | 1784        |\n|    total_timesteps              | 1476000     |\n| train/                          |             |\n|    approx_kl                    | 0.002678215 |\n|    clip_fraction                | 0.00625     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.4        |\n|    explained_variance           | 0.473       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 94.7        |\n|    n_updates                    | 736         |\n|    policy_gradient_loss         | 2.16e-06    |\n|    value_loss                   | 224         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 129         |\n|    action_queue_updates_total   | 143         |\n|    ice_dug                      | 327         |\n|    water_produced               | 68.8        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 67.5         |\n| time/                           |              |\n|    fps                          | 827          |\n|    iterations                   | 370          |\n|    time_elapsed                 | 1789         |\n|    total_timesteps              | 1480000      |\n| train/                          |              |\n|    approx_kl                    | 0.0002475088 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.45        |\n|    explained_variance           | 0.472        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 71.7         |\n|    n_updates                    | 738          |\n|    policy_gradient_loss         | 0.000381     |\n|    value_loss                   | 208          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 125          |\n|    action_queue_updates_total   | 133          |\n|    ice_dug                      | 401          |\n|    water_produced               | 65.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 70.7          |\n| time/                           |               |\n|    fps                          | 827           |\n|    iterations                   | 371           |\n|    time_elapsed                 | 1793          |\n|    total_timesteps              | 1484000       |\n| train/                          |               |\n|    approx_kl                    | 0.00083621533 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.28         |\n|    explained_variance           | 0.462         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 89.1          |\n|    n_updates                    | 740           |\n|    policy_gradient_loss         | -0.00068      |\n|    value_loss                   | 185           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 123           |\n|    action_queue_updates_total   | 127           |\n|    ice_dug                      | 189           |\n|    water_produced               | 43            |\n---------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 62.4         |\n| time/                           |              |\n|    fps                          | 827          |\n|    iterations                   | 372          |\n|    time_elapsed                 | 1798         |\n|    total_timesteps              | 1488000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011812802 |\n|    clip_fraction                | 0.00675      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.3         |\n|    explained_variance           | 0.424        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 62.7         |\n|    n_updates                    | 742          |\n|    policy_gradient_loss         | -0.000737    |\n|    value_loss                   | 145          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 139          |\n|    ice_dug                      | 248          |\n|    water_produced               | 41.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 55.3         |\n| time/                           |              |\n|    fps                          | 827          |\n|    iterations                   | 373          |\n|    time_elapsed                 | 1803         |\n|    total_timesteps              | 1492000      |\n| train/                          |              |\n|    approx_kl                    | 0.0007848259 |\n|    clip_fraction                | 0.00275      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.41        |\n|    explained_variance           | 0.477        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 48           |\n|    n_updates                    | 744          |\n|    policy_gradient_loss         | -0.000524    |\n|    value_loss                   | 134          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 133          |\n|    action_queue_updates_total   | 142          |\n|    ice_dug                      | 233          |\n|    water_produced               | 44           |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 47.5         |\n| time/                           |              |\n|    fps                          | 827          |\n|    iterations                   | 374          |\n|    time_elapsed                 | 1808         |\n|    total_timesteps              | 1496000      |\n| train/                          |              |\n|    approx_kl                    | 0.0015707914 |\n|    clip_fraction                | 0.00288      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.45        |\n|    explained_variance           | 0.453        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 69.8         |\n|    n_updates                    | 746          |\n|    policy_gradient_loss         | -0.000135    |\n|    value_loss                   | 158          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 137          |\n|    water_produced               | 31.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 53.1         |\n| time/                           |              |\n|    fps                          | 827          |\n|    iterations                   | 375          |\n|    time_elapsed                 | 1813         |\n|    total_timesteps              | 1500000      |\n| train/                          |              |\n|    approx_kl                    | 0.0036763034 |\n|    clip_fraction                | 0.0166       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.67        |\n|    explained_variance           | 0.51         |\n|    learning_rate                | 0.0003       |\n|    loss                         | 42.8         |\n|    n_updates                    | 748          |\n|    policy_gradient_loss         | -0.000443    |\n|    value_loss                   | 82.3         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 148          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 394          |\n|    water_produced               | 93.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 55.1         |\n| time/                           |              |\n|    fps                          | 827          |\n|    iterations                   | 376          |\n|    time_elapsed                 | 1817         |\n|    total_timesteps              | 1504000      |\n| train/                          |              |\n|    approx_kl                    | 0.0007597172 |\n|    clip_fraction                | 0            |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.51        |\n|    explained_variance           | 0.516        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 135          |\n|    n_updates                    | 750          |\n|    policy_gradient_loss         | -0.000129    |\n|    value_loss                   | 250          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 156          |\n|    ice_dug                      | 422          |\n|    water_produced               | 50.5         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 48.9         |\n| time/                           |              |\n|    fps                          | 827          |\n|    iterations                   | 377          |\n|    time_elapsed                 | 1822         |\n|    total_timesteps              | 1508000      |\n| train/                          |              |\n|    approx_kl                    | 0.0015540597 |\n|    clip_fraction                | 0.000375     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.56        |\n|    explained_variance           | 0.497        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 70.1         |\n|    n_updates                    | 752          |\n|    policy_gradient_loss         | -0.000361    |\n|    value_loss                   | 152          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 138          |\n|    action_queue_updates_total   | 149          |\n|    ice_dug                      | 65           |\n|    water_produced               | 12.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 51.8         |\n| time/                           |              |\n|    fps                          | 827          |\n|    iterations                   | 378          |\n|    time_elapsed                 | 1827         |\n|    total_timesteps              | 1512000      |\n| train/                          |              |\n|    approx_kl                    | 0.0051081693 |\n|    clip_fraction                | 0.0294       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.67        |\n|    explained_variance           | 0.584        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 12.6         |\n|    n_updates                    | 754          |\n|    policy_gradient_loss         | 0.000933     |\n|    value_loss                   | 27.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 142          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 330          |\n|    water_produced               | 57.8         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 63            |\n| time/                           |               |\n|    fps                          | 827           |\n|    iterations                   | 379           |\n|    time_elapsed                 | 1832          |\n|    total_timesteps              | 1516000       |\n| train/                          |               |\n|    approx_kl                    | 0.00017399379 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.61         |\n|    explained_variance           | 0.535         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 89.7          |\n|    n_updates                    | 756           |\n|    policy_gradient_loss         | -0.000171     |\n|    value_loss                   | 179           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 137           |\n|    action_queue_updates_total   | 139           |\n|    ice_dug                      | 390           |\n|    water_produced               | 84.2          |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 60.9        |\n| time/                           |             |\n|    fps                          | 827         |\n|    iterations                   | 380         |\n|    time_elapsed                 | 1836        |\n|    total_timesteps              | 1520000     |\n| train/                          |             |\n|    approx_kl                    | 0.004943449 |\n|    clip_fraction                | 0.0246      |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.35       |\n|    explained_variance           | 0.47        |\n|    learning_rate                | 0.0003      |\n|    loss                         | 99.5        |\n|    n_updates                    | 758         |\n|    policy_gradient_loss         | 0.00129     |\n|    value_loss                   | 230         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 129         |\n|    action_queue_updates_total   | 136         |\n|    ice_dug                      | 465         |\n|    water_produced               | 82.8        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 50.8        |\n| time/                           |             |\n|    fps                          | 827         |\n|    iterations                   | 381         |\n|    time_elapsed                 | 1841        |\n|    total_timesteps              | 1524000     |\n| train/                          |             |\n|    approx_kl                    | 0.002567702 |\n|    clip_fraction                | 0.00988     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.32       |\n|    explained_variance           | 0.486       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 115         |\n|    n_updates                    | 760         |\n|    policy_gradient_loss         | -0.000433   |\n|    value_loss                   | 229         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 127         |\n|    action_queue_updates_total   | 137         |\n|    ice_dug                      | 17          |\n|    water_produced               | 4           |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 61.8         |\n| time/                           |              |\n|    fps                          | 827          |\n|    iterations                   | 382          |\n|    time_elapsed                 | 1846         |\n|    total_timesteps              | 1528000      |\n| train/                          |              |\n|    approx_kl                    | 0.0011406463 |\n|    clip_fraction                | 0.00412      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.5         |\n|    explained_variance           | 0.537        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 5.79         |\n|    n_updates                    | 762          |\n|    policy_gradient_loss         | -0.000714    |\n|    value_loss                   | 17.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 145          |\n|    action_queue_updates_total   | 152          |\n|    ice_dug                      | 284          |\n|    water_produced               | 65.2         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 54.7         |\n| time/                           |              |\n|    fps                          | 827          |\n|    iterations                   | 383          |\n|    time_elapsed                 | 1850         |\n|    total_timesteps              | 1532000      |\n| train/                          |              |\n|    approx_kl                    | 0.0014206026 |\n|    clip_fraction                | 0.0045       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.51        |\n|    explained_variance           | 0.453        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 109          |\n|    n_updates                    | 764          |\n|    policy_gradient_loss         | -0.000753    |\n|    value_loss                   | 254          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 150          |\n|    ice_dug                      | 102          |\n|    water_produced               | 24.7         |\n--------------------------------------------------\nEval num_timesteps=1536000, episode_reward=148.20 +/- 180.83\nEpisode length: 442.00 +/- 173.76\n--------------------------------------------------\n| eval/                           |              |\n|    mean_ep_length               | 442          |\n|    mean_reward                  | 148          |\n| time/                           |              |\n|    total_timesteps              | 1536000      |\n| train/                          |              |\n|    approx_kl                    | 0.0005775105 |\n|    clip_fraction                | 0.000125     |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.64        |\n|    explained_variance           | 0.522        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 34.6         |\n|    n_updates                    | 766          |\n|    policy_gradient_loss         | -0.000153    |\n|    value_loss                   | 69.7         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 144          |\n|    action_queue_updates_total   | 159          |\n|    ice_dug                      | 172          |\n|    water_produced               | 42.8         |\n--------------------------------------------------\nNew best mean reward!\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 200      |\n|    ep_rew_mean     | 46       |\n| time/              |          |\n|    fps             | 825      |\n|    iterations      | 384      |\n|    time_elapsed    | 1861     |\n|    total_timesteps | 1536000  |\n---------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 35           |\n| time/                           |              |\n|    fps                          | 825          |\n|    iterations                   | 385          |\n|    time_elapsed                 | 1866         |\n|    total_timesteps              | 1540000      |\n| train/                          |              |\n|    approx_kl                    | 0.0022543606 |\n|    clip_fraction                | 0.001        |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.84        |\n|    explained_variance           | 0.557        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 57.2         |\n|    n_updates                    | 768          |\n|    policy_gradient_loss         | 0.00069      |\n|    value_loss                   | 117          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 141          |\n|    action_queue_updates_total   | 158          |\n|    ice_dug                      | 267          |\n|    water_produced               | 29.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 46.6         |\n| time/                           |              |\n|    fps                          | 825          |\n|    iterations                   | 386          |\n|    time_elapsed                 | 1871         |\n|    total_timesteps              | 1544000      |\n| train/                          |              |\n|    approx_kl                    | 0.0012980937 |\n|    clip_fraction                | 0.001        |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.76        |\n|    explained_variance           | 0.521        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 45.9         |\n|    n_updates                    | 770          |\n|    policy_gradient_loss         | 0.000118     |\n|    value_loss                   | 99.5         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 151          |\n|    action_queue_updates_total   | 162          |\n|    ice_dug                      | 246          |\n|    water_produced               | 59.8         |\n--------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 40.9         |\n| time/                           |              |\n|    fps                          | 825          |\n|    iterations                   | 387          |\n|    time_elapsed                 | 1875         |\n|    total_timesteps              | 1548000      |\n| train/                          |              |\n|    approx_kl                    | 0.0007145946 |\n|    clip_fraction                | 0.00175      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.8         |\n|    explained_variance           | 0.534        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 95.4         |\n|    n_updates                    | 772          |\n|    policy_gradient_loss         | 0.000478     |\n|    value_loss                   | 190          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 148          |\n|    action_queue_updates_total   | 165          |\n|    ice_dug                      | 331          |\n|    water_produced               | 36.5         |\n--------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 41.6        |\n| time/                           |             |\n|    fps                          | 825         |\n|    iterations                   | 388         |\n|    time_elapsed                 | 1880        |\n|    total_timesteps              | 1552000     |\n| train/                          |             |\n|    approx_kl                    | 0.001006267 |\n|    clip_fraction                | 0           |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.84       |\n|    explained_variance           | 0.537       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 62.9        |\n|    n_updates                    | 774         |\n|    policy_gradient_loss         | 0.000207    |\n|    value_loss                   | 122         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 139         |\n|    action_queue_updates_total   | 144         |\n|    ice_dug                      | 158         |\n|    water_produced               | 27.5        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 43.3         |\n| time/                           |              |\n|    fps                          | 825          |\n|    iterations                   | 389          |\n|    time_elapsed                 | 1885         |\n|    total_timesteps              | 1556000      |\n| train/                          |              |\n|    approx_kl                    | 0.0034312743 |\n|    clip_fraction                | 0.0119       |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.63        |\n|    explained_variance           | 0.544        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 39.5         |\n|    n_updates                    | 776          |\n|    policy_gradient_loss         | -0.000533    |\n|    value_loss                   | 67.2         |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 146          |\n|    action_queue_updates_total   | 155          |\n|    ice_dug                      | 247          |\n|    water_produced               | 50.5         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 50.3          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 390           |\n|    time_elapsed                 | 1890          |\n|    total_timesteps              | 1560000       |\n| train/                          |               |\n|    approx_kl                    | 0.00073186384 |\n|    clip_fraction                | 0             |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.72         |\n|    explained_variance           | 0.538         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 92.2          |\n|    n_updates                    | 778           |\n|    policy_gradient_loss         | 0.000255      |\n|    value_loss                   | 157           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 142           |\n|    action_queue_updates_total   | 149           |\n|    ice_dug                      | 292           |\n|    water_produced               | 64.5          |\n---------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 52.2        |\n| time/                           |             |\n|    fps                          | 825         |\n|    iterations                   | 391         |\n|    time_elapsed                 | 1895        |\n|    total_timesteps              | 1564000     |\n| train/                          |             |\n|    approx_kl                    | 0.002189424 |\n|    clip_fraction                | 0.00388     |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.6        |\n|    explained_variance           | 0.566       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 103         |\n|    n_updates                    | 780         |\n|    policy_gradient_loss         | -0.000697   |\n|    value_loss                   | 177         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 148         |\n|    action_queue_updates_total   | 154         |\n|    ice_dug                      | 326         |\n|    water_produced               | 68.2        |\n-------------------------------------------------\n-------------------------------------------------\n| rollout/                        |             |\n|    ep_len_mean                  | 200         |\n|    ep_rew_mean                  | 61.1        |\n| time/                           |             |\n|    fps                          | 825         |\n|    iterations                   | 392         |\n|    time_elapsed                 | 1900        |\n|    total_timesteps              | 1568000     |\n| train/                          |             |\n|    approx_kl                    | 0.001144874 |\n|    clip_fraction                | 0.000375    |\n|    clip_range                   | 0.2         |\n|    entropy_loss                 | -1.64       |\n|    explained_variance           | 0.549       |\n|    learning_rate                | 0.0003      |\n|    loss                         | 89.8        |\n|    n_updates                    | 782         |\n|    policy_gradient_loss         | -9.89e-05   |\n|    value_loss                   | 204         |\n| train_metrics/                  |             |\n|    action_queue_updates_success | 138         |\n|    action_queue_updates_total   | 145         |\n|    ice_dug                      | 439         |\n|    water_produced               | 80.3        |\n-------------------------------------------------\n--------------------------------------------------\n| rollout/                        |              |\n|    ep_len_mean                  | 200          |\n|    ep_rew_mean                  | 61.7         |\n| time/                           |              |\n|    fps                          | 825          |\n|    iterations                   | 393          |\n|    time_elapsed                 | 1905         |\n|    total_timesteps              | 1572000      |\n| train/                          |              |\n|    approx_kl                    | 0.0023118933 |\n|    clip_fraction                | 0.00625      |\n|    clip_range                   | 0.2          |\n|    entropy_loss                 | -1.48        |\n|    explained_variance           | 0.525        |\n|    learning_rate                | 0.0003       |\n|    loss                         | 123          |\n|    n_updates                    | 784          |\n|    policy_gradient_loss         | -0.000882    |\n|    value_loss                   | 231          |\n| train_metrics/                  |              |\n|    action_queue_updates_success | 142          |\n|    action_queue_updates_total   | 151          |\n|    ice_dug                      | 152          |\n|    water_produced               | 30.2         |\n--------------------------------------------------\n---------------------------------------------------\n| rollout/                        |               |\n|    ep_len_mean                  | 200           |\n|    ep_rew_mean                  | 68.8          |\n| time/                           |               |\n|    fps                          | 825           |\n|    iterations                   | 394           |\n|    time_elapsed                 | 1910          |\n|    total_timesteps              | 1576000       |\n| train/                          |               |\n|    approx_kl                    | 0.00097276305 |\n|    clip_fraction                | 0.0005        |\n|    clip_range                   | 0.2           |\n|    entropy_loss                 | -1.61         |\n|    explained_variance           | 0.503         |\n|    learning_rate                | 0.0003        |\n|    loss                         | 68.6          |\n|    n_updates                    | 786           |\n|    policy_gradient_loss         | -0.000294     |\n|    value_loss                   | 106           |\n| train_metrics/                  |               |\n|    action_queue_updates_success | 146           |\n|    action_queue_updates_total   | 155           |\n|    ice_dug                      | 364           |\n|    water_produced               | 85            |\n---------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Packaging and Submission\n\nWe now have a trained policy. In order to make it submittable to the competition we recommend you write code on separate files and only use kaggle notebooks for training as it can get very messy to program an RL agent just using a Kaggle notebook interface. The starter kit that was downloaded earlier has all of the code above written already and organized into separate files and folders. The observation wrapper and controller written here are saved to the `wrappers` folder. The SB3Wrapper is not in the kit, but is a part of the official luxai_s2 package and you can import it with\n\n```\nfrom luxai_s2.wrappers import SB3Wrapper\n```\n\nThe main files to take note of are `nn.py` and `agent.py`. Since kaggle servers don't have Stable Baselines 3 installed, `nn.py` is where we program some utility functions as well as the neural network model to load the SB3 trained weights into a PyTorch neural network model. `agent.py` will then use those utilities to load the model zip file at `MODEL_WEIGHTS_RELATIVE_PATH` which can be changed at the top of `agent.py`\n\n`agent.py` also uses the actions_mask function to invalidate some actions so that the policy only generates valid actions, which is a easy way to improve performance.","metadata":{}},{"cell_type":"code","source":"# if running on kaggle, run below to copy the rl starter kit files to the working directory\n!cp -r ../input/luxai-s2-rl-sb3-kit/* .\n!mv best_model.dontunzipme best_model.zip # kaggle auto unzips files but we don't want it to here so we do this","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if you trained an actual agent, copy its model weights here\n!mv logs/exp_1/models/best_model.zip best_model.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nTo submit your trained agent create a .tar.gz file. You can download the submission.tar.gz file from the right and submit it to the competition directly.","metadata":{}},{"cell_type":"code","source":"!tar -cvzf submission.tar.gz *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tips for Improving your Agent\n\nThis tutorial agent will train a policy that can efficiently control a single heavy robot that learns to pickup power, constantly dig ice, and transfer ice back to the factory and survive the full 1000 turns in the game. A simple improvement would be to add lichen planting to the action space / controller or program it directly as a rule in the agent.py file, allowing you to score points by the end of the game as well as generate more power.\n\nAnother easy idea is to modify the `agent.py` code so that you spawn multiple factories and multiple heavy robots, and simply run the trained policy on each heavy robot.\n\n\nIf you want to look into more scalable solutions, it's critical to first figure out how to model multiple units at once. This kit shows you how to control a single heavy robot effectively but not multiple. Another thing to consider is what observations and features would be the most useful. Finally, you can always try and develop a more complex action controller in addition to developing better reward functions.\n\nIf you feel you are experienced enough, you can take a look at [last season's winning solution by team Toad Brigade](https://www.kaggle.com/competitions/lux-ai-2021/discussion/294993) or [our paper: Emergent collective intelligence from massive-agent cooperation and competition](https://arxiv.org/abs/2301.01609) which show how to use convolutional neural nets and various other techniques (e.g. invalid action masking) to control a massive number of units at once.","metadata":{}}]}