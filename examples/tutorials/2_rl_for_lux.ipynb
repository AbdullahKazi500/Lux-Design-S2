{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537b9577-2a77-474b-aaca-7288338e2b47",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Lux AI Season 2 ðŸ¤–\n",
    "\n",
    "Part 2 of the RL series will now dig into building a working RL agent for the Lux AI Challenge, Season 2!\n",
    "\n",
    "Lux AI is designed to be intuitive to understand, but heavily layered in complexity and interactions of game mechanics in an multi-agent cooperative and competitive environment. \n",
    "\n",
    "Lux AI Season 2's rules can be found here: https://www.lux-ai.org/specs-s2. Make sure to read them to learn how to the game works, and the rest of this tutorial will be easier to understand.\n",
    "\n",
    "Part 1 of the series covered the single-agent RL setup, but Lux AI Season 2 is multi-agent! Moreover, the environment has different phases and a complex action space which makes it difficult to learn or use of the box. \n",
    "\n",
    "This tutorial will cover simple tools and tricks on how to reduce a complex problem into a easier one! We will primarily focus on three things: Reducing a multi-agent environment into a single-agent setup, modifying and transforming observations into learnable forms, and simplifying the action space with controllers.\n",
    "\n",
    "An upcoming part 3 tutorial will go over how to tackle the multi-agent setting to potentially learn better policies more efficiently and with self-play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e6e74-76e3-4cc5-96f7-e7abbead4f99",
   "metadata": {},
   "source": [
    "## Multi-agent to Single-agent\n",
    "\n",
    "Multi-agent environments are effectively multiple singular agents put together in the same environment at the same time. In typical multi-agent RL, you may often want to train a policy that plays all the controllable agents, not just one, which gathers more replay data and enables self-play. Self-play is a future topic to be discussed but shows potential to autonomously train a policy against itself and lead to learning stronger and stronger behaviors and potentially even emergent ones.\n",
    "\n",
    "For the sake of a tutorial, we will treat Lux as a single-agent environment. Naturally, a way to reduce the problem is to simply control only one of the agents in the game and let the other agent be a agent that does nothing or a heuristic of some kind to train against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b42e84-52ee-4efd-bb47-69b4ace7b4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
